{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/essdata/IDU/venvs/common_venv/lib/python3.9/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.10.3-CAPI-1.16.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_732/1634903444.py:16: MonkeyPatchWarning: Patching more than once will result in the union of all True parameters being patched\n",
      "  gevent.monkey.patch_all()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import grequests\n",
    "import gevent.monkey\n",
    "import copy\n",
    "import shapely\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import LineString, Point\n",
    "\n",
    "tqdm.pandas()\n",
    "gevent.monkey.patch_all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **responses.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = pd.read_csv(\"responses.csv\", delimiter=\";\")\n",
    "curricula_vitae = pd.read_csv(\"curricula_vitae.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "responses = responses[[\"date_creation\", \"id_candidate\", \"id_cv\", \"id_hiring_organization\", \"id_response\", \"id_vacancy\", \"region_code\", \"response_type\"]]\n",
    "responses = responses.dropna(subset=[\"id_cv\", \"id_vacancy\"], how=\"any\")\n",
    "responses[\"year\"] = responses[\"date_creation\"].apply(lambda x: x.split(\"-\")[0])\n",
    "\n",
    "print(\"Responses duplicates:\", responses.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_type\n",
      "Отказ           146829\n",
      "Приглашение    1384804\n",
      "Принятие        288255\n",
      "Name: id_cv, dtype: int64 \n",
      "\n",
      "year\n",
      "1970       404\n",
      "2015     48950\n",
      "2016    302166\n",
      "2017    597095\n",
      "2018    260358\n",
      "2019    126573\n",
      "2020    258085\n",
      "2021    226257\n",
      "Name: id_cv, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(responses.groupby(\"response_type\")[\"id_cv\"].count(), '\\n')\n",
    "print(responses.groupby(\"year\")[\"id_cv\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of candidates responding to vacacies: 692769\n",
      "Total number of vacancies under consideration: 172001\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of candidates responding to vacacies:\", len(responses[\"id_candidate\"].unique()))\n",
    "print(\"Total number of vacancies under consideration:\", len(responses[\"id_vacancy\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/essdata/IDU/venvs/common_venv/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/var/essdata/IDU/venvs/common_venv/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaiUlEQVR4nO3de5gdVZ3u8e9Lwk1AbulxgAQCGEaDcm2j4yDmKGBASRQFgjiCwvAwQ0YYdWbwoJmInnNAjs4ByYwTAcELBkTAZgwDzGAekOGSDoZLCJEQg0m4NZeAgAjB3/mjVkOx2bv3Trpr7+6s9/M8++ldVauqfl3p9Nu1qvYqRQRmZpavjTpdgJmZdZaDwMwscw4CM7PMOQjMzDLnIDAzy5yDwMwscw4CG3EkfUfSV1pot7Ok5ySNqqiO+ZJOTO+PlXT9EG57saTJ6f0sST8cwm3/T0kXDNX2bORzEFhbpV+eT0vatMX2x0v6ZXleRJwcEV9rtm5E/DYitoyIV0r7PnH9Km+6rx9FxCHN2km6WNLXW9jenhExf7B1SZosaVXNtv93RFRyHGxkchBY20gaD7wPCGBqZ6sZniSN7nQNlh8HgbXTp4HbgIuB48oLJI2TdKWkPklPSjpf0tuB7wB/nrp41qS2r/5VLWmJpI+UtjM6bWM/SeMlRZr3vyhC6Py0rfMlzZb0zZo6eiT9Xb3iJR0s6X5Jz0g6H1Bp2atnLir8s6THJT0r6R5J75B0EnAs8A+phmtS+xWS/lHS3cDzqd4Vkg4q7X4zSZdJ+p2kOyXtXdp3SHprafpiSV+XtAVwLbBj2t9zknas7WqSNDV1Ra1JZ01vLy1bIemLku5O3/dlkjZr9A9sI5ODwNrp08CP0utDkt4CkPrw/x14CBgP7ATMjYglwMnAramLZ5s62/wxcExp+kPAExFxZ7lRRJwB3AzMSNuaAVwCHCNpo1THGOAg4NLanaRlVwJfBsYADwJ/0eD7PAQ4ENgD2Bo4CngyIuak7/0bqYbDS+scA3wY2CYi1tbZ5jTgJ8B2qb6rJW3cYP/93/PzwKHAw2l/W0bEwzXf1x4Ux/A0oAuYB1wjaZNSs6OAKcCuwF7A8QPt10YeB4G1haQDgF2AyyNiIcUv0k+mxZOAHYG/j4jnI+LFiPhlg03VuhSYKulNafqTFL/YmoqIO4BngA+mWdOB+RHxWJ3mhwGLI+KKiHgZ+H/Aow02/TKwFfA2QBGxJCIeaVLOeRGxMiJ+32D5wtK+vwVsBrynyTZbcTTw84i4IW37/wKbA++tqe3hiHgKuAbYZwj2a8OIg8Da5Tjg+oh4Ik1fymvdQ+OAhxr8JTygiFgGLAEOT2EwlTp/0Q/gEuBT6f2ngB80aLcjsLK03yhP19R0I3A+MBt4XNIcSW9uUkfdbdVbHhF/BFalmgZrR4ozsfK2V1KclfUrB94LwJZDsF8bRnxhyionaXOK7oVRkvp/qWwKbJP6ulcCO0saXScMWhket797aCPgvhQO9dTb1g+Be1MdbweubrDuIxSBBRTXAcrTb9hRxHnAeZL+BLgc+HvgKw1qaFRbWXnfGwFjgf5unheAN5Xa/ilFULSy3YeBd5a23f99rW6ynm1AfEZg7fBR4BVgIkW3wj4Uv3RvprhucAfFL9qzJG0haTNJ/f3vjwFja/qsa82l6Jf/awY+G3gM2K08IyJWAQsozgR+OkDXzM+BPSUdke7s+RzFL9w3kPQuSe9OffjPAy8Cf2xUQ4v2L+37NOAPFBfeARYBn5Q0StIU4P2l9R4Dtpe0dYPtXg58WNIHU71fSNv+7/Wo0UYoB4G1w3HA99J9/Y/2vyi6T46luPvmcOCtwG8p/po9Oq17I7AYeFTSE2/cNKT+91sp+rUvG6COc4FPqPgcw3ml+ZdQ/FXcqFuI1KV1JHAW8CQwAbilQfM3A98FnqbodnkSOCctuxCYmO7QuXqAWmv9jOKYPA38JXBE6tMHOJXi+K2hOJ6vbjci7qc4Y1qe9vm67qSIWErRJfZt4Im0ncMj4qV1qM1GOPnBNJY7SQdSdBHtEv4PYRnyGYFlLXWHnApc4BCwXDkILFvpg1NrgB0obgc1y5K7hszMMuczAjOzzFX6OYJ0K9u5wCiKPtiz6rQ5CphFcb/zXRHxydo2ZWPGjInx48cPfbFmZhuwhQsXPhERXfWWVRYEafyY2cDBFLcDLpDUExH3ldpMAL4E/EVEPJ0+fDOg8ePH09vbW1XZZmYbJEkPNVpWZdfQJGBZRCxP9yTPpRg4q+yvgNkR8TRARDxeYT1mZlZHlUGwE68fP2UVrx+/BIrRGfeQdIuk21JXkpmZtVGnxxoaTfEJzckUY6fcJOmdEbGm3CiN434SwM4779zmEs3MNmxVnhGs5vWDco3ljQNZrQJ6IuLliPgN8GuKYHidiJgTEd0R0d3VVfdah5mZracqg2ABMEHSrmnAsOlAT02bqynOBvof/LEHsLzCmszMrEZlQZCGE54BXEcxXvzlEbFY0pmS+p9Xex3wpKT7gF9QPJjkyapqMjOzNxpxnyzu7u4O3z5qZrZuJC2MiO56y/zJYjOzzDkIzMwy1+nbR9tq1vxZg1t/8uDWNzMbjnxGYGaWOQeBmVnmHARmZplzEJiZZc5BYGaWOQeBmVnmHARmZplzEJiZZc5BYGaWOQeBmVnmHARmZplzEJiZZc5BYGaWOQeBmVnmHARmZplzEJiZZc5BYGaWOQeBmVnmHARmZplzEJiZZc5BYGaWOQeBmVnmHARmZplzEJiZZa7SIJA0RdJSScsknV5n+fGS+iQtSq8Tq6zHzMzeaHRVG5Y0CpgNHAysAhZI6omI+2qaXhYRM6qqw8zMBlblGcEkYFlELI+Il4C5wLQK92dmZuuhyiDYCVhZml6V5tX6uKS7JV0haVy9DUk6SVKvpN6+vr4qajUzy1anLxZfA4yPiL2AG4BL6jWKiDkR0R0R3V1dXW0t0MxsQ1dlEKwGyn/hj03zXhURT0bEH9LkBcD+FdZjZmZ1VBkEC4AJknaVtAkwHegpN5C0Q2lyKrCkwnrMzKyOyu4aioi1kmYA1wGjgIsiYrGkM4HeiOgBPidpKrAWeAo4vqp6zMysvsqCACAi5gHzaubNLL3/EvClKmswM7OBdfpisZmZdZiDwMwscw4CM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PMVRoEkqZIWippmaTTB2j3cUkhqbvKeszM7I0qCwJJo4DZwKHAROAYSRPrtNsKOBW4vapazMyssSrPCCYByyJieUS8BMwFptVp9zXgbODFCmsxM7MGqgyCnYCVpelVad6rJO0HjIuInw+0IUknSeqV1NvX1zf0lZqZZaxjF4slbQR8C/hCs7YRMSciuiOiu6urq/rizMwyUmUQrAbGlabHpnn9tgLeAcyXtAJ4D9DjC8ZmZu1VZRAsACZI2lXSJsB0oKd/YUQ8ExFjImJ8RIwHbgOmRkRvhTWZmVmNyoIgItYCM4DrgCXA5RGxWNKZkqZWtV8zM1s3o6vceETMA+bVzJvZoO3kKmsxM7P6/MliM7PMOQjMzDLnIDAzy5yDwMwscy0FgaTD0wfAzMxsA9PqL/ejgQckfUPS26osyMzM2qulIIiITwH7Ag8CF0u6NY3/s1Wl1ZmZWeVa7u6JiGeBKyhGEd0B+Bhwp6S/rag2MzNrg1avEUyTdBUwH9gYmBQRhwJ708KgcWZmNny1+sniI4B/joibyjMj4gVJJwx9WWZm1i6tdg09WhsCks4GiIj/GvKqzMysbVoNgoPrzDt0KAsxM7POGLBrSNJfA38D7C7p7tKirYBbqizMzMzao9k1gkuBa4H/A5xemv+7iHiqsqrMzKxtmgVBRMQKSafULpC0ncPAzGzka+WM4CPAQiAAlZYFsFtFdZmZWZsMGAQR8ZH0ddf2lGNmZu3W7GLxfgMtj4g7h7YcMzNrt2ZdQ98cYFkAHxjCWszMrAOadQ39j3YVYmZmndGsa+gDEXGjpCPqLY+IK6spy8zM2qVZ19D7gRuBw+ssC8BBYGY2wjXrGvqn9PUz7SnHzMzardVhqLeXdJ6kOyUtlHSupO2rLs7MzKrX6qBzc4E+4OPAJ9L7y6oqyszM2qfV5xHsEBFfK01/XdLRVRRkZmbt1eoZwfWSpkvaKL2OAq5rtpKkKZKWSlom6fQ6y0+WdI+kRZJ+KWniun4DZmY2OM1uH/0dr40xdBrww7RoI+A54IsDrDsKmE3xLINVwAJJPRFxX6nZpRHxndR+KvAtYMp6fSdmZrZemt01tNUgtj0JWBYRywEkzQWmAa8GQUQ8W2q/BUXomJlZG7V6jQBJ2wITgM3659U+vrLGTsDK0vQq4N11tnsK8HlgEzxkhZlZ27V6++iJwE0U1wW+mr7OGooCImJ2ROwO/CPw5Qb7P0lSr6Tevr6+oditmZklrV4sPhV4F/BQGn9oX2BNk3VWA+NK02PTvEbmAh+ttyAi5kREd0R0d3V1tViymZm1otUgeDEiXgSQtGlE3A/8WZN1FgATJO0qaRNgOtBTbiBpQmnyw8ADLdZjZmZDpNVrBKskbQNcDdwg6WngoYFWiIi1kmZQdCONAi6KiMWSzgR6I6IHmCHpIOBl4GnguPX7NszMbH21FAQR8bH0dpakXwBbA//RwnrzgHk182aW3p/aeqlmZlaFdblraD/gAIpbPG+JiJcqq8rMzNqm1buGZgKXANsDY4DvSap7h4+ZmY0srZ4RHAvsXbpgfBawCPh6RXWZmVmbtHrX0MOUPkgGbMrAt4KamdkI0WysoW9TXBN4Blgs6YY0fTBwR/XlmZlZ1Zp1DfWmrwuBq0rz51dSjZmZtV2zQecu6X+fPhS2R5pcGhEvV1mYmZm1R0sXiyVNprhraAXFkNTjJB3XZNA5MzMbAVq9a+ibwCERsRRA0h7Aj4H9qyrMzMzao9W7hjbuDwGAiPg1sHE1JZmZWTu1ekawUNIFvPaEsmN57UKymZmNYK0GwcnAKcDn0vTNwL9UUpGZmbVV0yBIzx6+KyLeRvFMYTMz24A0vUYQEa8ASyXt3IZ6zMyszVrtGtqW4pPFdwDP98+MiKmVVGVmZm3TahB8pdIqzMysY5qNNbQZxYXitwL3ABdGxNp2FGZmZu3R7BrBJUA3RQgcSvHBMjMz24A06xqaGBHvBJB0IR5x1Mxsg9PsjODVgeXcJWRmtmFqdkawt6Rn03sBm6dpARERb660OjMzq1yzYahHtasQMzPrjFYHnTMzsw2Ug8DMLHMOAjOzzDkIzMwy5yAwM8tcq2MNrRdJU4BzgVHABRFxVs3yzwMnAmuBPuCzEfFQlTUNxqz5swa3/uTBrW9mVoXKzgjScwxmUwxNMRE4RtLEmma/ArojYi/gCuAbVdVjZmb1Vdk1NAlYFhHLI+IlYC4wrdwgIn4RES+kyduAsRXWY2ZmdVQZBDsBK0vTq9K8Rk4Arq23QNJJknol9fb19Q1hiWZmNiwuFkv6FMUop+fUWx4RcyKiOyK6u7q62lucmdkGrsqLxauBcaXpsWne60g6CDgDeH9E/KHCeszMrI4qzwgWABMk7SppE2A60FNuIGlf4N+AqRHxeIW1mJlZA5UFQRq2egZwHbAEuDwiFks6U1L/s47PAbYEfiJpkaSeBpszM7OKVPo5goiYB8yrmTez9P6gKvdvZmbNDYuLxWZm1jkOAjOzzDkIzMwy5yAwM8ucg8DMLHMOAjOzzDkIzMwy5yAwM8ucg8DMLHMOAjOzzDkIzMwy5yAwM8ucg8DMLHMOAjOzzDkIzMwy5yAwM8ucg8DMLHMOAjOzzDkIzMwy5yAwM8ucg8DMLHMOAjOzzDkIzMwy5yAwM8ucg8DMLHMOAjOzzDkIzMwyV2kQSJoiaamkZZJOr7P8QEl3Slor6RNV1mJmZvVVFgSSRgGzgUOBicAxkibWNPstcDxwaVV1mJnZwEZXuO1JwLKIWA4gaS4wDbivv0FErEjL/lhhHWZmNoAqu4Z2AlaWpleleetM0kmSeiX19vX1DUlxZmZWGBEXiyNiTkR0R0R3V1dXp8sxM9ugVNk1tBoYV5oem+Zla9b8WYNbf/Lg1jczq6fKM4IFwARJu0raBJgO9FS4PzMzWw+VBUFErAVmANcBS4DLI2KxpDMlTQWQ9C5Jq4AjgX+TtLiqeszMrL4qu4aIiHnAvJp5M0vvF1B0GZmZWYeMiIvFZmZWHQeBmVnmHARmZplzEJiZZc5BYGaWOQeBmVnmHARmZplzEJiZZc5BYGaWOQeBmVnmHARmZpmrdKwhG1oextrMquAzAjOzzDkIzMwy5yAwM8ucg8DMLHMOAjOzzDkIzMwy59tHM+LbT82sHp8RmJllzkFgZpY5B4GZWeZ8jcBa5msMZhsmnxGYmWXOQWBmljl3DVnbuGvJbHhyENiI4SAxq0alQSBpCnAuMAq4ICLOqlm+KfB9YH/gSeDoiFhRZU2WLweJWX2VBYGkUcBs4GBgFbBAUk9E3FdqdgLwdES8VdJ04Gzg6KpqMhuMwQbJcOAws3qqPCOYBCyLiOUAkuYC04ByEEwDZqX3VwDnS1JERIV1mWVrQwiznFUV5FUGwU7AytL0KuDdjdpExFpJzwDbA0+UG0k6CTgpTT4naWklFQ+dMdR8D8OU6xxaI6VOGDm1us6Sr/LVway+S6MFI+JicUTMAeZ0uo5WSeqNiO5O19GM6xxaI6VOGDm1us72qPJzBKuBcaXpsWle3TaSRgNbU1w0NjOzNqkyCBYAEyTtKmkTYDrQU9OmBzguvf8EcKOvD5iZtVdlXUOpz38GcB3F7aMXRcRiSWcCvRHRA1wI/EDSMuApirDYEIyUbizXObRGSp0wcmp1nW0g/wFuZpY3jzVkZpY5B4GZWeYcBOtB0jhJv5B0n6TFkk6t02aypGckLUqvmZ2oNdWyQtI9qY7eOssl6TxJyyTdLWm/DtT4Z6VjtUjSs5JOq2nTsWMq6SJJj0u6tzRvO0k3SHogfd22wbrHpTYPSDquXpuK6zxH0v3p3/YqSds0WHfAn5M21DlL0urSv+9hDdadImlp+nk9vQN1XlaqcYWkRQ3WbdvxHLSI8GsdX8AOwH7p/VbAr4GJNW0mA//e6VpTLSuAMQMsPwy4FhDwHuD2Dtc7CngU2GW4HFPgQGA/4N7SvG8Ap6f3pwNn11lvO2B5+rpter9tm+s8BBid3p9dr85Wfk7aUOcs4Ist/Gw8COwGbALcVft/r+o6a5Z/E5jZ6eM52JfPCNZDRDwSEXem978DllB8SnqkmgZ8Pwq3AdtI2qGD9XwQeDAiHupgDa8TETdR3NlWNg24JL2/BPhonVU/BNwQEU9FxNPADcCUdtYZEddHxNo0eRvFZ3o6qsHxbMWrQ9dExEtA/9A1lRioTkkCjgJ+XNX+28VBMEiSxgP7ArfXWfznku6SdK2kPdtb2esEcL2khWm4jlr1hgPpZLBNp/F/ruFyTAHeEhGPpPePAm+p02a4HdvPUpz91dPs56QdZqQurIsadLUNp+P5PuCxiHigwfLhcDxb4iAYBElbAj8FTouIZ2sW30nRtbE38G3g6jaXV3ZAROwHHAqcIunADtYyoPThw6nAT+osHk7H9HWi6AsY1vdiSzoDWAv8qEGTTv+c/CuwO7AP8AhFt8twdgwDnw10+ni2zEGwniRtTBECP4qIK2uXR8SzEfFcej8P2FjSmDaX2V/L6vT1ceAqitPrslaGA2mXQ4E7I+Kx2gXD6Zgmj/V3oaWvj9dpMyyOraTjgY8Ax6bQeoMWfk4qFRGPRcQrEfFH4LsN9j9cjudo4AjgskZtOn0814WDYD2kvsELgSUR8a0Gbf40tUPSJIpj3fZxlCRtIWmr/vcUFw7vrWnWA3w63T30HuCZUpdHuzX8K2u4HNOS8hApxwE/q9PmOuAQSdumro5D0ry2UfGAqH8ApkbECw3atPJzUqma61Ifa7D/VoauaYeDgPsjYlW9hcPheK6TTl+tHokv4ACKboC7gUXpdRhwMnByajMDWExxV8NtwHs7VOtuqYa7Uj1npPnlWkXxEKEHgXuA7g7VugXFL/atS/OGxTGlCKdHgJcp+qVPoBgy/b+AB4D/BLZLbbspnsjXv+5ngWXp9ZkO1LmMol+9/2f1O6ntjsC8gX5O2lznD9LP390Uv9x3qK0zTR9Gcafeg52oM82/uP/nstS2Y8dzsC8PMWFmljl3DZmZZc5BYGaWOQeBmVnmHARmZplzEJiZZc5BYFmR9EoaDfJeSdc0GomzEySNL49yadYuDgLLze8jYp+IeAfFYGKndLogs05zEFjObiUNWCZpd0n/kQYIu1nS29L8I9PZw12Sbkrzjpf0M0nz0zMG/ql/g5I+n9rfq/Q8hfSX/hJJ31Xx/IrrJW2elu2ftn0XpVCStKekO9LZy92SJrTtqFh2HASWJUmjKIa77h+eYA7wtxGxP/BF4F/S/JnAh6IY6G5qaROTgI8DewFHSuqWtD/wGeDdFM91+CtJ+6b2E4DZEbEnsCatC/C9tN+9a0o8GTg3Ivah+KRy3aEMzIbC6E4XYNZmm6cnSu1E8RyJG9Iosu8FfpKGMgLYNH29BbhY0uVAeXDBGyLiSQBJV/LasCNXRcTzpfnvowib30TEorTuQmB8uj6xTRRj3kMxxMKh6f2twBmSxgJXRuOhjs0GzWcElpvfp7+yd6EYY+kUiv8Ha9K1g/7X2wEi4mTgyxQjXi6UtH3aTu3YLM3GavlD6f0rNPkjLCIupTgD+T0wT9IHmn5nZuvJQWBZimIUzs8BXwBeAH4j6Uh49RnOe6f3u0fE7RExE+jjtSGQD1bxzOLNKZ5MdgtwM/BRSW9KI05+LM1rVMMaYI2kA9KsY/uXSdoNWB4R51GMarrX0HznZm/kILBsRcSvKEa6PIbil/AJ6aLtYl57/OE5Kh5Afi/w3xSjSQLcQfE8iruBn0ZEbxSPL704LbudYgTSXzUp4zPA7NRdpdL8o4B70/x3AN8fxLdqNiCPPmq2jtJDXrojYkanazEbCj4jMDPLnM8IzMwy5zMCM7PMOQjMzDLnIDAzy5yDwMwscw4CM7PM/X/rHzmROg0GxgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of applying for a vacancy:\n",
      "count    692769.000\n",
      "mean          2.627\n",
      "std           4.713\n",
      "min           1.000\n",
      "25%           1.000\n",
      "50%           1.000\n",
      "75%           3.000\n",
      "max        1187.000\n",
      "Name: id_vacancy, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "freq = responses.groupby([\"id_candidate\"])[\"id_vacancy\"].count()\n",
    "q = freq.quantile(0.99)\n",
    "\n",
    "plt.title(\"Activity distribution\")\n",
    "plt.xlabel(\"Responds\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.hist(freq[freq <= q].values, bins=int(q), density = 1, color ='green', alpha = 0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"Frequency of applying for a vacancy:\")\n",
    "print(freq.describe().round(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **curricula_vitae.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "curricula_vitae = curricula_vitae[[\n",
    "    \"id_candidate\", \"id_cv\", \"birthday\",\"skills\", \"additional_skills\", \"busy_type\", \"country\", \"date_creation\", \"date_publish\", \n",
    "    \"date_modify_inner_info\", \"education_type\", \"experience\", \"gender\", \"industry_code\", \"locality\", \"other_info\", \n",
    "    \"position_name\", \"profession_code\", \"relocation\", \"salary\"]]\n",
    "\n",
    "curricula_vitae = curricula_vitae.drop_duplicates()\n",
    "curricula_vitae = curricula_vitae.dropna(subset=[\"locality\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**define location of CV in each year**\n",
    "\n",
    "CV is considered active in a year if it was created, published, or modified in this year.   \n",
    "For some CVs, location tends to change within a year. In this case, the primary location is the location of the longest staying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_years = curricula_vitae[[\"id_cv\", \"date_creation\", \"date_publish\", \"date_modify_inner_info\", \"locality\"]]\n",
    "\n",
    "date_columns = [\"date_creation\", \"date_publish\", \"date_modify_inner_info\"]\n",
    "cv_years = cv_years.astype(dict((c, \"datetime64[ns]\") for c in date_columns))\n",
    "\n",
    "# fix mistake when date_creation is later than date_publish or date_modify_inner_info\n",
    "# new field - fixed_date_creation\n",
    "\n",
    "cv_years_grouped = cv_years.groupby([\"id_cv\"])\n",
    "first_activity = pd.concat((\n",
    "    cv_years_grouped[\"date_publish\"].min(), cv_years_grouped[\"date_modify_inner_info\"].min()\n",
    "    ), axis=1).min(axis=1)\n",
    "date_creation = cv_years_grouped[\"date_creation\"].min()\n",
    "fixed_date_creation = pd.concat((date_creation, first_activity), axis=1).min(axis=1)\n",
    "cv_years = cv_years.join(fixed_date_creation.rename(\"fixed_date_creation\"), on=\"id_cv\")\n",
    "\n",
    "# fillna in the field date_publish, assuming that a resume was published on the day of creation if another date is initially unknown\n",
    "# corrected field - date_publish\n",
    "\n",
    "cv_years[\"date_publish\"] = cv_years[\"date_publish\"].fillna(cv_years[\"date_creation\"])\n",
    "cv_years[\"date_modify_inner_info\"] = cv_years[\"date_modify_inner_info\"].fillna(cv_years[\"date_publish\"])\n",
    "\n",
    "# define CV's location at the moment of creation\n",
    "\n",
    "cv_years[\"year_creation\"] = cv_years[\"fixed_date_creation\"].astype(\"str\").str.split(\"-\").str[0]\n",
    "cv_years = cv_years.join(cv_years.groupby([\"id_cv\"])[\"date_modify_inner_info\"].min().rename(\"first_modification\"), on=\"id_cv\")\n",
    "creation_locality = cv_years[cv_years[\"first_modification\"] == cv_years[\"date_modify_inner_info\"]][[\"id_cv\", \"year_creation\", \"locality\"]]\n",
    "creation_locality = creation_locality.drop_duplicates()\n",
    "\n",
    "# define CV's location at the year of publishing\n",
    "\n",
    "cv_years[\"year_publish\"] = cv_years[\"date_publish\"].astype(\"str\").str.split(\"-\").str[0]\n",
    "cv_years[\"publich_modify_margin\"] = (cv_years[\"date_publish\"] - cv_years[\"date_modify_inner_info\"]).abs()\n",
    "min_margin = cv_years.groupby([\"id_cv\", \"date_publish\"])[\"publich_modify_margin\"].min().rename(\"min_publich_modify_margin\")\n",
    "cv_years = cv_years.join(min_margin, on =[\"id_cv\", \"date_publish\"])\n",
    "publish_locality = cv_years[cv_years[\"publich_modify_margin\"] == cv_years[\"min_publich_modify_margin\"]][[\"id_cv\", \"year_publish\", \"locality\"]]\n",
    "publish_locality = publish_locality.drop_duplicates()\n",
    "\n",
    "# define CV's location of the longest staying in each year\n",
    "\n",
    "cv_years = cv_years.dropna(subset=[\"date_modify_inner_info\"])\n",
    "cv_years[\"year_modification\"] = cv_years[\"date_modify_inner_info\"].astype(\"str\").str.split(\"-\").str[0]\n",
    "cv_years = cv_years.sort_values([\"id_cv\", \"year_modification\", \"date_modify_inner_info\"], ascending=False)\n",
    "cv_years[\"modification_margin\"] = cv_years.groupby([\"id_cv\", \"year_modification\"])[\"date_modify_inner_info\"].diff().dt.days.fillna(0)\n",
    "cv_years = cv_years.join(\n",
    "    cv_years.groupby([\"id_cv\", \"year_modification\"])[\"modification_margin\"].min().rename(\"staying\"), \n",
    "    on=[\"id_cv\", \"year_modification\"]\n",
    "    )\n",
    "staying_locality = cv_years[cv_years[\"modification_margin\"] == cv_years[\"staying\"]][[\"id_cv\", \"year_modification\", \"locality\"]]\n",
    "staying_locality = staying_locality.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define locality for each year of activity\n",
    "\n",
    "for df in [creation_locality, publish_locality, staying_locality]:\n",
    "    df.columns = [\"id_cv\", \"year\", \"locality\"]\n",
    "    df.set_index([\"id_cv\", \"year\"], inplace=True)\n",
    "\n",
    "locality = staying_locality.join(publish_locality, rsuffix=\"_publish\", how=\"outer\")\n",
    "locality[\"locality\"] = locality[\"locality\"].fillna(locality[\"locality_publish\"])\n",
    "\n",
    "locality = locality.join(creation_locality, rsuffix=\"_creation\", how=\"outer\")\n",
    "locality[\"locality\"] = locality[\"locality\"].fillna(locality[\"locality_creation\"])\n",
    "locality = locality[[\"locality\"]]\n",
    "locality[\"locality\"] = locality[\"locality\"].astype(\"int\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**decode kladr to city name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "locality = pd.read_csv(\"output/curricula_vitae_years.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create separate table for cities\n",
    "cv_cities = pd.DataFrame({\"kladr\": locality[\"locality\"].unique()})\n",
    "\n",
    "# downloaded KLADR DBs\n",
    "kladr_2018 = pd.read_csv(\"data/cities.csv\").set_index(\"Код КЛАДР\")\n",
    "kladr_2021 = pd.read_csv(\"data/kladr.csv\").set_index(\"code\")\n",
    "\n",
    "# create full column with name of regular cities (city_name), federal cities (name - for Mocsow, SPb and Sevastopol)\n",
    "# and settelments in case they are not included in bigger city (name)\n",
    "kladr_2021[\"city\"] = kladr_2021[\"city_name\"].fillna(kladr_2021[\"name\"])\n",
    "\n",
    "# create row with place' fullest description for yandex geocoder \n",
    "description_func = lambda x: (\", \".join([x.region_name, x.dist_name, x.city]) if not pd.isnull(x.dist_name) \n",
    "                              else \", \".join([x.region_name, x.city]))\n",
    "\n",
    "kladr_2021.dist_name = kladr_2021.dist_name + \" район\"\n",
    "kladr_2021[\"place_description\"] = kladr_2021.apply(description_func, axis=1)\n",
    "\n",
    "kladr_2018[\"region_name\"] = kladr_2018[\"Тип региона\"] + \" \" + kladr_2018[\"Регион\"]\n",
    "kladr_2018[\"dist_name\"] = kladr_2018[\"Тип района\"] + \" \" + kladr_2018[\"Район\"]\n",
    "kladr_2018[\"city\"] = kladr_2018[\"Город\"]\n",
    "kladr_2018 = kladr_2018.dropna(subset=[\"city\"])\n",
    "kladr_2018[\"place_description\"] = kladr_2018.apply(description_func, axis=1)\n",
    "\n",
    "cv_cities = cv_cities.join(kladr_2021[[\"city\", \"place_description\"]], on=\"kladr\")\n",
    "cv_cities_undefind = cv_cities[cv_cities[\"city\"].isna()]\n",
    "cv_cities_undefind = cv_cities_undefind[[\"kladr\"]].join(kladr_2018[[\"city\", \"place_description\"]], on=\"kladr\")\n",
    "cv_cities_defind = pd.concat((cv_cities[cv_cities[\"city\"].notna()], cv_cities_undefind[cv_cities_undefind[\"city\"].notna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 582/582 [14:45<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# geocode place description to coordinates\n",
    "# might need several code runnings with keys shuffling/replacment (due to yandex limits)\n",
    "\n",
    "from utils.yandex_geocoder import yandex_create_urls, yandex_geocode\n",
    "\n",
    "with open(\"./keys.txt\") as f:\n",
    "    api_keys = f.read().splitlines()\n",
    "\n",
    "urls = []\n",
    "api_key_counter = 0\n",
    "for i, adr in enumerate(list(cv_cities_defind[\"place_description\"])):\n",
    "    if i%24500 == 0 and i != 0:\n",
    "        api_key_counter += 1\n",
    "    urls.append(yandex_create_urls(adr, api_keys[api_key_counter]))\n",
    "yandex_geocoded_data = yandex_geocode(urls)\n",
    "\n",
    "cities_coordinates = pd.DataFrame(yandex_geocoded_data).replace({0: None})\n",
    "cv_cities_defind[[\"x\", \"y\"]] = cities_coordinates[[\"x\", \"y\"]].to_numpy()\n",
    "cv_cities_defind = cv_cities_defind.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kladr numbers that couldn't be decoded with DB's\n",
    "cv_cities_undefind =  cv_cities_undefind[cv_cities_undefind[\"city\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [02:10<00:00,  1.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.dadata_decoder import dadata_create_request, dadata_decode_kladr\n",
    "\n",
    "\n",
    "# decode kladr numbers and get coordinates with dadata service\n",
    "\n",
    "with open(\"./dadata_token.txt\") as f:\n",
    "    token = f.read().splitlines()\n",
    "    \n",
    "requests = []\n",
    "for i, kladr in enumerate(list(cv_cities_undefind[\"kladr\"])):\n",
    "    requests.append(dadata_create_request(kladr, token[1]))\n",
    "dadata_decoded_data = dadata_decode_kladr(requests)\n",
    "\n",
    "cv_cities_undefind = cv_cities_undefind.set_index(\"kladr\")\n",
    "\n",
    "cities_coordinates = pd.DataFrame(dadata_decoded_data, index=list(cv_cities_undefind.index)).replace({0: None})\n",
    "cities_coordinates = cities_coordinates[cities_coordinates[\"x\"].notna()]\n",
    "cities_coordinates[\"city\"] = cities_coordinates[\"city\"].fillna(cities_coordinates[\"settlement\"])\n",
    "cities_coordinates[\"place_description\"] = cities_coordinates[\"region\"] + \", \" + cities_coordinates[\"city\"] \n",
    "\n",
    "cv_cities_undefind.loc[cities_coordinates.index, [\"city\", \"place_description\", \"x\", \"y\"]] = cities_coordinates[[\"city\", \"place_description\", \"x\", \"y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the process of decoding several times (for some reason dadata can not decode all at once)\n",
    "\n",
    "while len(cities_coordinates) > 0:\n",
    "    with open(\"./dadata_token.txt\") as f:\n",
    "        token = f.read().splitlines()\n",
    "        \n",
    "    requests = []\n",
    "    for i, kladr in enumerate(list(cv_cities_undefind[cv_cities_undefind[\"city\"].isna()].index)):\n",
    "        requests.append(dadata_create_request(kladr, token[0]))\n",
    "    dadata_decoded_data = dadata_decode_kladr(requests)\n",
    "\n",
    "    if cities_coordinates[\"x\"].notna().any():\n",
    "        cities_coordinates = pd.DataFrame(\n",
    "            dadata_decoded_data, index=list(cv_cities_undefind[cv_cities_undefind[\"city\"].isna()].index)\n",
    "            ).replace({0: None})\n",
    "        cities_coordinates = cities_coordinates[cities_coordinates[\"x\"].notna()]\n",
    "        cities_coordinates[\"city\"] = cities_coordinates[\"city\"].fillna(cities_coordinates[\"settlement\"])\n",
    "        cities_coordinates[\"place_description\"] = cities_coordinates[\"region\"] + \", \" + cities_coordinates[\"city\"]\n",
    "\n",
    "        cv_cities_undefind.loc[cities_coordinates.index, [\"city\", \"place_description\", \"x\", \"y\"]] = cities_coordinates[[\"city\", \"place_description\", \"x\", \"y\"]]\n",
    "        cv_cities_undefind = cv_cities_undefind.dropna(subset=[\"x\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decodes CVs: 63487 (0.94 %)\n",
      "Undecoded CVs: 3892 (0.06 %)\n"
     ]
    }
   ],
   "source": [
    "cv_cities_decoded = pd.concat((cv_cities_defind, cv_cities_undefind.reset_index()))\n",
    "\n",
    "print(\"Decodes CVs:\", len(cv_cities_decoded), f\"({round(len(cv_cities_decoded) / len(cv_cities), 2)} %)\")\n",
    "print(\"Undecoded CVs:\", len(cv_cities) - len(cv_cities_decoded), f\"({(round((len(cv_cities) - len(cv_cities_decoded)) / len(cv_cities), 2))} %)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**select unique CVs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# drop CV duplicates taking (first) the fullest and (second) the recent version of resume \n",
    "\n",
    "curricula_vitae = curricula_vitae.drop([\"locality\"], axis=1)\n",
    "\n",
    "# split df into duplicatded and unique parts\n",
    "cv_duplicates_mask = curricula_vitae[\"id_cv\"].isin(curricula_vitae[\"id_cv\"][curricula_vitae[\"id_cv\"].duplicated()].unique())\n",
    "cv_duplicates = curricula_vitae[cv_duplicates_mask]\n",
    "cv_duplicates = cv_duplicates.astype({\"date_modify_inner_info\": \"datetime64[ns]\"})\n",
    "\n",
    "# find the fullest rows\n",
    "count_nan = pd.concat((cv_duplicates[\"id_cv\"], cv_duplicates.isna().sum(axis=1).rename(\"count_nan\")), axis=1)\n",
    "min_nan = count_nan.groupby(\"id_cv\").min().rename(columns={\"count_nan\": \"min_nan\"})\n",
    "count_nan = count_nan.join(min_nan, on=\"id_cv\")\n",
    "cv_fullest_rows = cv_duplicates[count_nan[\"count_nan\"] == count_nan[\"min_nan\"]]\n",
    "\n",
    "# leave the fullest rows updated most recently\n",
    "recent_activity = cv_fullest_rows.groupby(\"id_cv\")[\"date_modify_inner_info\"].max().rename(\"recent_activity\")\n",
    "recent_activity = cv_fullest_rows[[\"id_cv\", \"date_modify_inner_info\"]].join(recent_activity, on=\"id_cv\")\n",
    "cv_last_activity = cv_fullest_rows[recent_activity[\"date_modify_inner_info\"] == recent_activity[\"recent_activity\"]]\n",
    "cv_duplicated_unique = cv_last_activity.drop_duplicates(subset=[\"id_cv\"], keep=\"first\")\n",
    "\n",
    "# merge two part into one dataset\n",
    "cv_duplicated_unique = cv_duplicated_unique.astype({\"date_modify_inner_info\": \"datetime64[ns]\"})\n",
    "curricula_vitae_unique = pd.concat((curricula_vitae[~cv_duplicates_mask], cv_duplicated_unique)).reset_index(drop=True)\n",
    "\n",
    "print(\"CV duplicates:\", curricula_vitae_unique[\"id_cv\"].duplicated().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**merge CVs with education and work experience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id_candidate         1.000\n",
       "id_cv                1.000\n",
       "position_name        0.998\n",
       "industry_code        1.000\n",
       "skills               0.284\n",
       "additional_skills    0.349\n",
       "other_info           0.070\n",
       "profession_code      0.432\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_professions = curricula_vitae_unique[[\"id_candidate\", \"id_cv\", \"position_name\", \"industry_code\", \"skills\", \"additional_skills\", \"other_info\", \"profession_code\"]]\n",
    "(cv_professions.notna().sum() / len(cv_professions)).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_edu = pd.read_csv(\"data/edu.csv\", delimiter=\";\")\n",
    "cv_edu = cv_edu[[\"faculty\", \"id_cv\", \"qualification\", \"speciality\"]]\n",
    "cv_edu = cv_edu.drop_duplicates()\n",
    "\n",
    "workexp = pd.read_csv(\"data/workexp.csv\", delimiter=\";\")\n",
    "workexp = workexp[[\"id_cv\", \"job_title\", \"demands\"]]\n",
    "workexp = workexp.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each column: find duplicates, add separator to each row, sum strings under the same index and drop last hanging separator\n",
    "\n",
    "def join_inf(df, index, column, sep=\" \\ \"):\n",
    "\n",
    "    df = df[[index, column]].dropna().drop_duplicates()\n",
    "    df[column] = df[column] + sep\n",
    "    df = df.groupby([index])[column].sum()\n",
    "    df = df.str.rstrip(sep)\n",
    "\n",
    "    return df\n",
    "\n",
    "new_columns = list(map(lambda x: join_inf(cv_edu, \"id_cv\", x), cv_edu.set_index(\"id_cv\").columns))\n",
    "edu_joint = pd.concat(new_columns, axis=1)\n",
    "cv_professions = cv_professions.join(edu_joint, on=\"id_cv\")\n",
    "\n",
    "new_columns = list(map(lambda x: join_inf(workexp, \"id_cv\", x), workexp.set_index(\"id_cv\").columns))\n",
    "workexp_joint = pd.concat(new_columns, axis=1)\n",
    "cv_professions = cv_professions.join(workexp_joint, on=\"id_cv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **vacancies.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3597418/491186538.py:1: DtypeWarning: Columns (1,6,28,41,46,53,61,65,67,70) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  vacancies = pd.read_csv(\"data/vacancies.csv\", delimiter=\";\")\n",
      "/tmp/ipykernel_3597418/491186538.py:7: DtypeWarning: Columns (36,37,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  organizations = pd.read_csv(\"data/organizations.csv\", delimiter=\";\")\n"
     ]
    }
   ],
   "source": [
    "vacancies = pd.read_csv(\"data/vacancies.csv\", delimiter=\";\")\n",
    "vacancies = vacancies.drop_duplicates()\n",
    "vacancies = vacancies[[\"identifier\", \"title\", \"id_hiring_organization\", \"industry\", \"profession\", \"region\", \n",
    "           \"job_location_address\", \"job_location_geo_longitude\", \"job_location_geo_latitude\", \"okso_code\",\n",
    "           \"date_creation\", \"date_posted\", \"date_modify_inner_info\"]]\n",
    "\n",
    "organizations = pd.read_csv(\"data/organizations.csv\", delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select recent ones from duplicated\n",
    "\n",
    "vacancies[\"job_location_geo_longitude\"] = vacancies[\"job_location_geo_longitude\"].round(2)\n",
    "vacancies[\"job_location_geo_latitude\"] = vacancies[\"job_location_geo_latitude\"].round(2)\n",
    "\n",
    "vacancies_copy = copy.deepcopy(vacancies).drop_duplicates(\n",
    "    subset=[\"identifier\", \"job_location_address\", \"job_location_geo_longitude\", \"job_location_geo_latitude\"]\n",
    "    )\n",
    "\n",
    "date_columns = [\"date_creation\", \"date_posted\", \"date_modify_inner_info\"]\n",
    "vacancies_copy = vacancies_copy.astype(dict((c, \"datetime64[ns]\") for c in date_columns))\n",
    "\n",
    "vacancies_copy = vacancies_copy.sort_values(by=[\"date_modify_inner_info\", \"date_posted\", \"date_creation\", \"job_location_geo_latitude\"])\n",
    "vacancies_copy =  vacancies_copy.groupby([\"identifier\"]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1665/1665 [01:40<00:00, 16.59it/s]\n"
     ]
    }
   ],
   "source": [
    "kladr_2021 = pd.read_csv(\"data/kladr.csv\").set_index(\"code\")\n",
    "kladr_2021[\"city\"] = kladr_2021[\"city_name\"].fillna(kladr_2021[\"name\"])\n",
    "\n",
    "regions = list(kladr_2021[\"region_name\"].unique())\n",
    "regions = pd.Series(regions)\n",
    "regions = regions.replace({\n",
    "    'Республика Татарстан (Татарстан)': 'Республика Татарстан', \n",
    "    'Ханты-Мансийский автономный округ – Югра': 'Ханты-Мансийский автономный округ - Югра',\n",
    "    'Чувашская Республика - Чувашия': 'Чувашская Республика'\n",
    "    })\n",
    "\n",
    "cities_name = np.array(kladr_2021[\"city\"][kladr_2021[\"city\"].notna()].unique())\n",
    "\n",
    "# fill coordinates for regional ciities\n",
    "vacancies_copy[\"city\"] = None\n",
    "\n",
    "coord_columns = [\"job_location_geo_longitude\", \"job_location_geo_latitude\", \"city\"]\n",
    "vacancies_copy.loc[vacancies_copy[\"job_location_address\"] == \"г. Москва\", coord_columns] = [37.618423, 55.751244, 'Москва']\n",
    "vacancies_copy.loc[vacancies_copy[\"job_location_address\"] == \"г. Санкт-Петербург\", coord_columns] = [30.3350986, 59.9342802, 'Санкт-Петербург']\n",
    "vacancies_copy.loc[vacancies_copy[\"job_location_address\"] == \"г. Севастополь\", coord_columns] = [33.535667, 44.629650, 'Севастополь']\n",
    "\n",
    "# defines rows that contains information only about region\n",
    "vacancies_nocoord = vacancies_copy[vacancies_copy[\"job_location_geo_longitude\"].isna()]\n",
    "vacancies_only_region = vacancies_nocoord[vacancies_nocoord[\"job_location_address\"].isin(regions)]\n",
    "vacancies_bad_address = vacancies_nocoord[~vacancies_nocoord[\"job_location_address\"].isin(regions)]\n",
    "\n",
    "# extract cities from unstructured address row\n",
    "\n",
    "def preprocess_str(series):\n",
    "    return series.copy().str.split(\",\").str.join(\" \").str.split(\";\").str.join(\" \") + \" \"\n",
    "\n",
    "def get_city(loc, cities_name):\n",
    "    city = cities_name[np.array([\" \" + c + \" \" in loc for c in cities_name])]\n",
    "    if len(city) > 0:\n",
    "        return city[0]\n",
    "\n",
    "job_location_address_fixed = preprocess_str(vacancies_bad_address[\"job_location_address\"])\n",
    "city_match = job_location_address_fixed.progress_apply(lambda loc: get_city(loc, cities_name))\n",
    "vacancies_copy.loc[city_match.index, \"city\"] = city_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "undefined_address = pd.concat((vacancies_only_region, vacancies_copy.loc[city_match[city_match.isna()].index]))\n",
    "undefined_address = undefined_address.join(\n",
    "    organizations.set_index(\"id_organization\")[[\"inn\", \"ogrn\"]], on=\"id_hiring_organization\"\n",
    "    )\n",
    "undefined_address[\"org_code\"] = undefined_address[\"inn\"].fillna(undefined_address[\"ogrn\"])\n",
    "undefined_address = undefined_address.dropna(subset=[\"org_code\"])\n",
    "org_codes = list(undefined_address['org_code'].astype(\"int\").unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517/517 [04:27<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.dadata_organizations import dadata_create_request, dadata_decode_organization\n",
    "\n",
    "# decode organization address by its inn/ogrn\n",
    "\n",
    "with open(\"./dadata_token.txt\") as f:\n",
    "    token = f.read().splitlines()\n",
    "    \n",
    "requests = []\n",
    "for i, code in enumerate(org_codes):\n",
    "    requests.append(dadata_create_request(code, token[1]))\n",
    "dadata_decoded_data = dadata_decode_organization(requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy city name and lat/long to initial dataset with vacancies\n",
    "\n",
    "org_adress = pd.DataFrame(dadata_decoded_data)\n",
    "org_adress[\"code\"] = org_codes\n",
    "org_adress[\"city\"] = org_adress[\"city\"].fillna(org_adress[\"settlement\"])\n",
    "org_adress = org_adress.set_index(\"code\")\n",
    "\n",
    "undefined_address[\"org_code\"] = undefined_address[\"org_code\"].astype(\"int\")\n",
    "undefined_address = undefined_address.reset_index().set_index(\"org_code\")\n",
    "undefined_address.loc[org_adress.index, \"city\"] = org_adress['city']\n",
    "undefined_address.loc[org_adress.index, \"job_location_geo_latitude\"] = org_adress['geo_lat']\n",
    "undefined_address.loc[org_adress.index, \"job_location_geo_longitude\"] = org_adress['geo_lon']\n",
    "undefined_address = undefined_address.reset_index().set_index(\"identifier\")\n",
    "\n",
    "undefined_address = undefined_address.dropna(subset=[\"city\"])\n",
    "vacancies_copy.loc[undefined_address.index, \"city\"] = undefined_address[\"city\"]\n",
    "vacancies_copy.loc[undefined_address.index, \"job_location_geo_latitude\"] = undefined_address[\"job_location_geo_latitude\"]\n",
    "vacancies_copy.loc[undefined_address.index, \"job_location_geo_longitude\"] = undefined_address[\"job_location_geo_longitude\"]\n",
    "\n",
    "vacancies_copy = vacancies_copy.dropna(subset=[\"city\", \"job_location_geo_longitude\", \"job_location_geo_latitude\"], how=\"all\")\n",
    "vacancies_coord = vacancies_copy[vacancies_copy[\"city\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1212/1212 [28:33<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "from utils.yandex_geocoder import yandex_create_urls, yandex_geocode\n",
    "\n",
    "# geocode coordinates to cities/settelments\n",
    "\n",
    "all_coords = vacancies_coord[[\"job_location_geo_longitude\", \"job_location_geo_latitude\"]].to_numpy()\n",
    "unique_coords = np.unique(all_coords, axis=0)\n",
    "unique_coords_str = list(map(lambda x: str(x[0]) + \",\" + str(x[1]), unique_coords.tolist()))\n",
    "\n",
    "with open(\"./keys_1.txt\") as f:\n",
    "    api_keys = f.read().splitlines()\n",
    "\n",
    "urls = []\n",
    "api_key_counter = 0\n",
    "for i, adr in enumerate(unique_coords_str):\n",
    "    if i%24500 == 0 and i != 0:\n",
    "        api_key_counter += 1\n",
    "    urls.append(yandex_create_urls(adr, api_keys[api_key_counter]))\n",
    "yandex_geocoded_data = yandex_geocode(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancies_city = pd.DataFrame(yandex_geocoded_data)\n",
    "vacancies_city[\"coords\"] = unique_coords_str\n",
    "vacancies_city = vacancies_city.replace({0: np.nan})\n",
    "vacancies_city[\"locality\"] = vacancies_city[\"locality\"].fillna(vacancies_city[\"name\"])\n",
    "\n",
    "vacancies_copy[\"coords\"] = vacancies_copy[\"job_location_geo_longitude\"].astype(\"str\") + \",\" + vacancies_copy[\"job_location_geo_latitude\"].astype(\"str\")\n",
    "vacancies_copy = vacancies_copy.join(vacancies_city.set_index(\"coords\")[\"locality\"], on=\"coords\")\n",
    "vacancies_copy[\"city\"] = vacancies_copy[\"city\"].fillna(vacancies_copy[\"locality\"])\n",
    "vacancies_copy = vacancies_copy[vacancies_copy[\"city\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_without_coord = list(vacancies_copy[vacancies_copy[\"job_location_geo_latitude\"].isna()][\"city\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:05<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "from utils.yandex_geocoder import yandex_create_urls, yandex_geocode\n",
    "\n",
    "# geocode cities to coords (to fill nan value in dataset)\n",
    "\n",
    "cities_without_coord = list(vacancies_copy[vacancies_copy[\"job_location_geo_latitude\"].isna()][\"city\"].unique())\n",
    "\n",
    "with open(\"./keys_1.txt\") as f:\n",
    "    api_keys = f.read().splitlines()\n",
    "api_keys.reverse()\n",
    "\n",
    "urls = []\n",
    "api_key_counter = 0\n",
    "for i, adr in enumerate(cities_without_coord):\n",
    "    if i%24500 == 0 and i != 0:\n",
    "        api_key_counter += 1\n",
    "    urls.append(yandex_create_urls(adr, api_keys[api_key_counter]))\n",
    "yandex_geocoded_data = yandex_geocode(urls)\n",
    "\n",
    "city_coords = pd.DataFrame(yandex_geocoded_data)\n",
    "city_coords[\"city\"] = cities_without_coord\n",
    "city_coords = city_coords[city_coords[\"yand_adr\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancies_copy = vacancies_copy.join(city_coords.set_index(\"city\")[[\"x\", \"y\"]], on=\"city\")\n",
    "vacancies_copy[\"job_location_geo_latitude\"] = vacancies_copy[\"job_location_geo_latitude\"].fillna(vacancies_copy[\"x\"])\n",
    "vacancies_copy[\"job_location_geo_longitude\"] = vacancies_copy[\"job_location_geo_longitude\"].fillna(vacancies_copy[\"y\"])\n",
    "vacancies_copy = vacancies_copy[vacancies_copy[\"job_location_geo_latitude\"].notna()]\n",
    "vacancies_copy = vacancies_copy.drop([\"coords\", \"locality\", \"x\", \"y\"], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
