{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from shapely import Point\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "MAIN_PATH = ''\n",
    "OUTPUT_DATA_PATH = f'{MAIN_PATH}output_data/'\n",
    "INPUT_DATA_PATH = f'{MAIN_PATH}input_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE 1: CLUSTERTING\n",
    "\n",
    "1. load data – responses, ueqi\n",
    "2. retrieve all locations (with coordinates) from responses\n",
    "3. multiply (repeat) the cities that are in the ueqi\n",
    "4. perform clusterisation by coordinates\n",
    "5. count cluster centers per cluster\n",
    "6. if there are clusters with more than 1 ueqi cities in them, divide them\n",
    "7. if there are clusters with no ueqi cities in them, find their centroid and merge them with the closest cluster\n",
    "8. get location-clustercenter table\n",
    "9. replace locations with their respective cluster centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. load data – responses, ueqi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = pd.read_csv(f'{OUTPUT_DATA_PATH}responses.csv')\n",
    "responses['coordinates_vacancy'] = gpd.GeoSeries.from_wkt(responses['coordinates_vacancy'])\n",
    "responses['coordinates_cv'] = gpd.GeoSeries.from_wkt(responses['coordinates_cv'])\n",
    "\n",
    "ueqi = pd.read_csv(f'{OUTPUT_DATA_PATH}ueqi.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. retrieve all locations (with coordinates) from responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Артемовск, Красноярский край', 'Горбатов, Нижегородская область'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = pd.DataFrame(np.concatenate(\n",
    "    [responses[['address_vacancy','coordinates_vacancy']].drop_duplicates().values,\n",
    "    responses[['address_cv','coordinates_cv']].drop_duplicates().values]\n",
    "    ),columns=['address','coordinates']).drop_duplicates(subset='address')\n",
    "\n",
    "locations['ueqi'] = locations['address'].map(lambda x: x in list(ueqi['address']))\n",
    "\n",
    "# Print cities present in UEQI but missing in responses:\n",
    "set(ueqi['address'])-set(locations['address'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. multiply (repeat) the cities that are in the ueqi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_rows(df,repeat_mask,repeat_n_times):\n",
    "    rows_to_repeat = df.query(f'{repeat_mask}==True')\n",
    "    res = pd.concat(\n",
    "        [\n",
    "            df.query(f'{repeat_mask}==False'),\n",
    "            rows_to_repeat.loc[rows_to_repeat.index.repeat(repeat_n_times)]\n",
    "            ]).reset_index(drop=True)\n",
    "    return res\n",
    "\n",
    "locations_weighted = repeat_rows(locations,'ueqi',2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. cluster locations by coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes approximately 5 minutes\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_kmeans(df,n_clusters=100,algorithm='lloyd',clustering_columns=['x','y']):\n",
    "    X = np.asarray(df[clustering_columns])\n",
    "    clustering = KMeans(n_clusters=n_clusters,random_state=42,algorithm=algorithm,n_init='auto')\n",
    "    df['cluster'] = clustering.fit_predict(X)\n",
    "\n",
    "    return df\n",
    "\n",
    "locations_weighted['x'] = locations_weighted['coordinates'].map(lambda x: x.x)\n",
    "locations_weighted['y'] = locations_weighted['coordinates'].map(lambda x: x.y)\n",
    "\n",
    "locations_clustered = get_kmeans(locations_weighted,n_clusters=len(locations.query('ueqi==True')))\n",
    "\n",
    "locations_clustered_copy = locations_clustered.copy()\n",
    "locations_clustered = locations_clustered.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. count cluster centers per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers_count = locations_clustered.groupby(['cluster','ueqi'])['address'].count().reset_index()\n",
    "cluster_centers_count = cluster_centers_count.drop_duplicates(subset='cluster',keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. if there are clusters with more than 1 ueqi cities in them, divide them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:02<00:00, 21.73it/s]\n"
     ]
    }
   ],
   "source": [
    "def divide_clusters(df, clusters_to_divide):\n",
    "    df = repeat_rows(df.query('cluster in @clusters_to_divide'),'ueqi',3000)\n",
    "    res = []\n",
    "    i_max = df['cluster'].max()\n",
    "\n",
    "    for i in tqdm(clusters_to_divide):\n",
    "        #print(i_max)\n",
    "        cluster = df.query(f'cluster == {i}')\n",
    "        n_clusters = len(cluster.query('ueqi == True').drop_duplicates())\n",
    "\n",
    "        new_clusters = get_kmeans(cluster,n_clusters)\n",
    "        new_cluster_indices = {\n",
    "            x:y for x,y in zip (\n",
    "                set(new_clusters['cluster']),np.arange(i_max,i_max+n_clusters))\n",
    "            }\n",
    "        i_max = i_max+n_clusters\n",
    "\n",
    "        new_clusters['cluster'] = new_clusters['cluster'].replace(new_cluster_indices)\n",
    "\n",
    "        res.append(new_clusters)\n",
    "\n",
    "    res = pd.concat(res)\n",
    "    res['cluster'] = res['cluster']+1000\n",
    "\n",
    "    return res.drop_duplicates()\n",
    "\n",
    "clusters_to_divide = list(cluster_centers_count.query('ueqi==True and address>1')['cluster'])\n",
    "new_clusters = divide_clusters(locations_clustered,clusters_to_divide)\n",
    "locations_clustered = pd.concat([locations_clustered.query('cluster not in @clusters_to_divide'),new_clusters],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. if there are clusters with no ueqi cities in them, find their centroid and merge them with the closest cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/65 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:02<00:00, 25.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine(point1, point2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "\n",
    "    lon1 = point1.x\n",
    "    lat1 = point1.y\n",
    "    lon2 = point2.x\n",
    "    lat2 = point2.y\n",
    "\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    return c * r\n",
    "\n",
    "def merge_clusters(df,clusters_to_merge):\n",
    "    addresses = list(df.query('ueqi==True')['address'])\n",
    "    coordinates = [Point(x,y) for x,y in zip(df.query('ueqi==True')['x'],df.query('ueqi==True')['y'])]\n",
    "    location_coordinates_dict = {x:y for x,y in zip(addresses,coordinates)}\n",
    "\n",
    "    cluster_centroids = {}\n",
    "    closest_city = {}\n",
    "    merge_clusters_dict = {}\n",
    "\n",
    "    for x in tqdm(clusters_to_merge):\n",
    "        try:\n",
    "            cluster_centroids[x] = Polygon(df.query('cluster==@x')['coordinates']).centroid\n",
    "        except:\n",
    "            cluster_centroids[x] = df.query('cluster==@x')['coordinates'].iloc[0]\n",
    "\n",
    "        #print(cluster_centroids[x],x)\n",
    "        i = np.argmin([haversine(cluster_centroids[x],location_coordinates_dict[y]) for y in location_coordinates_dict])\n",
    "        closest_city[x] = [*location_coordinates_dict.keys()][i]\n",
    "\n",
    "    for x in closest_city:\n",
    "        merge_clusters_dict[x] = df.loc[df['address']==closest_city[x],'cluster'].iloc[0]\n",
    "\n",
    "    df['cluster'] = df['cluster'].replace(merge_clusters_dict)\n",
    "\n",
    "    return df\n",
    "\n",
    "clusters_to_merge = list(cluster_centers_count.query('ueqi==False')['cluster'])\n",
    "locations_clustered = merge_clusters(locations_clustered,clusters_to_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. get location-clustercenter table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = locations_clustered.groupby(['cluster','ueqi'])['address'].first().reset_index()\n",
    "cluster_centers = cluster_centers.query('ueqi==True').drop('ueqi',axis=1)\n",
    "\n",
    "cluster_centers_dict = {x:y for x,y in zip(cluster_centers['cluster'],cluster_centers['address'])}\n",
    "location_cluster_dict = {x:y for x,y in zip(locations_clustered['address'],locations_clustered['cluster'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. replace locations with their respective cluster centers in the responses table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses['cluster_cv'] = responses['address_cv'].map(lambda x: location_cluster_dict[x])\n",
    "responses['cluster_vacancy'] = responses['address_vacancy'].map(lambda x: location_cluster_dict[x])\n",
    "\n",
    "responses['cluster_center_cv'] = responses['cluster_cv'].map(lambda x: cluster_centers_dict[x])\n",
    "responses['cluster_center_vacancy'] = responses['cluster_vacancy'].map(lambda x: cluster_centers_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.to_csv(f'{OUTPUT_DATA_PATH}responses_clustered.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAGE 2: AGGREGATING\n",
    "\n",
    "1. load data – responses_clustered, specialists, ontology, ueqi\n",
    "2. group responses by cluster_center_cv and cluster_center_vacancy to get: \n",
    "    - distance\n",
    "    - migrations_total\n",
    "3. from the groupped table, get all locations and evaluate [to get cities_agg table]:\n",
    "    - domestic_migrations_count (incoming, when cluster_center_cv==cluster_center_vacancy)\n",
    "    - incoming_migrations_count (migrations_total - domestic_migrations_count)\n",
    "    - outcoming_migrations_count (migrations_total - domestic_migrations_count)\n",
    "4. merge specialists_table with ontology and replace speciality columnn with dummy columns\n",
    "5. merge responses with the resulting table\n",
    "6. group responses by cluster_center_vacancy and domains to evaluate the number of opened vacancies per domain in cities\n",
    "7. merge the resulting groupped table with the groupped table from table 3\n",
    "8. merge cities_agg with ueqi\n",
    "9. merge cities_agg with population from city_info\n",
    "10. compute additional variables:\n",
    "    - domestic_migrations_count_rel (domestic_migrations_count/population)\n",
    "    - incoming_migrations_count_rel (incoming_migrations_count/population)\n",
    "    - outcoming_migrations_count_rel (outcoming_migrations_count/population)\n",
    "    - migration_ratio (incoming_migrations_count/outcoming_migrations_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr,spearmanr,kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_matching = {'Оператор производственной линии':'Оператор, аппаратчик',\n",
    "                     'Оператор станков с ЧПУ':'Оператор, аппаратчик',\n",
    "                     'Монтажник':'Монтажник',\n",
    "                     'Машинист':'Машинист',\n",
    "                     'Мастер по ремонту оборудования, техники':'Мастер по ремонту оборудования',\n",
    "                     'Разнорабочий':'Разнорабочий',\n",
    "                     'Слесарь, сантехник':'Слесарь',\n",
    "                     'Геодезист':'Геодезист',\n",
    "                     'Геолог':'Геолог',\n",
    "                     'Технолог':'Технолог',\n",
    "                     'Инженер по охране труда и технике безопасности, инженер-эколог':'Инженер-эколог',\n",
    "                     'Инженер-конструктор, инженер-проектировщик':'Инженер-конструктор',\n",
    "                     'Сварщик':'Сварщик', \n",
    "                     'Инженер-электроник, инженер-электронщик':'Инженер-проектировщик',\n",
    "                     'Инженер ПНР':'Наладчик',\n",
    "                     'Токарь, фрезеровщик, шлифовщик':'Токарь, фрезеровщик, шлифовщик', \n",
    "                     'Контролёр ОТК':'Контролер ОТК',\n",
    "                     #'Инженер-конструктор, инженер-проектировщик':'Инженер-проектировщик', \n",
    "                     'Специалист по сертификации':'Специалист по сертификации',\n",
    "                     'Инженер по качеству':'Инженер по качеству', \n",
    "                     'Механик':'Механик',\n",
    "                     'Инженер по эксплуатации':'Инженер по эксплуатации',\n",
    "                     'Лаборант':'Лаборант', \n",
    "                     'Научный специалист, исследователь':'Исследователь', \n",
    "                     'Электромонтажник':'Электромонтажник',\n",
    "                     'Инженер ПТО, инженер-сметчик':'Инженер-проектировщик',\n",
    "                     #'Инженер по охране труда и технике безопасности, инженер-эколог':'Инженер по охране труда и технике безопасности',\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df = salary_df.replace(0,np.nan)\n",
    "\n",
    "salary_info = salary_df.groupby(['cluster_center_vacancy'])[industries].median().reset_index()\n",
    "salary_info = salary_info.rename(columns=industries_translation)\n",
    "salary_info.columns = ['salary_'+x if x != 'cluster_center_vacancy' else x for x in salary_info.columns]\n",
    "\n",
    "vacancy_count = salary_df.groupby(['cluster_center_vacancy'])[industries].count().reset_index()\n",
    "vacancy_count = vacancy_count.rename(columns=industries_translation)\n",
    "vacancy_count.columns = ['vacancy_count_'+x if x != 'cluster_center_vacancy' else x for x in vacancy_count.columns ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. load data – responses_clustered, specialists, ontology, ueqi, city_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology = pd.read_csv(f'{OUTPUT_DATA_PATH}ontology.csv')\n",
    "specialists = pd.read_csv(f'{OUTPUT_DATA_PATH}specialists.csv')\n",
    "city_info = pd.read_csv(f'{OUTPUT_DATA_PATH}city_info.csv')\n",
    "city_info['address'] = city_info['address'].replace({'Дмитриев-Льговский, Курская область':'Дмитриев, Курская область'})\n",
    "city_info = city_info.dropna(subset='population')\n",
    "city_info['address'] = city_info['address'].str.replace('ё','е')\n",
    "city_info = city_info.drop_duplicates(subset='address')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "industries = ['Авиастроение и космическая отрасль', 'Добыча нефти и газа',\n",
    "              'Добыча угля и металлических руд', 'Машиностроение', 'Судостроение',\n",
    "              'Фармацевтика', 'Химическая промышленность', 'Цветная металлургия',\n",
    "              'Черная металлургия', 'Электроника и микроэлектроника']\n",
    "\n",
    "industries_translation = {'Авиастроение и космическая отрасль':'aircraft_and_space',\n",
    "                          'Добыча нефти и газа':'oil_and_gas_ext',\n",
    "                          'Добыча угля и металлических руд':'coal_and_metal_ext',\n",
    "                          'Машиностроение':'machinery',\n",
    "                          'Судостроение':'shipbuilding',\n",
    "                          'Фармацевтика':'pharmacy',\n",
    "                          'Химическая промышленность':'chemicals',\n",
    "                          'Цветная металлургия':'nonferrous_metallurgy',\n",
    "                          'Черная металлургия':'ferrous_metallurgy',\n",
    "                          'Электроника и микроэлектроника':'electronics'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. group responses by cluster_center_cv and cluster_center_vacancy to get distance and migrations_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "migrations = responses.groupby(['cluster_center_cv','cluster_center_vacancy'])['id_response'].count().rename('migrations_total').reset_index()\n",
    "\n",
    "migrations_temp = responses.groupby(['cluster_center_cv','cluster_center_vacancy']).agg(\n",
    "    {'coordinates_cv':'first','coordinates_vacancy':'first'}).reset_index()\n",
    "#migrations_temp['distance'] = migrations_temp.apply(lambda x:haversine(x.coordinates_cv,x.coordinates_vacancy),axis=1)\n",
    "\n",
    "migrations = migrations.merge(migrations_temp[['cluster_center_cv','cluster_center_vacancy','distance']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. from the groupped table, get all locations and evaluate domestic_migrations_count, incoming_migrations_count, and outcoming_migrations_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_agg = migrations.query('cluster_center_cv==cluster_center_vacancy').drop_duplicates(\n",
    "    subset=['cluster_center_vacancy'])[['cluster_center_cv','migrations_total']]\n",
    "\n",
    "a = set(list(migrations['cluster_center_cv'])+list(migrations['cluster_center_vacancy']))\n",
    "b = set(cities_agg['cluster_center_cv'])\n",
    "cities_agg = pd.DataFrame(np.concatenate([cities_agg.values,[[x,0] for x in list(a-b)]]),columns=['city','migrations_domestic'])\n",
    "cities_agg['migrations_domestic']= cities_agg['migrations_domestic'].astype(int)\n",
    "\n",
    "cities_agg = cities_agg.merge(migrations.groupby('cluster_center_vacancy')['migrations_total'].sum().reset_index().rename(\n",
    "    columns={'cluster_center_vacancy':'city','migrations_total':'migrations_incoming'}),how='left').fillna(0)\n",
    "cities_agg['migrations_incoming'] = cities_agg['migrations_incoming']-cities_agg['migrations_domestic']\n",
    "cities_agg['migrations_incoming'] = cities_agg['migrations_incoming'].astype(int)\n",
    "\n",
    "cities_agg = cities_agg.merge(migrations.groupby('cluster_center_cv')['migrations_total'].sum().reset_index().rename(\n",
    "    columns={'cluster_center_cv':'city','migrations_total':'migrations_outcoming'}),how='left').fillna(0)\n",
    "cities_agg['migrations_outcoming'] = cities_agg['migrations_outcoming']-cities_agg['migrations_domestic']\n",
    "cities_agg['migrations_outcoming'] = cities_agg['migrations_outcoming'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. merge specialists with ontology and replace speciality columnn with dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialists = specialists.merge(ontology[['speciality','prof_domain']],how='left')\n",
    "specialists = pd.concat([specialists,pd.get_dummies(specialists['prof_domain'])],axis=1).drop(\n",
    "    'prof_domain',axis=1).groupby(['id_candidate','id_cv','speciality']).max().reset_index()\n",
    "#specialists = specialists.rename(columns=industries_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. merge responses with the resulting table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_domains = responses.merge(specialists,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. group responses by cluster_center_vacancy and domains to evaluate the number of opened vacancies per domain in cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology['prof_domain'] = ontology['prof_domain'].replace(industries_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = list(set(ontology['prof_domain']))\n",
    "city_responses_per_domain = responses_domains.groupby('cluster_center_vacancy').agg({x:'sum' for x in domains}).replace(\n",
    "    {True:1,False:0}).reset_index().rename(columns={'cluster_center_vacancy':'city'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_responses_per_domain.columns = ['responses_'+x if x not in 'city' else x for x in city_responses_per_domain.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. merge the resulting groupped table with the cities_agg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_agg = cities_agg.merge(city_responses_per_domain,how='left')\n",
    "\n",
    "for x in city_responses_per_domain.drop('city',axis=1).columns:\n",
    "    cities_agg[x] = cities_agg[x].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. merge cities_agg with ueqi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_agg = cities_agg.merge(ueqi.drop(['city','region'],axis=1),left_on='city',right_on='address',how='left').drop_duplicates(\n",
    "    subset=['city']).drop('address',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_agg = cities_agg.rename(columns={'greens_spaces':'green_spaces'})\n",
    "cities_agg = cities_agg.rename(columns={'residential':'ueqi_residential',\n",
    "                           'street_networks':'ueqi_street_networks',\n",
    "                           'green_spaces':'ueqi_green_spaces',\n",
    "                           'public_and_business_infrastructure':'ueqi_public_and_business_infrastructure',\n",
    "                           'social_and_leisure_infrastructure':'ueqi_social_and_leisure_infrastructure',\n",
    "                           'citywide_space':'ueqi_citywide_space'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. merge cities_agg with population from city_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_agg = cities_agg.merge(city_info[['address','population']],how='left',left_on='city',right_on='address').drop('address',axis=1)\n",
    "cities_agg['population'] = cities_agg['population'].astype(int)\n",
    "cities_agg = cities_agg.merge(locations.drop('ueqi',axis=1),how='left',left_on='city',right_on='address')\n",
    "cities_agg = cities_agg.drop('address',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. compute additional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_migration_coeff(incoming_migrations,outcoming_migrations):\n",
    "    if outcoming_migrations > incoming_migrations:\n",
    "        try:\n",
    "            res = -(outcoming_migrations/incoming_migrations-1)\n",
    "        except:\n",
    "            res = -np.inf\n",
    "    else:\n",
    "        try:\n",
    "            res = incoming_migrations/outcoming_migrations-1\n",
    "        except:\n",
    "            res = np.inf\n",
    "\n",
    "    return res\n",
    "\n",
    "cities_agg['migrations_rel_domestic'] = cities_agg['migrations_domestic']/cities_agg['population']\n",
    "cities_agg['migrations_rel_incoming'] = cities_agg['migrations_incoming']/cities_agg['population']\n",
    "cities_agg['migrations_rel_outcoming'] = cities_agg['migrations_outcoming']/cities_agg['population']\n",
    "cities_agg['migration_coeff'] = [get_migration_coeff(x,y) for x,y in zip(\n",
    "    cities_agg['migrations_incoming'],\n",
    "    cities_agg['migrations_outcoming'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_agg.to_csv(f'{OUTPUT_DATA_PATH}cities_aggregated.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
