{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "from geopy.distance import great_circle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MigrationDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(MigrationDataset, self).__init__(root, transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        files = []\n",
    "        for file in os.listdir(self.root):\n",
    "            if file.endswith(\".geojson\") or file.endswith(\".csv\"):\n",
    "                files.append(file)\n",
    "        return files\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['migration_dataset']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        \n",
    "        # read files in specified folder\n",
    "        cities = gpd.read_file(os.path.join(self.root, \"cities_aggregated.geojson\")).set_index(\"city\")\n",
    "        responses = gpd.read_file(os.path.join(self.root, \"responses_clustered.csv\"))\n",
    "\n",
    "\n",
    "        # extract cities features \n",
    "        cities_features = cities[[\n",
    "            \"population\",\n",
    "            \"city_category\", \n",
    "            \"harsh_climate\", \n",
    "            \"ueqi_score\", \n",
    "            \"ueqi_residential\", \n",
    "            \"ueqi_street_networks\", \n",
    "            \"ueqi_green_spaces\", \n",
    "            \"ueqi_public_and_business_infrastructure\", \n",
    "            \"ueqi_social_and_leisure_infrastructure\",\n",
    "            \"ueqi_citywide_space\",\n",
    "            \"cvs_total\",\n",
    "            \"vacancies_total\",\n",
    "            \"factories_count\"\n",
    "            ]]\n",
    "\n",
    "        # encode categorical features\n",
    "        one_hot = OneHotEncoder()\n",
    "        encoded_category = one_hot.fit_transform(np.expand_dims(cities[\"city_category\"].to_numpy(), 1)).toarray()\n",
    "        encoded_category_names = one_hot.get_feature_names_out([\"category\"])\n",
    "        cities_features.loc[:, encoded_category_names] = encoded_category\n",
    "        cities_features = cities_features.drop([\"city_category\"], axis=1)\n",
    "        cities_features[\"harsh_climate\"] = cities_features[\"harsh_climate\"].astype(int)\n",
    "\n",
    "        # form distance matrix\n",
    "        DM = cities[\"geometry\"].progress_apply(\n",
    "            lambda p1: cities[\"geometry\"].apply(\n",
    "                lambda p2: great_circle(p1.coords[0], p2.coords[0]).km\n",
    "                ))\n",
    "\n",
    "        # form origin-destination matrix\n",
    "\n",
    "        responses_counts = responses.groupby([\"cluster_center_cv\", \"cluster_center_vacancy\"])[\"id_candidate\"].count()\n",
    "        responses_cities = responses_counts.index.get_level_values(0).drop_duplicates()\n",
    "        OD = pd.DataFrame(None, index=DM.columns, columns=DM.columns)\n",
    "        OD = OD.progress_apply(\n",
    "            lambda city: city.fillna(responses_counts[city.name]).fillna(0) \n",
    "            if city.name in responses_cities else city.fillna(0)\n",
    "            )\n",
    "        \n",
    "        # transform data\n",
    "        \n",
    "        cities_num = len(OD)\n",
    "        edge_index = [[], []]\n",
    "        for i in range(cities_num):\n",
    "            edge_index[0].extend([i for j in range(cities_num)])\n",
    "            edge_index[1].extend([j for j in range(cities_num)])\n",
    "\n",
    "        edge_index = torch.tensor(edge_index)\n",
    "        y = torch.tensor(np.concatenate((OD.to_numpy())), dtype=torch.float32)\n",
    "        edge_attr = torch.tensor(np.concatenate((DM.to_numpy())), dtype=torch.float32)\n",
    "        x = torch.tensor(cities_features.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "        # exclude diagonal\n",
    "        non_diagonal = edge_attr > 0\n",
    "        edge_attr = edge_attr[non_diagonal]\n",
    "        edge_index = edge_index[:, non_diagonal]\n",
    "        y = y[non_diagonal]\n",
    "        \n",
    "        # create torch object          \n",
    "        graph = Data(x=x,edge_index=edge_index, y=y, edge_attr=edge_attr)\n",
    "        \n",
    "        data_list = []\n",
    "        data_list.append(graph)\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be found here: https://disk.yandex.ru/client/disk/%D0%A2%D1%80%D1%83%D0%B4%D0%BE%D0%B2%D1%8B%D0%B5%20%D1%80%D0%B5%D1%81%D1%83%D1%80%D1%81%D1%8B/%D0%A4%D0%B0%D0%B9%D0%BB%D1%8B/ML_experiments/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MigrationDataset(\"/var/essdata/IDU/other/mm_22/industrial-location/ml/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super(FNN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.lins.append(nn.Linear(input_dim, hidden_dim))\n",
    "        for _ in range(self.num_layers - 2):\n",
    "            self.lins.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.lins.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.norm = nn.ModuleList()\n",
    "        for l in range(self.num_layers):\n",
    "            self.norm.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "\n",
    "        edge_weight = edge_attr.unsqueeze(-1)\n",
    "\n",
    "        x_s = x[edge_index[0]]\n",
    "        x_d = x[edge_index[1]]\n",
    "        y = torch.cat((x_s, x_d, edge_weight), axis=1)\n",
    "        # y = F.normalize(y)\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            y = self.lins[i](y) \n",
    "            y = nn.functional.leaky_relu(y)\n",
    "            y = F.dropout(y, p=self.dropout, training=self.training)\n",
    "            y = self.norm[i](y)\n",
    "\n",
    "        y = self.lins[-1](y)\n",
    "        y = torch.relu(y).squeeze()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_loss(output, target):\n",
    "    target_mean = torch.mean(target)\n",
    "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
    "    ss_res = torch.sum((target - output) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2\n",
    "\n",
    "def train_func(data, model, epochs, writer):\n",
    "\n",
    "    optimize = torch.optim.Adam(list(model.parameters()),  lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimize, factor=0.9, min_lr=0.0005)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(epochs + 1):\n",
    "\n",
    "        optimize.zero_grad()\n",
    "        model.train()\n",
    "        y_train = data.y[data.train_idx]\n",
    "        y_hat = model(data.x, data.edge_index[:, data.train_idx], data.edge_attr[data.train_idx])\n",
    "\n",
    "        loss = F.mse_loss(y_hat, y_train)\n",
    "        r2 = r2_loss(y_hat, y_train)\n",
    "\n",
    "        loss.backward()\n",
    "        optimize.step()\n",
    "\n",
    "        t_metrics = {\"train_loss\": loss, \"train_r2\": r2}  \n",
    "        for name, t_metric in t_metrics.items(): writer.add_scalar(name, t_metric, epoch)\n",
    "\n",
    "        if epoch % 10 == 0: \n",
    "\n",
    "            v_metrics = val_func(data, model)\n",
    "            scheduler.step(v_metrics[\"test_loss\"])\n",
    "            for name, v_metric in v_metrics.items(): writer.add_scalar(name, v_metric, epoch)\n",
    "\n",
    "            print(\n",
    "                \"Epoch {}. TRAIN: loss {:.4f}, r2: {:.4f}. \".format(\n",
    "                    epoch, t_metrics[\"train_loss\"], t_metrics[\"train_r2\"]\n",
    "                    ) + \\\n",
    "                \"TEST: loss {:.4f}, r2: {:.4f}. lr: {:.4f} \".format(\n",
    "                    v_metrics[\"test_loss\"], v_metrics[\"test_r2\"], optimize.param_groups[0][\"lr\"]\n",
    "                )\n",
    "            ) \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def val_func(data, model):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        y_test = data.y[data.test_idx]\n",
    "        y_hat = model(data.x, data.edge_index[:, data.test_idx], data.edge_attr[data.test_idx])\n",
    "\n",
    "        loss = F.mse_loss(y_hat, y_test)\n",
    "        r2 = r2_loss(y_hat, y_test)\n",
    "\n",
    "        return {\"test_loss\": loss, \"test_r2\": r2} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to the train and test set\n",
    "data = dataset.data\n",
    "perm = torch.randperm(data.num_edges)\n",
    "data.train_idx = perm[:int(0.7 * data.num_edges)]\n",
    "data.test_idx = perm[int(0.7 * data.num_edges):]\n",
    "\n",
    "# set parameters\n",
    "input_dim = data.x.shape[1] * 2 + 1\n",
    "hidden_dim = 256\n",
    "num_layers = 5\n",
    "dropout = 0.2\n",
    "\n",
    "model_v2 = FNN(input_dim, hidden_dim, num_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. TRAIN: loss 453.2109, r2: -0.0007. TEST: loss 260.4305, r2: -0.0168. lr: 0.0010 \n",
      "Epoch 10. TRAIN: loss 452.5634, r2: 0.0008. TEST: loss 255.5312, r2: 0.0023. lr: 0.0010 \n",
      "Epoch 20. TRAIN: loss 452.1676, r2: 0.0017. TEST: loss 255.9202, r2: 0.0008. lr: 0.0010 \n",
      "Epoch 30. TRAIN: loss 451.8819, r2: 0.0023. TEST: loss 255.0395, r2: 0.0042. lr: 0.0010 \n",
      "Epoch 40. TRAIN: loss 451.3644, r2: 0.0034. TEST: loss 254.2294, r2: 0.0074. lr: 0.0010 \n",
      "Epoch 50. TRAIN: loss 450.6646, r2: 0.0050. TEST: loss 253.0377, r2: 0.0120. lr: 0.0010 \n",
      "Epoch 60. TRAIN: loss 449.7331, r2: 0.0070. TEST: loss 251.8439, r2: 0.0167. lr: 0.0010 \n",
      "Epoch 70. TRAIN: loss 448.3998, r2: 0.0100. TEST: loss 251.3926, r2: 0.0185. lr: 0.0010 \n",
      "Epoch 80. TRAIN: loss 446.9092, r2: 0.0133. TEST: loss 249.1868, r2: 0.0271. lr: 0.0010 \n",
      "Epoch 90. TRAIN: loss 445.3583, r2: 0.0167. TEST: loss 247.8882, r2: 0.0321. lr: 0.0010 \n",
      "Epoch 100. TRAIN: loss 444.0997, r2: 0.0195. TEST: loss 246.6770, r2: 0.0369. lr: 0.0010 \n",
      "Epoch 110. TRAIN: loss 441.9966, r2: 0.0241. TEST: loss 245.3811, r2: 0.0419. lr: 0.0010 \n",
      "Epoch 120. TRAIN: loss 440.2574, r2: 0.0279. TEST: loss 243.7517, r2: 0.0483. lr: 0.0010 \n",
      "Epoch 130. TRAIN: loss 438.5484, r2: 0.0317. TEST: loss 241.7938, r2: 0.0559. lr: 0.0010 \n",
      "Epoch 140. TRAIN: loss 437.4552, r2: 0.0341. TEST: loss 241.0538, r2: 0.0588. lr: 0.0010 \n",
      "Epoch 150. TRAIN: loss 435.0765, r2: 0.0394. TEST: loss 240.1165, r2: 0.0625. lr: 0.0010 \n",
      "Epoch 160. TRAIN: loss 432.9783, r2: 0.0440. TEST: loss 238.3940, r2: 0.0692. lr: 0.0010 \n",
      "Epoch 170. TRAIN: loss 431.6590, r2: 0.0469. TEST: loss 235.5110, r2: 0.0805. lr: 0.0010 \n",
      "Epoch 180. TRAIN: loss 428.7259, r2: 0.0534. TEST: loss 233.8001, r2: 0.0871. lr: 0.0010 \n",
      "Epoch 190. TRAIN: loss 426.6572, r2: 0.0580. TEST: loss 233.1445, r2: 0.0897. lr: 0.0010 \n",
      "Epoch 200. TRAIN: loss 424.6423, r2: 0.0624. TEST: loss 231.1265, r2: 0.0976. lr: 0.0010 \n",
      "Epoch 210. TRAIN: loss 422.1486, r2: 0.0679. TEST: loss 228.9497, r2: 0.1061. lr: 0.0010 \n",
      "Epoch 220. TRAIN: loss 420.1375, r2: 0.0724. TEST: loss 227.7293, r2: 0.1108. lr: 0.0010 \n",
      "Epoch 230. TRAIN: loss 417.8770, r2: 0.0774. TEST: loss 226.3481, r2: 0.1162. lr: 0.0010 \n",
      "Epoch 240. TRAIN: loss 416.0272, r2: 0.0814. TEST: loss 224.8759, r2: 0.1220. lr: 0.0010 \n",
      "Epoch 250. TRAIN: loss 413.7116, r2: 0.0866. TEST: loss 224.1907, r2: 0.1247. lr: 0.0010 \n",
      "Epoch 260. TRAIN: loss 411.8939, r2: 0.0906. TEST: loss 222.6508, r2: 0.1307. lr: 0.0010 \n",
      "Epoch 270. TRAIN: loss 410.0776, r2: 0.0946. TEST: loss 221.9751, r2: 0.1333. lr: 0.0010 \n",
      "Epoch 280. TRAIN: loss 408.1841, r2: 0.0988. TEST: loss 221.6792, r2: 0.1345. lr: 0.0010 \n",
      "Epoch 290. TRAIN: loss 406.4712, r2: 0.1025. TEST: loss 219.9571, r2: 0.1412. lr: 0.0010 \n",
      "Epoch 300. TRAIN: loss 405.2590, r2: 0.1052. TEST: loss 219.4375, r2: 0.1432. lr: 0.0010 \n",
      "Epoch 310. TRAIN: loss 410.6581, r2: 0.0933. TEST: loss 218.5628, r2: 0.1466. lr: 0.0010 \n",
      "Epoch 320. TRAIN: loss 403.3810, r2: 0.1094. TEST: loss 217.7208, r2: 0.1499. lr: 0.0010 \n",
      "Epoch 330. TRAIN: loss 402.8091, r2: 0.1106. TEST: loss 217.1638, r2: 0.1521. lr: 0.0010 \n",
      "Epoch 340. TRAIN: loss 399.2885, r2: 0.1184. TEST: loss 212.3222, r2: 0.1710. lr: 0.0010 \n",
      "Epoch 350. TRAIN: loss 397.4702, r2: 0.1224. TEST: loss 211.8763, r2: 0.1727. lr: 0.0010 \n",
      "Epoch 360. TRAIN: loss 393.7851, r2: 0.1306. TEST: loss 208.8815, r2: 0.1844. lr: 0.0010 \n",
      "Epoch 370. TRAIN: loss 392.5816, r2: 0.1332. TEST: loss 206.1591, r2: 0.1951. lr: 0.0010 \n",
      "Epoch 380. TRAIN: loss 388.6394, r2: 0.1419. TEST: loss 203.1622, r2: 0.2068. lr: 0.0010 \n",
      "Epoch 390. TRAIN: loss 384.8326, r2: 0.1503. TEST: loss 210.4817, r2: 0.1782. lr: 0.0010 \n",
      "Epoch 400. TRAIN: loss 382.5227, r2: 0.1554. TEST: loss 200.6666, r2: 0.2165. lr: 0.0010 \n",
      "Epoch 410. TRAIN: loss 379.1048, r2: 0.1630. TEST: loss 197.9825, r2: 0.2270. lr: 0.0010 \n",
      "Epoch 420. TRAIN: loss 377.1446, r2: 0.1673. TEST: loss 196.7843, r2: 0.2317. lr: 0.0010 \n",
      "Epoch 430. TRAIN: loss 373.9036, r2: 0.1745. TEST: loss 195.2355, r2: 0.2377. lr: 0.0010 \n",
      "Epoch 440. TRAIN: loss 370.7872, r2: 0.1813. TEST: loss 193.9313, r2: 0.2428. lr: 0.0010 \n",
      "Epoch 450. TRAIN: loss 368.8086, r2: 0.1857. TEST: loss 191.8392, r2: 0.2510. lr: 0.0010 \n",
      "Epoch 460. TRAIN: loss 365.4861, r2: 0.1930. TEST: loss 190.8177, r2: 0.2550. lr: 0.0010 \n",
      "Epoch 470. TRAIN: loss 363.1725, r2: 0.1981. TEST: loss 189.2153, r2: 0.2612. lr: 0.0010 \n",
      "Epoch 480. TRAIN: loss 360.4225, r2: 0.2042. TEST: loss 188.2095, r2: 0.2651. lr: 0.0010 \n",
      "Epoch 490. TRAIN: loss 357.8531, r2: 0.2099. TEST: loss 186.8031, r2: 0.2706. lr: 0.0010 \n",
      "Epoch 500. TRAIN: loss 355.5735, r2: 0.2149. TEST: loss 186.7250, r2: 0.2709. lr: 0.0010 \n",
      "Epoch 510. TRAIN: loss 354.0154, r2: 0.2184. TEST: loss 183.4233, r2: 0.2838. lr: 0.0010 \n",
      "Epoch 520. TRAIN: loss 350.7534, r2: 0.2256. TEST: loss 181.6800, r2: 0.2906. lr: 0.0010 \n",
      "Epoch 530. TRAIN: loss 348.2846, r2: 0.2310. TEST: loss 179.9148, r2: 0.2975. lr: 0.0010 \n",
      "Epoch 540. TRAIN: loss 346.5587, r2: 0.2348. TEST: loss 179.4597, r2: 0.2993. lr: 0.0010 \n",
      "Epoch 550. TRAIN: loss 343.1974, r2: 0.2422. TEST: loss 177.6732, r2: 0.3063. lr: 0.0010 \n",
      "Epoch 560. TRAIN: loss 341.5416, r2: 0.2459. TEST: loss 176.3704, r2: 0.3114. lr: 0.0010 \n",
      "Epoch 570. TRAIN: loss 339.7809, r2: 0.2498. TEST: loss 175.1297, r2: 0.3162. lr: 0.0010 \n",
      "Epoch 580. TRAIN: loss 336.5035, r2: 0.2570. TEST: loss 172.8378, r2: 0.3252. lr: 0.0010 \n",
      "Epoch 590. TRAIN: loss 334.4282, r2: 0.2616. TEST: loss 172.6795, r2: 0.3258. lr: 0.0010 \n",
      "Epoch 600. TRAIN: loss 332.5335, r2: 0.2658. TEST: loss 170.2433, r2: 0.3353. lr: 0.0010 \n",
      "Epoch 610. TRAIN: loss 330.3796, r2: 0.2705. TEST: loss 170.0869, r2: 0.3359. lr: 0.0010 \n",
      "Epoch 620. TRAIN: loss 327.5750, r2: 0.2767. TEST: loss 168.5003, r2: 0.3421. lr: 0.0010 \n",
      "Epoch 630. TRAIN: loss 325.3899, r2: 0.2816. TEST: loss 167.2997, r2: 0.3468. lr: 0.0010 \n",
      "Epoch 640. TRAIN: loss 324.1473, r2: 0.2843. TEST: loss 168.2535, r2: 0.3431. lr: 0.0010 \n",
      "Epoch 650. TRAIN: loss 321.5218, r2: 0.2901. TEST: loss 164.7981, r2: 0.3566. lr: 0.0010 \n",
      "Epoch 660. TRAIN: loss 320.1818, r2: 0.2931. TEST: loss 165.4062, r2: 0.3542. lr: 0.0010 \n",
      "Epoch 670. TRAIN: loss 317.4207, r2: 0.2992. TEST: loss 161.9246, r2: 0.3678. lr: 0.0010 \n",
      "Epoch 680. TRAIN: loss 314.8158, r2: 0.3049. TEST: loss 162.3176, r2: 0.3662. lr: 0.0010 \n",
      "Epoch 690. TRAIN: loss 313.4378, r2: 0.3080. TEST: loss 160.5309, r2: 0.3732. lr: 0.0010 \n",
      "Epoch 700. TRAIN: loss 311.5459, r2: 0.3121. TEST: loss 159.1738, r2: 0.3785. lr: 0.0010 \n",
      "Epoch 710. TRAIN: loss 308.6047, r2: 0.3186. TEST: loss 158.8669, r2: 0.3797. lr: 0.0010 \n",
      "Epoch 720. TRAIN: loss 307.0002, r2: 0.3222. TEST: loss 157.2844, r2: 0.3859. lr: 0.0010 \n",
      "Epoch 730. TRAIN: loss 307.5777, r2: 0.3209. TEST: loss 159.3529, r2: 0.3778. lr: 0.0010 \n",
      "Epoch 740. TRAIN: loss 307.4722, r2: 0.3211. TEST: loss 154.8529, r2: 0.3954. lr: 0.0010 \n",
      "Epoch 750. TRAIN: loss 306.6720, r2: 0.3229. TEST: loss 155.3566, r2: 0.3934. lr: 0.0010 \n",
      "Epoch 760. TRAIN: loss 303.2514, r2: 0.3304. TEST: loss 154.6759, r2: 0.3961. lr: 0.0010 \n",
      "Epoch 770. TRAIN: loss 300.5046, r2: 0.3365. TEST: loss 153.5527, r2: 0.4005. lr: 0.0010 \n",
      "Epoch 780. TRAIN: loss 297.1227, r2: 0.3440. TEST: loss 151.9478, r2: 0.4067. lr: 0.0010 \n",
      "Epoch 790. TRAIN: loss 295.2480, r2: 0.3481. TEST: loss 148.4485, r2: 0.4204. lr: 0.0010 \n",
      "Epoch 800. TRAIN: loss 293.8038, r2: 0.3513. TEST: loss 150.7421, r2: 0.4114. lr: 0.0010 \n",
      "Epoch 810. TRAIN: loss 292.1588, r2: 0.3549. TEST: loss 150.2768, r2: 0.4133. lr: 0.0010 \n",
      "Epoch 820. TRAIN: loss 290.3899, r2: 0.3588. TEST: loss 149.3153, r2: 0.4170. lr: 0.0010 \n",
      "Epoch 830. TRAIN: loss 290.5085, r2: 0.3586. TEST: loss 148.6885, r2: 0.4195. lr: 0.0010 \n",
      "Epoch 840. TRAIN: loss 286.2012, r2: 0.3681. TEST: loss 148.7359, r2: 0.4193. lr: 0.0010 \n",
      "Epoch 850. TRAIN: loss 284.3594, r2: 0.3722. TEST: loss 147.2501, r2: 0.4251. lr: 0.0010 \n",
      "Epoch 860. TRAIN: loss 283.2646, r2: 0.3746. TEST: loss 146.8580, r2: 0.4266. lr: 0.0010 \n",
      "Epoch 870. TRAIN: loss 282.3074, r2: 0.3767. TEST: loss 143.3887, r2: 0.4401. lr: 0.0010 \n",
      "Epoch 880. TRAIN: loss 283.2288, r2: 0.3747. TEST: loss 146.8059, r2: 0.4268. lr: 0.0010 \n",
      "Epoch 890. TRAIN: loss 281.7269, r2: 0.3780. TEST: loss 141.4915, r2: 0.4476. lr: 0.0010 \n",
      "Epoch 900. TRAIN: loss 278.0153, r2: 0.3862. TEST: loss 145.1299, r2: 0.4334. lr: 0.0010 \n",
      "Epoch 910. TRAIN: loss 275.9076, r2: 0.3908. TEST: loss 143.4207, r2: 0.4400. lr: 0.0010 \n",
      "Epoch 920. TRAIN: loss 275.1262, r2: 0.3925. TEST: loss 138.3420, r2: 0.4599. lr: 0.0010 \n",
      "Epoch 930. TRAIN: loss 274.3015, r2: 0.3944. TEST: loss 138.6697, r2: 0.4586. lr: 0.0010 \n",
      "Epoch 940. TRAIN: loss 272.9390, r2: 0.3974. TEST: loss 141.0941, r2: 0.4491. lr: 0.0010 \n",
      "Epoch 950. TRAIN: loss 276.4362, r2: 0.3897. TEST: loss 136.4450, r2: 0.4673. lr: 0.0010 \n",
      "Epoch 960. TRAIN: loss 269.0977, r2: 0.4059. TEST: loss 139.7784, r2: 0.4542. lr: 0.0010 \n",
      "Epoch 970. TRAIN: loss 270.2732, r2: 0.4033. TEST: loss 139.8776, r2: 0.4539. lr: 0.0010 \n",
      "Epoch 980. TRAIN: loss 270.1627, r2: 0.4035. TEST: loss 139.2162, r2: 0.4564. lr: 0.0010 \n",
      "Epoch 990. TRAIN: loss 263.9698, r2: 0.4172. TEST: loss 137.6591, r2: 0.4625. lr: 0.0010 \n",
      "Epoch 1000. TRAIN: loss 267.2577, r2: 0.4099. TEST: loss 132.2395, r2: 0.4837. lr: 0.0010 \n"
     ]
    }
   ],
   "source": [
    "datetime_now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = \"./logs/\" + \"test_train_split_\" + datetime_now\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "trained_model = train_func(data, model_v2, 1000, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_v1, \"/var/essdata/IDU/other/mm_22/industrial-location/ml/model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
