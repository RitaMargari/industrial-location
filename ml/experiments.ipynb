{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/essdata/IDU/venvs/common_venv/lib/python3.9/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.10.3-CAPI-1.16.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "from geopy.distance import great_circle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MigrationDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(MigrationDataset, self).__init__(root, transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        files = []\n",
    "        for file in os.listdir(self.root):\n",
    "            if file.endswith(\".geojson\") or file.endswith(\".csv\"):\n",
    "                files.append(file)\n",
    "        return files\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['migration_dataset']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        \n",
    "        # read files in specified folder\n",
    "        cities = gpd.read_file(os.path.join(self.root, \"cities_aggregated.geojson\")).set_index(\"city\")\n",
    "        responses = gpd.read_file(os.path.join(self.root, \"responses_clustered.csv\"))\n",
    "\n",
    "\n",
    "        # extract cities features \n",
    "        cities_features = cities[[\n",
    "            \"population\",\n",
    "            \"city_category\", \n",
    "            \"harsh_climate\", \n",
    "            \"ueqi_score\", \n",
    "            \"residential\", \n",
    "            \"street_networks\", \n",
    "            \"greens_spaces\", \n",
    "            \"public_and_business_infrastructure\", \n",
    "            \"social_and_leisure_infrastructure\",\n",
    "            \"citywide_space\"\n",
    "            ]]\n",
    "        \n",
    "        # encode categorical features\n",
    "        one_hot = OneHotEncoder()\n",
    "        encoded_category = one_hot.fit_transform(np.expand_dims(cities[\"city_category\"].to_numpy(), 1)).toarray()\n",
    "        encoded_category_names = one_hot.get_feature_names_out([\"category\"])\n",
    "        cities_features.loc[:, encoded_category_names] = encoded_category\n",
    "        cities_features = cities_features.drop([\"city_category\"], axis=1)\n",
    "        cities_features[\"harsh_climate\"] = cities_features[\"harsh_climate\"].astype(int)\n",
    "\n",
    "        # form distance matrix\n",
    "        DM = cities[\"geometry\"].progress_apply(\n",
    "            lambda p1: cities[\"geometry\"].apply(\n",
    "                lambda p2: great_circle(p1.coords[0], p2.coords[0]).km\n",
    "                ))\n",
    "\n",
    "        # form origin-destination matrix\n",
    "\n",
    "        responses_counts = responses.groupby([\"cluster_center_cv\", \"cluster_center_vacancy\"])[\"id_candidate\"].count()\n",
    "        responses_cities = responses_counts.index.get_level_values(0).drop_duplicates()\n",
    "        OD = pd.DataFrame(None, index=DM.columns, columns=DM.columns)\n",
    "        OD = OD.progress_apply(\n",
    "            lambda city: city.fillna(responses_counts[city.name]).fillna(0) \n",
    "            if city.name in responses_cities else city.fillna(0)\n",
    "            )\n",
    "        \n",
    "        # transform data\n",
    "        \n",
    "        cities_num = len(OD)\n",
    "        edge_index = [[], []]\n",
    "        for i in range(cities_num):\n",
    "            edge_index[0].extend([i for j in range(cities_num)])\n",
    "            edge_index[1].extend([j for j in range(cities_num)])\n",
    "\n",
    "        edge_index = torch.tensor(edge_index)\n",
    "        y = torch.tensor(np.concatenate((OD.to_numpy())), dtype=torch.float32)\n",
    "        edge_attr = torch.tensor(np.concatenate((DM.to_numpy())), dtype=torch.float32)\n",
    "        x = torch.tensor(cities_features.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "        # exclude diagonal\n",
    "        non_diagonal = edge_attr > 0\n",
    "        edge_attr = edge_attr[non_diagonal]\n",
    "        edge_index = edge_index[:, non_diagonal]\n",
    "        y = y[non_diagonal]\n",
    "        \n",
    "        # create torch object          \n",
    "        graph = Data(x=x,edge_index=edge_index, y=y, edge_attr=edge_attr)\n",
    "        \n",
    "        data_list = []\n",
    "        data_list.append(graph)\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be found here: https://disk.yandex.ru/client/disk/%D0%A2%D1%80%D1%83%D0%B4%D0%BE%D0%B2%D1%8B%D0%B5%20%D1%80%D0%B5%D1%81%D1%83%D1%80%D1%81%D1%8B/%D0%A4%D0%B0%D0%B9%D0%BB%D1%8B/ML_experiments/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "/tmp/ipykernel_62476/3110626135.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cities_features.loc[:, encoded_category_names] = encoded_category\n",
      "100%|██████████| 1106/1106 [01:20<00:00, 13.74it/s]\n",
      "100%|██████████| 1106/1106 [00:01<00:00, 1063.62it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = MigrationDataset(\"/var/essdata/IDU/other/mm_22/industrial-location/ml/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1106, 14], edge_index=[2, 1222130], edge_attr=[1222130], y=[1222130])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super(FNN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.lins.append(nn.Linear(input_dim, hidden_dim))\n",
    "        for _ in range(self.num_layers - 2):\n",
    "            self.lins.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.lins.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.norm = nn.ModuleList()\n",
    "        for l in range(self.num_layers):\n",
    "            self.norm.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr.unsqueeze(-1)\n",
    "\n",
    "        x_s = x[edge_index[0]]\n",
    "        x_d = x[edge_index[1]]\n",
    "        y = torch.cat((x_s, x_d, edge_weight), axis=1)\n",
    "        # y = F.normalize(y)\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            y = self.lins[i](y) \n",
    "            y = nn.functional.leaky_relu(y)\n",
    "            y = F.dropout(y, p=self.dropout, training=self.training)\n",
    "            y = self.norm[i](y)\n",
    "\n",
    "        y = self.lins[-1](y)\n",
    "        y = torch.relu(y).squeeze()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_loss(output, target):\n",
    "    target_mean = torch.mean(target)\n",
    "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
    "    ss_res = torch.sum((target - output) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2\n",
    "\n",
    "def train_func(dataset, model, epochs, writer):\n",
    "\n",
    "    optimize = torch.optim.Adam(list(model.parameters()),  lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimize, factor=0.9, min_lr=0.0001)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(epochs + 1):\n",
    "\n",
    "        optimize.zero_grad()\n",
    "        model.train()\n",
    "        y_hat = model(dataset)\n",
    "\n",
    "        loss = F.mse_loss(y_hat, dataset.y)\n",
    "        r2 = r2_loss(y_hat, dataset.y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimize.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        t_metrics = {\"train_loss\": loss, \"train_r2\": r2}  \n",
    "        for name, v_metric in t_metrics.items(): writer.add_scalar(name, v_metric, epoch)\n",
    "\n",
    "        if epoch % 10 == 0:    \n",
    "            print(\n",
    "                \"Epoch {}. TRAIN: loss {:.4f}, r2: {:.4f}. lr: {:.4f} \".format(\n",
    "                    epoch, t_metrics[\"train_loss\"], t_metrics[\"train_r2\"], optimize.param_groups[0][\"lr\"]\n",
    "                    )\n",
    "                ) \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = dataset.data.x.shape[1] * 2 + 1\n",
    "hidden_dim = 128\n",
    "num_layers = 3\n",
    "dropout = 0.2\n",
    "\n",
    "model_v1 = FNN(input_dim, hidden_dim, num_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. TRAIN: loss 393.5331, r2: 0.0009. lr: 0.0010 \n",
      "Epoch 10. TRAIN: loss 393.4045, r2: 0.0012. lr: 0.0010 \n",
      "Epoch 20. TRAIN: loss 393.2929, r2: 0.0015. lr: 0.0010 \n",
      "Epoch 30. TRAIN: loss 393.1610, r2: 0.0018. lr: 0.0010 \n",
      "Epoch 40. TRAIN: loss 393.0468, r2: 0.0021. lr: 0.0010 \n",
      "Epoch 50. TRAIN: loss 392.8981, r2: 0.0025. lr: 0.0010 \n",
      "Epoch 60. TRAIN: loss 392.7978, r2: 0.0027. lr: 0.0010 \n",
      "Epoch 70. TRAIN: loss 392.6240, r2: 0.0032. lr: 0.0010 \n",
      "Epoch 80. TRAIN: loss 392.3603, r2: 0.0039. lr: 0.0010 \n",
      "Epoch 90. TRAIN: loss 392.0068, r2: 0.0047. lr: 0.0010 \n",
      "Epoch 100. TRAIN: loss 391.6028, r2: 0.0058. lr: 0.0010 \n",
      "Epoch 110. TRAIN: loss 390.8111, r2: 0.0078. lr: 0.0010 \n",
      "Epoch 120. TRAIN: loss 390.0086, r2: 0.0098. lr: 0.0010 \n",
      "Epoch 130. TRAIN: loss 388.6362, r2: 0.0133. lr: 0.0010 \n",
      "Epoch 140. TRAIN: loss 387.3689, r2: 0.0165. lr: 0.0010 \n",
      "Epoch 150. TRAIN: loss 385.2827, r2: 0.0218. lr: 0.0010 \n",
      "Epoch 160. TRAIN: loss 383.3600, r2: 0.0267. lr: 0.0010 \n",
      "Epoch 170. TRAIN: loss 381.9162, r2: 0.0304. lr: 0.0010 \n",
      "Epoch 180. TRAIN: loss 380.3941, r2: 0.0342. lr: 0.0010 \n",
      "Epoch 190. TRAIN: loss 379.4601, r2: 0.0366. lr: 0.0010 \n",
      "Epoch 200. TRAIN: loss 385.0440, r2: 0.0224. lr: 0.0010 \n",
      "Epoch 210. TRAIN: loss 377.9928, r2: 0.0403. lr: 0.0009 \n",
      "Epoch 220. TRAIN: loss 376.7905, r2: 0.0434. lr: 0.0009 \n",
      "Epoch 230. TRAIN: loss 374.9535, r2: 0.0480. lr: 0.0009 \n",
      "Epoch 240. TRAIN: loss 373.6162, r2: 0.0514. lr: 0.0009 \n",
      "Epoch 250. TRAIN: loss 372.7112, r2: 0.0537. lr: 0.0009 \n",
      "Epoch 260. TRAIN: loss 372.5919, r2: 0.0540. lr: 0.0009 \n",
      "Epoch 270. TRAIN: loss 376.5482, r2: 0.0440. lr: 0.0009 \n",
      "Epoch 280. TRAIN: loss 372.7531, r2: 0.0536. lr: 0.0008 \n",
      "Epoch 290. TRAIN: loss 370.1199, r2: 0.0603. lr: 0.0007 \n",
      "Epoch 300. TRAIN: loss 369.2201, r2: 0.0626. lr: 0.0007 \n",
      "Epoch 310. TRAIN: loss 367.7551, r2: 0.0663. lr: 0.0007 \n",
      "Epoch 320. TRAIN: loss 366.2657, r2: 0.0701. lr: 0.0007 \n",
      "Epoch 330. TRAIN: loss 365.3909, r2: 0.0723. lr: 0.0007 \n",
      "Epoch 340. TRAIN: loss 364.2713, r2: 0.0752. lr: 0.0007 \n",
      "Epoch 350. TRAIN: loss 369.4552, r2: 0.0620. lr: 0.0007 \n",
      "Epoch 360. TRAIN: loss 364.6335, r2: 0.0742. lr: 0.0007 \n",
      "Epoch 370. TRAIN: loss 363.8220, r2: 0.0763. lr: 0.0007 \n",
      "Epoch 380. TRAIN: loss 361.7409, r2: 0.0816. lr: 0.0007 \n",
      "Epoch 390. TRAIN: loss 360.4558, r2: 0.0849. lr: 0.0007 \n",
      "Epoch 400. TRAIN: loss 359.5765, r2: 0.0871. lr: 0.0007 \n",
      "Epoch 410. TRAIN: loss 360.3269, r2: 0.0852. lr: 0.0007 \n",
      "Epoch 420. TRAIN: loss 358.6911, r2: 0.0893. lr: 0.0007 \n",
      "Epoch 430. TRAIN: loss 357.7075, r2: 0.0918. lr: 0.0007 \n",
      "Epoch 440. TRAIN: loss 357.9506, r2: 0.0912. lr: 0.0006 \n",
      "Epoch 450. TRAIN: loss 356.0139, r2: 0.0961. lr: 0.0006 \n",
      "Epoch 460. TRAIN: loss 355.5070, r2: 0.0974. lr: 0.0006 \n",
      "Epoch 470. TRAIN: loss 354.5741, r2: 0.0998. lr: 0.0006 \n",
      "Epoch 480. TRAIN: loss 353.8530, r2: 0.1016. lr: 0.0006 \n",
      "Epoch 490. TRAIN: loss 353.8311, r2: 0.1017. lr: 0.0006 \n",
      "Epoch 500. TRAIN: loss 352.1164, r2: 0.1060. lr: 0.0006 \n",
      "Epoch 510. TRAIN: loss 351.4664, r2: 0.1077. lr: 0.0006 \n",
      "Epoch 520. TRAIN: loss 353.7162, r2: 0.1020. lr: 0.0006 \n",
      "Epoch 530. TRAIN: loss 349.9798, r2: 0.1114. lr: 0.0005 \n",
      "Epoch 540. TRAIN: loss 349.7296, r2: 0.1121. lr: 0.0005 \n",
      "Epoch 550. TRAIN: loss 349.5200, r2: 0.1126. lr: 0.0005 \n",
      "Epoch 560. TRAIN: loss 349.5054, r2: 0.1127. lr: 0.0005 \n",
      "Epoch 570. TRAIN: loss 348.3868, r2: 0.1155. lr: 0.0005 \n",
      "Epoch 580. TRAIN: loss 348.4483, r2: 0.1153. lr: 0.0005 \n",
      "Epoch 590. TRAIN: loss 346.7921, r2: 0.1195. lr: 0.0005 \n",
      "Epoch 600. TRAIN: loss 346.3802, r2: 0.1206. lr: 0.0005 \n",
      "Epoch 610. TRAIN: loss 345.9076, r2: 0.1218. lr: 0.0005 \n",
      "Epoch 620. TRAIN: loss 345.1131, r2: 0.1238. lr: 0.0005 \n",
      "Epoch 630. TRAIN: loss 345.0652, r2: 0.1239. lr: 0.0005 \n",
      "Epoch 640. TRAIN: loss 344.6403, r2: 0.1250. lr: 0.0005 \n",
      "Epoch 650. TRAIN: loss 343.5842, r2: 0.1277. lr: 0.0005 \n",
      "Epoch 660. TRAIN: loss 343.3376, r2: 0.1283. lr: 0.0005 \n",
      "Epoch 670. TRAIN: loss 343.4563, r2: 0.1280. lr: 0.0004 \n",
      "Epoch 680. TRAIN: loss 342.9612, r2: 0.1293. lr: 0.0004 \n",
      "Epoch 690. TRAIN: loss 342.1578, r2: 0.1313. lr: 0.0004 \n",
      "Epoch 700. TRAIN: loss 341.7428, r2: 0.1324. lr: 0.0004 \n",
      "Epoch 710. TRAIN: loss 340.3192, r2: 0.1360. lr: 0.0004 \n",
      "Epoch 720. TRAIN: loss 341.4539, r2: 0.1331. lr: 0.0004 \n",
      "Epoch 730. TRAIN: loss 339.9467, r2: 0.1369. lr: 0.0003 \n",
      "Epoch 740. TRAIN: loss 339.3726, r2: 0.1384. lr: 0.0003 \n",
      "Epoch 750. TRAIN: loss 339.3214, r2: 0.1385. lr: 0.0003 \n",
      "Epoch 760. TRAIN: loss 338.4942, r2: 0.1406. lr: 0.0003 \n",
      "Epoch 770. TRAIN: loss 340.1856, r2: 0.1363. lr: 0.0003 \n",
      "Epoch 780. TRAIN: loss 338.8355, r2: 0.1397. lr: 0.0003 \n",
      "Epoch 790. TRAIN: loss 338.2573, r2: 0.1412. lr: 0.0003 \n",
      "Epoch 800. TRAIN: loss 337.2748, r2: 0.1437. lr: 0.0003 \n",
      "Epoch 810. TRAIN: loss 337.0140, r2: 0.1444. lr: 0.0003 \n",
      "Epoch 820. TRAIN: loss 336.4350, r2: 0.1458. lr: 0.0003 \n",
      "Epoch 830. TRAIN: loss 336.2480, r2: 0.1463. lr: 0.0003 \n",
      "Epoch 840. TRAIN: loss 335.4741, r2: 0.1483. lr: 0.0003 \n",
      "Epoch 850. TRAIN: loss 335.4079, r2: 0.1484. lr: 0.0003 \n",
      "Epoch 860. TRAIN: loss 336.9053, r2: 0.1446. lr: 0.0003 \n",
      "Epoch 870. TRAIN: loss 335.2710, r2: 0.1488. lr: 0.0003 \n",
      "Epoch 880. TRAIN: loss 334.2532, r2: 0.1514. lr: 0.0003 \n",
      "Epoch 890. TRAIN: loss 334.5618, r2: 0.1506. lr: 0.0003 \n",
      "Epoch 900. TRAIN: loss 334.4627, r2: 0.1508. lr: 0.0003 \n",
      "Epoch 910. TRAIN: loss 333.3773, r2: 0.1536. lr: 0.0003 \n",
      "Epoch 920. TRAIN: loss 332.6432, r2: 0.1555. lr: 0.0003 \n",
      "Epoch 930. TRAIN: loss 333.3683, r2: 0.1536. lr: 0.0002 \n",
      "Epoch 940. TRAIN: loss 332.6237, r2: 0.1555. lr: 0.0002 \n",
      "Epoch 950. TRAIN: loss 332.3808, r2: 0.1561. lr: 0.0002 \n",
      "Epoch 960. TRAIN: loss 332.4901, r2: 0.1559. lr: 0.0002 \n",
      "Epoch 970. TRAIN: loss 331.9281, r2: 0.1573. lr: 0.0002 \n",
      "Epoch 980. TRAIN: loss 331.4486, r2: 0.1585. lr: 0.0002 \n",
      "Epoch 990. TRAIN: loss 330.8284, r2: 0.1601. lr: 0.0002 \n",
      "Epoch 1000. TRAIN: loss 331.0604, r2: 0.1595. lr: 0.0002 \n",
      "Epoch 1010. TRAIN: loss 330.9898, r2: 0.1597. lr: 0.0002 \n",
      "Epoch 1020. TRAIN: loss 330.4398, r2: 0.1611. lr: 0.0002 \n",
      "Epoch 1030. TRAIN: loss 330.3184, r2: 0.1614. lr: 0.0002 \n",
      "Epoch 1040. TRAIN: loss 330.3118, r2: 0.1614. lr: 0.0002 \n",
      "Epoch 1050. TRAIN: loss 330.6343, r2: 0.1606. lr: 0.0002 \n",
      "Epoch 1060. TRAIN: loss 329.7463, r2: 0.1628. lr: 0.0002 \n",
      "Epoch 1070. TRAIN: loss 329.1970, r2: 0.1642. lr: 0.0002 \n",
      "Epoch 1080. TRAIN: loss 328.9974, r2: 0.1647. lr: 0.0001 \n",
      "Epoch 1090. TRAIN: loss 328.5597, r2: 0.1658. lr: 0.0001 \n",
      "Epoch 1100. TRAIN: loss 328.6287, r2: 0.1657. lr: 0.0001 \n",
      "Epoch 1110. TRAIN: loss 328.6865, r2: 0.1655. lr: 0.0001 \n",
      "Epoch 1120. TRAIN: loss 327.7959, r2: 0.1678. lr: 0.0001 \n",
      "Epoch 1130. TRAIN: loss 327.7827, r2: 0.1678. lr: 0.0001 \n",
      "Epoch 1140. TRAIN: loss 327.5853, r2: 0.1683. lr: 0.0001 \n",
      "Epoch 1150. TRAIN: loss 327.3178, r2: 0.1690. lr: 0.0001 \n",
      "Epoch 1160. TRAIN: loss 326.9675, r2: 0.1699. lr: 0.0001 \n",
      "Epoch 1170. TRAIN: loss 327.4601, r2: 0.1686. lr: 0.0001 \n",
      "Epoch 1180. TRAIN: loss 328.0851, r2: 0.1670. lr: 0.0001 \n",
      "Epoch 1190. TRAIN: loss 326.9994, r2: 0.1698. lr: 0.0001 \n",
      "Epoch 1200. TRAIN: loss 327.6184, r2: 0.1682. lr: 0.0001 \n",
      "Epoch 1210. TRAIN: loss 326.8303, r2: 0.1702. lr: 0.0001 \n",
      "Epoch 1220. TRAIN: loss 326.7059, r2: 0.1705. lr: 0.0001 \n",
      "Epoch 1230. TRAIN: loss 326.4040, r2: 0.1713. lr: 0.0001 \n",
      "Epoch 1240. TRAIN: loss 326.1509, r2: 0.1719. lr: 0.0001 \n",
      "Epoch 1250. TRAIN: loss 326.5186, r2: 0.1710. lr: 0.0001 \n",
      "Epoch 1260. TRAIN: loss 326.3343, r2: 0.1715. lr: 0.0001 \n",
      "Epoch 1270. TRAIN: loss 326.4112, r2: 0.1713. lr: 0.0001 \n",
      "Epoch 1280. TRAIN: loss 325.6343, r2: 0.1733. lr: 0.0001 \n",
      "Epoch 1290. TRAIN: loss 325.8153, r2: 0.1728. lr: 0.0001 \n",
      "Epoch 1300. TRAIN: loss 325.4783, r2: 0.1737. lr: 0.0001 \n",
      "Epoch 1310. TRAIN: loss 325.1793, r2: 0.1744. lr: 0.0001 \n",
      "Epoch 1320. TRAIN: loss 324.6423, r2: 0.1758. lr: 0.0001 \n",
      "Epoch 1330. TRAIN: loss 325.7471, r2: 0.1730. lr: 0.0001 \n",
      "Epoch 1340. TRAIN: loss 325.0975, r2: 0.1746. lr: 0.0001 \n",
      "Epoch 1350. TRAIN: loss 324.6547, r2: 0.1757. lr: 0.0001 \n",
      "Epoch 1360. TRAIN: loss 323.8382, r2: 0.1778. lr: 0.0001 \n",
      "Epoch 1370. TRAIN: loss 324.6892, r2: 0.1757. lr: 0.0001 \n",
      "Epoch 1380. TRAIN: loss 323.4703, r2: 0.1788. lr: 0.0001 \n",
      "Epoch 1390. TRAIN: loss 323.6361, r2: 0.1783. lr: 0.0001 \n",
      "Epoch 1400. TRAIN: loss 323.6728, r2: 0.1782. lr: 0.0001 \n",
      "Epoch 1410. TRAIN: loss 323.7140, r2: 0.1781. lr: 0.0001 \n",
      "Epoch 1420. TRAIN: loss 324.3008, r2: 0.1766. lr: 0.0001 \n",
      "Epoch 1430. TRAIN: loss 323.9295, r2: 0.1776. lr: 0.0001 \n",
      "Epoch 1440. TRAIN: loss 323.3256, r2: 0.1791. lr: 0.0001 \n",
      "Epoch 1450. TRAIN: loss 322.5739, r2: 0.1810. lr: 0.0001 \n",
      "Epoch 1460. TRAIN: loss 322.9189, r2: 0.1802. lr: 0.0001 \n",
      "Epoch 1470. TRAIN: loss 322.7107, r2: 0.1807. lr: 0.0001 \n",
      "Epoch 1480. TRAIN: loss 322.2070, r2: 0.1820. lr: 0.0001 \n",
      "Epoch 1490. TRAIN: loss 322.4991, r2: 0.1812. lr: 0.0001 \n",
      "Epoch 1500. TRAIN: loss 322.4846, r2: 0.1813. lr: 0.0001 \n",
      "Epoch 1510. TRAIN: loss 321.8593, r2: 0.1828. lr: 0.0001 \n",
      "Epoch 1520. TRAIN: loss 322.1562, r2: 0.1821. lr: 0.0001 \n",
      "Epoch 1530. TRAIN: loss 321.9713, r2: 0.1826. lr: 0.0001 \n",
      "Epoch 1540. TRAIN: loss 322.1475, r2: 0.1821. lr: 0.0001 \n",
      "Epoch 1550. TRAIN: loss 321.1844, r2: 0.1846. lr: 0.0001 \n",
      "Epoch 1560. TRAIN: loss 321.1789, r2: 0.1846. lr: 0.0001 \n",
      "Epoch 1570. TRAIN: loss 321.0965, r2: 0.1848. lr: 0.0001 \n",
      "Epoch 1580. TRAIN: loss 321.0140, r2: 0.1850. lr: 0.0001 \n",
      "Epoch 1590. TRAIN: loss 321.0054, r2: 0.1850. lr: 0.0001 \n",
      "Epoch 1600. TRAIN: loss 319.6546, r2: 0.1884. lr: 0.0001 \n",
      "Epoch 1610. TRAIN: loss 319.9131, r2: 0.1878. lr: 0.0001 \n",
      "Epoch 1620. TRAIN: loss 321.0196, r2: 0.1850. lr: 0.0001 \n",
      "Epoch 1630. TRAIN: loss 319.9741, r2: 0.1876. lr: 0.0001 \n",
      "Epoch 1640. TRAIN: loss 320.1339, r2: 0.1872. lr: 0.0001 \n",
      "Epoch 1650. TRAIN: loss 319.6383, r2: 0.1885. lr: 0.0001 \n",
      "Epoch 1660. TRAIN: loss 320.4089, r2: 0.1865. lr: 0.0001 \n",
      "Epoch 1670. TRAIN: loss 319.6897, r2: 0.1884. lr: 0.0001 \n",
      "Epoch 1680. TRAIN: loss 319.2535, r2: 0.1895. lr: 0.0001 \n",
      "Epoch 1690. TRAIN: loss 319.3465, r2: 0.1892. lr: 0.0001 \n",
      "Epoch 1700. TRAIN: loss 319.3065, r2: 0.1893. lr: 0.0001 \n",
      "Epoch 1710. TRAIN: loss 318.8468, r2: 0.1905. lr: 0.0001 \n",
      "Epoch 1720. TRAIN: loss 318.6574, r2: 0.1910. lr: 0.0001 \n",
      "Epoch 1730. TRAIN: loss 318.7553, r2: 0.1907. lr: 0.0001 \n",
      "Epoch 1740. TRAIN: loss 318.6589, r2: 0.1910. lr: 0.0001 \n",
      "Epoch 1750. TRAIN: loss 317.6545, r2: 0.1935. lr: 0.0001 \n",
      "Epoch 1760. TRAIN: loss 318.2676, r2: 0.1920. lr: 0.0001 \n",
      "Epoch 1770. TRAIN: loss 317.7752, r2: 0.1932. lr: 0.0001 \n",
      "Epoch 1780. TRAIN: loss 317.7386, r2: 0.1933. lr: 0.0001 \n",
      "Epoch 1790. TRAIN: loss 317.7126, r2: 0.1934. lr: 0.0001 \n",
      "Epoch 1800. TRAIN: loss 317.1576, r2: 0.1948. lr: 0.0001 \n",
      "Epoch 1810. TRAIN: loss 316.9764, r2: 0.1952. lr: 0.0001 \n",
      "Epoch 1820. TRAIN: loss 318.0274, r2: 0.1926. lr: 0.0001 \n",
      "Epoch 1830. TRAIN: loss 317.4654, r2: 0.1940. lr: 0.0001 \n",
      "Epoch 1840. TRAIN: loss 316.6404, r2: 0.1961. lr: 0.0001 \n",
      "Epoch 1850. TRAIN: loss 317.3475, r2: 0.1943. lr: 0.0001 \n",
      "Epoch 1860. TRAIN: loss 316.5607, r2: 0.1963. lr: 0.0001 \n",
      "Epoch 1870. TRAIN: loss 315.7192, r2: 0.1984. lr: 0.0001 \n",
      "Epoch 1880. TRAIN: loss 316.2458, r2: 0.1971. lr: 0.0001 \n",
      "Epoch 1890. TRAIN: loss 316.4024, r2: 0.1967. lr: 0.0001 \n",
      "Epoch 1900. TRAIN: loss 316.1664, r2: 0.1973. lr: 0.0001 \n",
      "Epoch 1910. TRAIN: loss 316.6000, r2: 0.1962. lr: 0.0001 \n",
      "Epoch 1920. TRAIN: loss 315.6137, r2: 0.1987. lr: 0.0001 \n",
      "Epoch 1930. TRAIN: loss 316.0100, r2: 0.1977. lr: 0.0001 \n",
      "Epoch 1940. TRAIN: loss 315.4953, r2: 0.1990. lr: 0.0001 \n",
      "Epoch 1950. TRAIN: loss 315.8939, r2: 0.1980. lr: 0.0001 \n",
      "Epoch 1960. TRAIN: loss 316.2695, r2: 0.1970. lr: 0.0001 \n",
      "Epoch 1970. TRAIN: loss 315.3756, r2: 0.1993. lr: 0.0001 \n",
      "Epoch 1980. TRAIN: loss 315.0403, r2: 0.2002. lr: 0.0001 \n",
      "Epoch 1990. TRAIN: loss 314.6077, r2: 0.2013. lr: 0.0001 \n",
      "Epoch 2000. TRAIN: loss 313.7900, r2: 0.2033. lr: 0.0001 \n",
      "Epoch 2010. TRAIN: loss 314.2977, r2: 0.2020. lr: 0.0001 \n",
      "Epoch 2020. TRAIN: loss 314.1811, r2: 0.2023. lr: 0.0001 \n",
      "Epoch 2030. TRAIN: loss 313.5033, r2: 0.2041. lr: 0.0001 \n",
      "Epoch 2040. TRAIN: loss 313.3362, r2: 0.2045. lr: 0.0001 \n",
      "Epoch 2050. TRAIN: loss 313.6825, r2: 0.2036. lr: 0.0001 \n",
      "Epoch 2060. TRAIN: loss 313.6751, r2: 0.2036. lr: 0.0001 \n",
      "Epoch 2070. TRAIN: loss 313.4838, r2: 0.2041. lr: 0.0001 \n",
      "Epoch 2080. TRAIN: loss 314.0062, r2: 0.2028. lr: 0.0001 \n",
      "Epoch 2090. TRAIN: loss 312.7811, r2: 0.2059. lr: 0.0001 \n",
      "Epoch 2100. TRAIN: loss 312.9202, r2: 0.2055. lr: 0.0001 \n",
      "Epoch 2110. TRAIN: loss 312.7515, r2: 0.2060. lr: 0.0001 \n",
      "Epoch 2120. TRAIN: loss 312.9309, r2: 0.2055. lr: 0.0001 \n",
      "Epoch 2130. TRAIN: loss 313.0457, r2: 0.2052. lr: 0.0001 \n",
      "Epoch 2140. TRAIN: loss 313.2409, r2: 0.2047. lr: 0.0001 \n",
      "Epoch 2150. TRAIN: loss 312.3922, r2: 0.2069. lr: 0.0001 \n",
      "Epoch 2160. TRAIN: loss 312.0514, r2: 0.2077. lr: 0.0001 \n",
      "Epoch 2170. TRAIN: loss 311.8804, r2: 0.2082. lr: 0.0001 \n",
      "Epoch 2180. TRAIN: loss 311.7701, r2: 0.2085. lr: 0.0001 \n",
      "Epoch 2190. TRAIN: loss 311.5176, r2: 0.2091. lr: 0.0001 \n",
      "Epoch 2200. TRAIN: loss 311.3277, r2: 0.2096. lr: 0.0001 \n",
      "Epoch 2210. TRAIN: loss 311.2543, r2: 0.2098. lr: 0.0001 \n",
      "Epoch 2220. TRAIN: loss 311.2335, r2: 0.2098. lr: 0.0001 \n",
      "Epoch 2230. TRAIN: loss 310.8440, r2: 0.2108. lr: 0.0001 \n",
      "Epoch 2240. TRAIN: loss 310.5454, r2: 0.2116. lr: 0.0001 \n",
      "Epoch 2250. TRAIN: loss 311.3329, r2: 0.2096. lr: 0.0001 \n",
      "Epoch 2260. TRAIN: loss 310.7392, r2: 0.2111. lr: 0.0001 \n",
      "Epoch 2270. TRAIN: loss 310.7904, r2: 0.2109. lr: 0.0001 \n",
      "Epoch 2280. TRAIN: loss 310.7220, r2: 0.2111. lr: 0.0001 \n",
      "Epoch 2290. TRAIN: loss 310.5273, r2: 0.2116. lr: 0.0001 \n",
      "Epoch 2300. TRAIN: loss 310.9011, r2: 0.2107. lr: 0.0001 \n",
      "Epoch 2310. TRAIN: loss 309.3167, r2: 0.2147. lr: 0.0001 \n",
      "Epoch 2320. TRAIN: loss 310.0237, r2: 0.2129. lr: 0.0001 \n",
      "Epoch 2330. TRAIN: loss 309.3193, r2: 0.2147. lr: 0.0001 \n",
      "Epoch 2340. TRAIN: loss 309.2784, r2: 0.2148. lr: 0.0001 \n",
      "Epoch 2350. TRAIN: loss 309.9355, r2: 0.2131. lr: 0.0001 \n",
      "Epoch 2360. TRAIN: loss 309.5297, r2: 0.2141. lr: 0.0001 \n",
      "Epoch 2370. TRAIN: loss 310.0355, r2: 0.2129. lr: 0.0001 \n",
      "Epoch 2380. TRAIN: loss 309.2979, r2: 0.2147. lr: 0.0001 \n",
      "Epoch 2390. TRAIN: loss 309.6280, r2: 0.2139. lr: 0.0001 \n",
      "Epoch 2400. TRAIN: loss 308.4387, r2: 0.2169. lr: 0.0001 \n",
      "Epoch 2410. TRAIN: loss 309.3175, r2: 0.2147. lr: 0.0001 \n",
      "Epoch 2420. TRAIN: loss 309.0617, r2: 0.2153. lr: 0.0001 \n",
      "Epoch 2430. TRAIN: loss 308.7563, r2: 0.2161. lr: 0.0001 \n",
      "Epoch 2440. TRAIN: loss 308.4341, r2: 0.2169. lr: 0.0001 \n",
      "Epoch 2450. TRAIN: loss 308.9469, r2: 0.2156. lr: 0.0001 \n",
      "Epoch 2460. TRAIN: loss 313.1573, r2: 0.2049. lr: 0.0001 \n",
      "Epoch 2470. TRAIN: loss 311.5534, r2: 0.2090. lr: 0.0001 \n",
      "Epoch 2480. TRAIN: loss 309.1422, r2: 0.2151. lr: 0.0001 \n",
      "Epoch 2490. TRAIN: loss 308.7932, r2: 0.2160. lr: 0.0001 \n",
      "Epoch 2500. TRAIN: loss 307.3792, r2: 0.2196. lr: 0.0001 \n",
      "Epoch 2510. TRAIN: loss 308.0520, r2: 0.2179. lr: 0.0001 \n",
      "Epoch 2520. TRAIN: loss 307.5489, r2: 0.2192. lr: 0.0001 \n",
      "Epoch 2530. TRAIN: loss 307.8540, r2: 0.2184. lr: 0.0001 \n",
      "Epoch 2540. TRAIN: loss 307.6130, r2: 0.2190. lr: 0.0001 \n",
      "Epoch 2550. TRAIN: loss 308.0451, r2: 0.2179. lr: 0.0001 \n",
      "Epoch 2560. TRAIN: loss 307.1140, r2: 0.2203. lr: 0.0001 \n",
      "Epoch 2570. TRAIN: loss 306.7043, r2: 0.2213. lr: 0.0001 \n",
      "Epoch 2580. TRAIN: loss 307.1001, r2: 0.2203. lr: 0.0001 \n",
      "Epoch 2590. TRAIN: loss 307.5678, r2: 0.2191. lr: 0.0001 \n",
      "Epoch 2600. TRAIN: loss 306.9895, r2: 0.2206. lr: 0.0001 \n",
      "Epoch 2610. TRAIN: loss 306.5908, r2: 0.2216. lr: 0.0001 \n",
      "Epoch 2620. TRAIN: loss 306.7161, r2: 0.2213. lr: 0.0001 \n",
      "Epoch 2630. TRAIN: loss 306.6722, r2: 0.2214. lr: 0.0001 \n",
      "Epoch 2640. TRAIN: loss 306.0212, r2: 0.2231. lr: 0.0001 \n",
      "Epoch 2650. TRAIN: loss 306.6062, r2: 0.2216. lr: 0.0001 \n",
      "Epoch 2660. TRAIN: loss 305.9823, r2: 0.2232. lr: 0.0001 \n",
      "Epoch 2670. TRAIN: loss 306.3808, r2: 0.2221. lr: 0.0001 \n",
      "Epoch 2680. TRAIN: loss 305.9382, r2: 0.2233. lr: 0.0001 \n",
      "Epoch 2690. TRAIN: loss 306.0689, r2: 0.2229. lr: 0.0001 \n",
      "Epoch 2700. TRAIN: loss 305.8176, r2: 0.2236. lr: 0.0001 \n",
      "Epoch 2710. TRAIN: loss 306.6230, r2: 0.2215. lr: 0.0001 \n",
      "Epoch 2720. TRAIN: loss 304.7773, r2: 0.2262. lr: 0.0001 \n",
      "Epoch 2730. TRAIN: loss 305.7598, r2: 0.2237. lr: 0.0001 \n",
      "Epoch 2740. TRAIN: loss 305.9189, r2: 0.2233. lr: 0.0001 \n",
      "Epoch 2750. TRAIN: loss 304.9150, r2: 0.2259. lr: 0.0001 \n",
      "Epoch 2760. TRAIN: loss 304.8763, r2: 0.2260. lr: 0.0001 \n",
      "Epoch 2770. TRAIN: loss 304.9590, r2: 0.2258. lr: 0.0001 \n",
      "Epoch 2780. TRAIN: loss 304.1925, r2: 0.2277. lr: 0.0001 \n",
      "Epoch 2790. TRAIN: loss 304.3961, r2: 0.2272. lr: 0.0001 \n",
      "Epoch 2800. TRAIN: loss 304.8097, r2: 0.2261. lr: 0.0001 \n",
      "Epoch 2810. TRAIN: loss 303.7567, r2: 0.2288. lr: 0.0001 \n",
      "Epoch 2820. TRAIN: loss 304.3580, r2: 0.2273. lr: 0.0001 \n",
      "Epoch 2830. TRAIN: loss 304.2072, r2: 0.2277. lr: 0.0001 \n",
      "Epoch 2840. TRAIN: loss 303.8676, r2: 0.2285. lr: 0.0001 \n",
      "Epoch 2850. TRAIN: loss 304.1524, r2: 0.2278. lr: 0.0001 \n",
      "Epoch 2860. TRAIN: loss 304.0533, r2: 0.2280. lr: 0.0001 \n",
      "Epoch 2870. TRAIN: loss 304.2799, r2: 0.2275. lr: 0.0001 \n",
      "Epoch 2880. TRAIN: loss 303.4001, r2: 0.2297. lr: 0.0001 \n",
      "Epoch 2890. TRAIN: loss 304.2227, r2: 0.2276. lr: 0.0001 \n",
      "Epoch 2900. TRAIN: loss 303.0460, r2: 0.2306. lr: 0.0001 \n",
      "Epoch 2910. TRAIN: loss 303.7947, r2: 0.2287. lr: 0.0001 \n",
      "Epoch 2920. TRAIN: loss 303.5682, r2: 0.2293. lr: 0.0001 \n",
      "Epoch 2930. TRAIN: loss 302.8159, r2: 0.2312. lr: 0.0001 \n",
      "Epoch 2940. TRAIN: loss 302.3632, r2: 0.2323. lr: 0.0001 \n",
      "Epoch 2950. TRAIN: loss 302.3970, r2: 0.2323. lr: 0.0001 \n",
      "Epoch 2960. TRAIN: loss 302.4548, r2: 0.2321. lr: 0.0001 \n",
      "Epoch 2970. TRAIN: loss 301.8856, r2: 0.2336. lr: 0.0001 \n",
      "Epoch 2980. TRAIN: loss 303.1641, r2: 0.2303. lr: 0.0001 \n",
      "Epoch 2990. TRAIN: loss 302.7440, r2: 0.2314. lr: 0.0001 \n",
      "Epoch 3000. TRAIN: loss 309.7072, r2: 0.2137. lr: 0.0001 \n",
      "Epoch 3010. TRAIN: loss 310.8181, r2: 0.2109. lr: 0.0001 \n",
      "Epoch 3020. TRAIN: loss 315.2053, r2: 0.1997. lr: 0.0001 \n",
      "Epoch 3030. TRAIN: loss 313.1277, r2: 0.2050. lr: 0.0001 \n",
      "Epoch 3040. TRAIN: loss 307.2137, r2: 0.2200. lr: 0.0001 \n",
      "Epoch 3050. TRAIN: loss 322.1540, r2: 0.1821. lr: 0.0001 \n",
      "Epoch 3060. TRAIN: loss 319.9669, r2: 0.1876. lr: 0.0001 \n",
      "Epoch 3070. TRAIN: loss 313.5119, r2: 0.2040. lr: 0.0001 \n",
      "Epoch 3080. TRAIN: loss 310.9594, r2: 0.2105. lr: 0.0001 \n",
      "Epoch 3090. TRAIN: loss 312.4941, r2: 0.2066. lr: 0.0001 \n",
      "Epoch 3100. TRAIN: loss 305.9488, r2: 0.2232. lr: 0.0001 \n",
      "Epoch 3110. TRAIN: loss 305.4425, r2: 0.2245. lr: 0.0001 \n",
      "Epoch 3120. TRAIN: loss 306.0269, r2: 0.2230. lr: 0.0001 \n",
      "Epoch 3130. TRAIN: loss 303.3448, r2: 0.2298. lr: 0.0001 \n",
      "Epoch 3140. TRAIN: loss 304.6936, r2: 0.2264. lr: 0.0001 \n",
      "Epoch 3150. TRAIN: loss 303.5489, r2: 0.2293. lr: 0.0001 \n",
      "Epoch 3160. TRAIN: loss 306.6718, r2: 0.2214. lr: 0.0001 \n",
      "Epoch 3170. TRAIN: loss 311.2347, r2: 0.2098. lr: 0.0001 \n",
      "Epoch 3180. TRAIN: loss 306.7916, r2: 0.2211. lr: 0.0001 \n",
      "Epoch 3190. TRAIN: loss 306.3535, r2: 0.2222. lr: 0.0001 \n",
      "Epoch 3200. TRAIN: loss 305.6285, r2: 0.2241. lr: 0.0001 \n",
      "Epoch 3210. TRAIN: loss 306.6800, r2: 0.2214. lr: 0.0001 \n",
      "Epoch 3220. TRAIN: loss 305.8885, r2: 0.2234. lr: 0.0001 \n",
      "Epoch 3230. TRAIN: loss 304.9140, r2: 0.2259. lr: 0.0001 \n",
      "Epoch 3240. TRAIN: loss 304.2544, r2: 0.2275. lr: 0.0001 \n",
      "Epoch 3250. TRAIN: loss 306.5974, r2: 0.2216. lr: 0.0001 \n",
      "Epoch 3260. TRAIN: loss 306.1944, r2: 0.2226. lr: 0.0001 \n",
      "Epoch 3270. TRAIN: loss 304.2760, r2: 0.2275. lr: 0.0001 \n",
      "Epoch 3280. TRAIN: loss 302.8759, r2: 0.2310. lr: 0.0001 \n",
      "Epoch 3290. TRAIN: loss 303.8925, r2: 0.2285. lr: 0.0001 \n",
      "Epoch 3300. TRAIN: loss 303.4539, r2: 0.2296. lr: 0.0001 \n",
      "Epoch 3310. TRAIN: loss 302.2908, r2: 0.2325. lr: 0.0001 \n",
      "Epoch 3320. TRAIN: loss 303.2010, r2: 0.2302. lr: 0.0001 \n",
      "Epoch 3330. TRAIN: loss 302.1202, r2: 0.2330. lr: 0.0001 \n",
      "Epoch 3340. TRAIN: loss 301.8119, r2: 0.2337. lr: 0.0001 \n",
      "Epoch 3350. TRAIN: loss 302.3556, r2: 0.2324. lr: 0.0001 \n",
      "Epoch 3360. TRAIN: loss 301.4191, r2: 0.2347. lr: 0.0001 \n",
      "Epoch 3370. TRAIN: loss 302.7617, r2: 0.2313. lr: 0.0001 \n",
      "Epoch 3380. TRAIN: loss 301.6562, r2: 0.2341. lr: 0.0001 \n",
      "Epoch 3390. TRAIN: loss 303.0005, r2: 0.2307. lr: 0.0001 \n",
      "Epoch 3400. TRAIN: loss 301.0274, r2: 0.2357. lr: 0.0001 \n",
      "Epoch 3410. TRAIN: loss 301.2001, r2: 0.2353. lr: 0.0001 \n",
      "Epoch 3420. TRAIN: loss 301.1076, r2: 0.2355. lr: 0.0001 \n",
      "Epoch 3430. TRAIN: loss 301.4200, r2: 0.2347. lr: 0.0001 \n",
      "Epoch 3440. TRAIN: loss 300.1328, r2: 0.2380. lr: 0.0001 \n",
      "Epoch 3450. TRAIN: loss 300.2493, r2: 0.2377. lr: 0.0001 \n",
      "Epoch 3460. TRAIN: loss 300.2776, r2: 0.2376. lr: 0.0001 \n",
      "Epoch 3470. TRAIN: loss 299.4276, r2: 0.2398. lr: 0.0001 \n",
      "Epoch 3480. TRAIN: loss 301.6864, r2: 0.2341. lr: 0.0001 \n",
      "Epoch 3490. TRAIN: loss 299.5534, r2: 0.2395. lr: 0.0001 \n",
      "Epoch 3500. TRAIN: loss 304.9721, r2: 0.2257. lr: 0.0001 \n",
      "Epoch 3510. TRAIN: loss 308.3184, r2: 0.2172. lr: 0.0001 \n",
      "Epoch 3520. TRAIN: loss 312.6291, r2: 0.2063. lr: 0.0001 \n",
      "Epoch 3530. TRAIN: loss 306.0288, r2: 0.2230. lr: 0.0001 \n",
      "Epoch 3540. TRAIN: loss 305.6505, r2: 0.2240. lr: 0.0001 \n",
      "Epoch 3550. TRAIN: loss 306.0735, r2: 0.2229. lr: 0.0001 \n",
      "Epoch 3560. TRAIN: loss 304.9370, r2: 0.2258. lr: 0.0001 \n",
      "Epoch 3570. TRAIN: loss 307.1622, r2: 0.2202. lr: 0.0001 \n",
      "Epoch 3580. TRAIN: loss 302.4203, r2: 0.2322. lr: 0.0001 \n",
      "Epoch 3590. TRAIN: loss 302.1497, r2: 0.2329. lr: 0.0001 \n",
      "Epoch 3600. TRAIN: loss 300.8319, r2: 0.2362. lr: 0.0001 \n",
      "Epoch 3610. TRAIN: loss 301.2019, r2: 0.2353. lr: 0.0001 \n",
      "Epoch 3620. TRAIN: loss 299.4565, r2: 0.2397. lr: 0.0001 \n",
      "Epoch 3630. TRAIN: loss 297.9034, r2: 0.2437. lr: 0.0001 \n",
      "Epoch 3640. TRAIN: loss 297.7470, r2: 0.2441. lr: 0.0001 \n",
      "Epoch 3650. TRAIN: loss 301.2355, r2: 0.2352. lr: 0.0001 \n",
      "Epoch 3660. TRAIN: loss 298.5373, r2: 0.2421. lr: 0.0001 \n",
      "Epoch 3670. TRAIN: loss 299.7915, r2: 0.2389. lr: 0.0001 \n",
      "Epoch 3680. TRAIN: loss 298.1509, r2: 0.2430. lr: 0.0001 \n",
      "Epoch 3690. TRAIN: loss 298.5334, r2: 0.2421. lr: 0.0001 \n",
      "Epoch 3700. TRAIN: loss 298.7749, r2: 0.2415. lr: 0.0001 \n",
      "Epoch 3710. TRAIN: loss 297.2879, r2: 0.2452. lr: 0.0001 \n",
      "Epoch 3720. TRAIN: loss 297.0678, r2: 0.2458. lr: 0.0001 \n",
      "Epoch 3730. TRAIN: loss 296.7307, r2: 0.2466. lr: 0.0001 \n",
      "Epoch 3740. TRAIN: loss 295.4228, r2: 0.2500. lr: 0.0001 \n",
      "Epoch 3750. TRAIN: loss 297.4990, r2: 0.2447. lr: 0.0001 \n",
      "Epoch 3760. TRAIN: loss 297.0953, r2: 0.2457. lr: 0.0001 \n",
      "Epoch 3770. TRAIN: loss 296.5605, r2: 0.2471. lr: 0.0001 \n",
      "Epoch 3780. TRAIN: loss 295.7087, r2: 0.2492. lr: 0.0001 \n",
      "Epoch 3790. TRAIN: loss 295.6784, r2: 0.2493. lr: 0.0001 \n",
      "Epoch 3800. TRAIN: loss 296.0719, r2: 0.2483. lr: 0.0001 \n",
      "Epoch 3810. TRAIN: loss 294.9887, r2: 0.2511. lr: 0.0001 \n",
      "Epoch 3820. TRAIN: loss 294.8882, r2: 0.2513. lr: 0.0001 \n",
      "Epoch 3830. TRAIN: loss 296.3654, r2: 0.2476. lr: 0.0001 \n",
      "Epoch 3840. TRAIN: loss 294.1925, r2: 0.2531. lr: 0.0001 \n",
      "Epoch 3850. TRAIN: loss 294.7090, r2: 0.2518. lr: 0.0001 \n",
      "Epoch 3860. TRAIN: loss 295.3698, r2: 0.2501. lr: 0.0001 \n",
      "Epoch 3870. TRAIN: loss 295.5833, r2: 0.2496. lr: 0.0001 \n",
      "Epoch 3880. TRAIN: loss 294.4397, r2: 0.2525. lr: 0.0001 \n",
      "Epoch 3890. TRAIN: loss 294.5633, r2: 0.2521. lr: 0.0001 \n",
      "Epoch 3900. TRAIN: loss 293.4904, r2: 0.2549. lr: 0.0001 \n",
      "Epoch 3910. TRAIN: loss 294.9662, r2: 0.2511. lr: 0.0001 \n",
      "Epoch 3920. TRAIN: loss 294.3543, r2: 0.2527. lr: 0.0001 \n",
      "Epoch 3930. TRAIN: loss 292.9492, r2: 0.2562. lr: 0.0001 \n",
      "Epoch 3940. TRAIN: loss 293.8551, r2: 0.2539. lr: 0.0001 \n",
      "Epoch 3950. TRAIN: loss 294.8712, r2: 0.2514. lr: 0.0001 \n",
      "Epoch 3960. TRAIN: loss 294.8703, r2: 0.2514. lr: 0.0001 \n",
      "Epoch 3970. TRAIN: loss 293.4875, r2: 0.2549. lr: 0.0001 \n",
      "Epoch 3980. TRAIN: loss 293.4515, r2: 0.2550. lr: 0.0001 \n",
      "Epoch 3990. TRAIN: loss 294.3490, r2: 0.2527. lr: 0.0001 \n",
      "Epoch 4000. TRAIN: loss 294.3739, r2: 0.2526. lr: 0.0001 \n",
      "Epoch 4010. TRAIN: loss 293.9343, r2: 0.2537. lr: 0.0001 \n",
      "Epoch 4020. TRAIN: loss 293.0126, r2: 0.2561. lr: 0.0001 \n",
      "Epoch 4030. TRAIN: loss 292.9320, r2: 0.2563. lr: 0.0001 \n",
      "Epoch 4040. TRAIN: loss 292.0187, r2: 0.2586. lr: 0.0001 \n",
      "Epoch 4050. TRAIN: loss 294.0786, r2: 0.2534. lr: 0.0001 \n",
      "Epoch 4060. TRAIN: loss 292.7401, r2: 0.2568. lr: 0.0001 \n",
      "Epoch 4070. TRAIN: loss 292.6022, r2: 0.2571. lr: 0.0001 \n",
      "Epoch 4080. TRAIN: loss 291.9138, r2: 0.2589. lr: 0.0001 \n",
      "Epoch 4090. TRAIN: loss 292.8969, r2: 0.2564. lr: 0.0001 \n",
      "Epoch 4100. TRAIN: loss 291.8494, r2: 0.2590. lr: 0.0001 \n",
      "Epoch 4110. TRAIN: loss 291.7480, r2: 0.2593. lr: 0.0001 \n",
      "Epoch 4120. TRAIN: loss 292.3968, r2: 0.2576. lr: 0.0001 \n",
      "Epoch 4130. TRAIN: loss 292.6051, r2: 0.2571. lr: 0.0001 \n",
      "Epoch 4140. TRAIN: loss 291.7376, r2: 0.2593. lr: 0.0001 \n",
      "Epoch 4150. TRAIN: loss 291.1133, r2: 0.2609. lr: 0.0001 \n",
      "Epoch 4160. TRAIN: loss 291.6791, r2: 0.2595. lr: 0.0001 \n",
      "Epoch 4170. TRAIN: loss 292.1298, r2: 0.2583. lr: 0.0001 \n",
      "Epoch 4180. TRAIN: loss 291.3479, r2: 0.2603. lr: 0.0001 \n",
      "Epoch 4190. TRAIN: loss 291.8966, r2: 0.2589. lr: 0.0001 \n",
      "Epoch 4200. TRAIN: loss 290.5077, r2: 0.2624. lr: 0.0001 \n",
      "Epoch 4210. TRAIN: loss 290.3084, r2: 0.2629. lr: 0.0001 \n",
      "Epoch 4220. TRAIN: loss 290.2051, r2: 0.2632. lr: 0.0001 \n",
      "Epoch 4230. TRAIN: loss 291.4903, r2: 0.2599. lr: 0.0001 \n",
      "Epoch 4240. TRAIN: loss 290.6074, r2: 0.2622. lr: 0.0001 \n",
      "Epoch 4250. TRAIN: loss 290.8888, r2: 0.2615. lr: 0.0001 \n",
      "Epoch 4260. TRAIN: loss 290.5903, r2: 0.2622. lr: 0.0001 \n",
      "Epoch 4270. TRAIN: loss 290.5132, r2: 0.2624. lr: 0.0001 \n",
      "Epoch 4280. TRAIN: loss 289.2117, r2: 0.2657. lr: 0.0001 \n",
      "Epoch 4290. TRAIN: loss 290.3037, r2: 0.2630. lr: 0.0001 \n",
      "Epoch 4300. TRAIN: loss 288.8082, r2: 0.2668. lr: 0.0001 \n",
      "Epoch 4310. TRAIN: loss 288.8445, r2: 0.2667. lr: 0.0001 \n",
      "Epoch 4320. TRAIN: loss 289.6783, r2: 0.2645. lr: 0.0001 \n",
      "Epoch 4330. TRAIN: loss 290.6559, r2: 0.2621. lr: 0.0001 \n",
      "Epoch 4340. TRAIN: loss 289.3568, r2: 0.2654. lr: 0.0001 \n",
      "Epoch 4350. TRAIN: loss 289.3159, r2: 0.2655. lr: 0.0001 \n",
      "Epoch 4360. TRAIN: loss 289.4956, r2: 0.2650. lr: 0.0001 \n",
      "Epoch 4370. TRAIN: loss 289.5373, r2: 0.2649. lr: 0.0001 \n",
      "Epoch 4380. TRAIN: loss 289.0429, r2: 0.2662. lr: 0.0001 \n",
      "Epoch 4390. TRAIN: loss 287.6353, r2: 0.2697. lr: 0.0001 \n",
      "Epoch 4400. TRAIN: loss 288.6935, r2: 0.2670. lr: 0.0001 \n",
      "Epoch 4410. TRAIN: loss 289.5966, r2: 0.2648. lr: 0.0001 \n",
      "Epoch 4420. TRAIN: loss 287.4820, r2: 0.2701. lr: 0.0001 \n",
      "Epoch 4430. TRAIN: loss 287.3927, r2: 0.2703. lr: 0.0001 \n",
      "Epoch 4440. TRAIN: loss 288.3727, r2: 0.2679. lr: 0.0001 \n",
      "Epoch 4450. TRAIN: loss 288.0222, r2: 0.2688. lr: 0.0001 \n",
      "Epoch 4460. TRAIN: loss 289.7971, r2: 0.2642. lr: 0.0001 \n",
      "Epoch 4470. TRAIN: loss 286.9199, r2: 0.2715. lr: 0.0001 \n",
      "Epoch 4480. TRAIN: loss 288.2305, r2: 0.2682. lr: 0.0001 \n",
      "Epoch 4490. TRAIN: loss 287.6186, r2: 0.2698. lr: 0.0001 \n",
      "Epoch 4500. TRAIN: loss 286.8932, r2: 0.2716. lr: 0.0001 \n",
      "Epoch 4510. TRAIN: loss 287.5179, r2: 0.2700. lr: 0.0001 \n",
      "Epoch 4520. TRAIN: loss 287.3861, r2: 0.2704. lr: 0.0001 \n",
      "Epoch 4530. TRAIN: loss 287.2939, r2: 0.2706. lr: 0.0001 \n",
      "Epoch 4540. TRAIN: loss 287.7167, r2: 0.2695. lr: 0.0001 \n",
      "Epoch 4550. TRAIN: loss 286.9910, r2: 0.2714. lr: 0.0001 \n",
      "Epoch 4560. TRAIN: loss 287.4506, r2: 0.2702. lr: 0.0001 \n",
      "Epoch 4570. TRAIN: loss 286.1585, r2: 0.2735. lr: 0.0001 \n",
      "Epoch 4580. TRAIN: loss 286.5353, r2: 0.2725. lr: 0.0001 \n",
      "Epoch 4590. TRAIN: loss 286.7896, r2: 0.2719. lr: 0.0001 \n",
      "Epoch 4600. TRAIN: loss 287.8824, r2: 0.2691. lr: 0.0001 \n",
      "Epoch 4610. TRAIN: loss 289.3648, r2: 0.2653. lr: 0.0001 \n",
      "Epoch 4620. TRAIN: loss 295.3403, r2: 0.2502. lr: 0.0001 \n",
      "Epoch 4630. TRAIN: loss 288.1247, r2: 0.2685. lr: 0.0001 \n",
      "Epoch 4640. TRAIN: loss 289.2719, r2: 0.2656. lr: 0.0001 \n",
      "Epoch 4650. TRAIN: loss 292.4550, r2: 0.2575. lr: 0.0001 \n",
      "Epoch 4660. TRAIN: loss 289.6627, r2: 0.2646. lr: 0.0001 \n",
      "Epoch 4670. TRAIN: loss 305.6839, r2: 0.2239. lr: 0.0001 \n",
      "Epoch 4680. TRAIN: loss 306.7681, r2: 0.2212. lr: 0.0001 \n",
      "Epoch 4690. TRAIN: loss 306.8871, r2: 0.2209. lr: 0.0001 \n",
      "Epoch 4700. TRAIN: loss 304.7015, r2: 0.2264. lr: 0.0001 \n",
      "Epoch 4710. TRAIN: loss 303.0411, r2: 0.2306. lr: 0.0001 \n",
      "Epoch 4720. TRAIN: loss 299.8586, r2: 0.2387. lr: 0.0001 \n",
      "Epoch 4730. TRAIN: loss 298.0255, r2: 0.2434. lr: 0.0001 \n",
      "Epoch 4740. TRAIN: loss 293.8400, r2: 0.2540. lr: 0.0001 \n",
      "Epoch 4750. TRAIN: loss 290.3201, r2: 0.2629. lr: 0.0001 \n",
      "Epoch 4760. TRAIN: loss 294.9939, r2: 0.2511. lr: 0.0001 \n",
      "Epoch 4770. TRAIN: loss 292.6354, r2: 0.2570. lr: 0.0001 \n",
      "Epoch 4780. TRAIN: loss 298.9304, r2: 0.2411. lr: 0.0001 \n",
      "Epoch 4790. TRAIN: loss 298.7960, r2: 0.2414. lr: 0.0001 \n",
      "Epoch 4800. TRAIN: loss 294.4352, r2: 0.2525. lr: 0.0001 \n",
      "Epoch 4810. TRAIN: loss 288.6628, r2: 0.2671. lr: 0.0001 \n",
      "Epoch 4820. TRAIN: loss 289.3255, r2: 0.2654. lr: 0.0001 \n",
      "Epoch 4830. TRAIN: loss 288.4837, r2: 0.2676. lr: 0.0001 \n",
      "Epoch 4840. TRAIN: loss 287.0117, r2: 0.2713. lr: 0.0001 \n",
      "Epoch 4850. TRAIN: loss 286.9739, r2: 0.2714. lr: 0.0001 \n",
      "Epoch 4860. TRAIN: loss 285.8909, r2: 0.2742. lr: 0.0001 \n",
      "Epoch 4870. TRAIN: loss 287.0872, r2: 0.2711. lr: 0.0001 \n",
      "Epoch 4880. TRAIN: loss 287.5967, r2: 0.2698. lr: 0.0001 \n",
      "Epoch 4890. TRAIN: loss 286.7531, r2: 0.2720. lr: 0.0001 \n",
      "Epoch 4900. TRAIN: loss 285.9380, r2: 0.2740. lr: 0.0001 \n",
      "Epoch 4910. TRAIN: loss 285.8535, r2: 0.2743. lr: 0.0001 \n",
      "Epoch 4920. TRAIN: loss 287.7734, r2: 0.2694. lr: 0.0001 \n",
      "Epoch 4930. TRAIN: loss 287.8003, r2: 0.2693. lr: 0.0001 \n",
      "Epoch 4940. TRAIN: loss 284.1411, r2: 0.2786. lr: 0.0001 \n",
      "Epoch 4950. TRAIN: loss 287.0287, r2: 0.2713. lr: 0.0001 \n",
      "Epoch 4960. TRAIN: loss 286.5173, r2: 0.2726. lr: 0.0001 \n",
      "Epoch 4970. TRAIN: loss 285.8836, r2: 0.2742. lr: 0.0001 \n",
      "Epoch 4980. TRAIN: loss 284.2662, r2: 0.2783. lr: 0.0001 \n",
      "Epoch 4990. TRAIN: loss 285.9885, r2: 0.2739. lr: 0.0001 \n",
      "Epoch 5000. TRAIN: loss 284.5693, r2: 0.2775. lr: 0.0001 \n"
     ]
    }
   ],
   "source": [
    "datetime_now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = \"./logs/\" + datetime_now\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "trained_model = train_func(dataset.data, model_v1, 5000, writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
