{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/essdata/IDU/venvs/common_venv/lib/python3.9/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.10.3-CAPI-1.16.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from shapely import wkt\n",
    "from tqdm import tqdm\n",
    "from geopy.distance import great_circle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MigrationDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(MigrationDataset, self).__init__(root, transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        files = []\n",
    "        for file in os.listdir(self.root):\n",
    "            if file.endswith(\".geojson\") or file.endswith(\".csv\"):\n",
    "                files.append(file)\n",
    "        return files\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['migration_dataset']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        \n",
    "        # read files in specified folder\n",
    "        cities = gpd.read_file(os.path.join(self.root, \"cities_aggregated.geojson\")).set_index(\"city\")\n",
    "        responses = gpd.read_file(os.path.join(self.root, \"responses_clustered.csv\"))\n",
    "\n",
    "\n",
    "        # extract cities features \n",
    "        cities_features = cities[[\n",
    "            \"population\",\n",
    "            \"city_category\", \n",
    "            \"harsh_climate\", \n",
    "            \"ueqi_score\", \n",
    "            \"ueqi_residential\", \n",
    "            \"ueqi_street_networks\", \n",
    "            \"ueqi_green_spaces\", \n",
    "            \"ueqi_public_and_business_infrastructure\", \n",
    "            \"ueqi_social_and_leisure_infrastructure\",\n",
    "            \"ueqi_citywide_space\",\n",
    "            \"cvs_total\",\n",
    "            \"vacancies_total\",\n",
    "            \"factories_count\"\n",
    "            ]]\n",
    "\n",
    "        # encode categorical features\n",
    "        one_hot = OneHotEncoder()\n",
    "        encoded_category = one_hot.fit_transform(np.expand_dims(cities[\"city_category\"].to_numpy(), 1)).toarray()\n",
    "        encoded_category_names = one_hot.get_feature_names_out([\"category\"])\n",
    "        cities_features.loc[:, encoded_category_names] = encoded_category\n",
    "        cities_features = cities_features.drop([\"city_category\"], axis=1)\n",
    "        cities_features[\"harsh_climate\"] = cities_features[\"harsh_climate\"].astype(int)\n",
    "\n",
    "        # form distance matrix\n",
    "        DM = cities[\"geometry\"].progress_apply(\n",
    "            lambda p1: cities[\"geometry\"].apply(\n",
    "                lambda p2: great_circle(p1.coords[0], p2.coords[0]).km\n",
    "                ))\n",
    "\n",
    "        # form origin-destination matrix\n",
    "\n",
    "        responses_counts = responses.groupby([\"cluster_center_cv\", \"cluster_center_vacancy\"])[\"id_candidate\"].count()\n",
    "        responses_cities = responses_counts.index.get_level_values(0).drop_duplicates()\n",
    "        OD = pd.DataFrame(None, index=DM.columns, columns=DM.columns)\n",
    "        OD = OD.progress_apply(\n",
    "            lambda city: city.fillna(responses_counts[city.name]).fillna(0) \n",
    "            if city.name in responses_cities else city.fillna(0)\n",
    "            )\n",
    "        \n",
    "        # transform data\n",
    "        \n",
    "        cities_num = len(OD)\n",
    "        edge_index = [[], []]\n",
    "        for i in range(cities_num):\n",
    "            edge_index[0].extend([i for j in range(cities_num)])\n",
    "            edge_index[1].extend([j for j in range(cities_num)])\n",
    "\n",
    "        edge_index = torch.tensor(edge_index)\n",
    "        y = torch.tensor(np.concatenate((OD.to_numpy())), dtype=torch.float32)\n",
    "        edge_attr = torch.tensor(np.concatenate((DM.to_numpy())), dtype=torch.float32)\n",
    "        x = torch.tensor(cities_features.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "        # exclude diagonal\n",
    "        non_diagonal = edge_attr > 0\n",
    "        edge_attr = edge_attr[non_diagonal]\n",
    "        edge_index = edge_index[:, non_diagonal]\n",
    "        y = y[non_diagonal]\n",
    "        \n",
    "        # create torch object          \n",
    "        graph = Data(x=x,edge_index=edge_index, y=y, edge_attr=edge_attr)\n",
    "        \n",
    "        data_list = []\n",
    "        data_list.append(graph)\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be found here: https://disk.yandex.ru/client/disk/%D0%A2%D1%80%D1%83%D0%B4%D0%BE%D0%B2%D1%8B%D0%B5%20%D1%80%D0%B5%D1%81%D1%83%D1%80%D1%81%D1%8B/%D0%A4%D0%B0%D0%B9%D0%BB%D1%8B/ML_experiments/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MigrationDataset(\"/var/essdata/IDU/other/mm_22/industrial-location/ml/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super(FNN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.lins.append(nn.Linear(input_dim, hidden_dim))\n",
    "        for _ in range(self.num_layers - 2):\n",
    "            self.lins.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.lins.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.norm = nn.ModuleList()\n",
    "        for l in range(self.num_layers):\n",
    "            self.norm.append(nn.LayerNorm(hidden_dim))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_attr.unsqueeze(-1)\n",
    "\n",
    "        x_s = x[edge_index[0]]\n",
    "        x_d = x[edge_index[1]]\n",
    "        y = torch.cat((x_s, x_d, edge_weight), axis=1)\n",
    "        # y = F.normalize(y)\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            y = self.lins[i](y) \n",
    "            y = nn.functional.leaky_relu(y)\n",
    "            y = F.dropout(y, p=self.dropout, training=self.training)\n",
    "            y = self.norm[i](y)\n",
    "\n",
    "        y = self.lins[-1](y)\n",
    "        y = torch.relu(y).squeeze()\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_loss(output, target):\n",
    "    target_mean = torch.mean(target)\n",
    "    ss_tot = torch.sum((target - target_mean) ** 2)\n",
    "    ss_res = torch.sum((target - output) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2\n",
    "\n",
    "def train_func(dataset, model, epochs, writer):\n",
    "\n",
    "    optimize = torch.optim.Adam(list(model.parameters()),  lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimize, factor=0.9, min_lr=0.0005)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(epochs + 1):\n",
    "\n",
    "        optimize.zero_grad()\n",
    "        model.train()\n",
    "        y_hat = model(dataset)\n",
    "\n",
    "        loss = F.mse_loss(y_hat, dataset.y)\n",
    "        r2 = r2_loss(y_hat, dataset.y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimize.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        t_metrics = {\"train_loss\": loss, \"train_r2\": r2}  \n",
    "        for name, v_metric in t_metrics.items(): writer.add_scalar(name, v_metric, epoch)\n",
    "\n",
    "        if epoch % 10 == 0:    \n",
    "            print(\n",
    "                \"Epoch {}. TRAIN: loss {:.4f}, r2: {:.4f}. lr: {:.4f} \".format(\n",
    "                    epoch, t_metrics[\"train_loss\"], t_metrics[\"train_r2\"], optimize.param_groups[0][\"lr\"]\n",
    "                    )\n",
    "                ) \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = dataset.data.x.shape[1] * 2 + 1\n",
    "hidden_dim = 256\n",
    "num_layers = 5\n",
    "dropout = 0.2\n",
    "\n",
    "model_v1 = FNN(input_dim, hidden_dim, num_layers, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. TRAIN: loss 394.2522, r2: -0.0010. lr: 0.0010 \n",
      "Epoch 10. TRAIN: loss 393.5278, r2: 0.0009. lr: 0.0010 \n",
      "Epoch 20. TRAIN: loss 393.1065, r2: 0.0020. lr: 0.0010 \n",
      "Epoch 30. TRAIN: loss 392.8091, r2: 0.0027. lr: 0.0010 \n",
      "Epoch 40. TRAIN: loss 392.2228, r2: 0.0042. lr: 0.0010 \n",
      "Epoch 50. TRAIN: loss 391.5364, r2: 0.0059. lr: 0.0010 \n",
      "Epoch 60. TRAIN: loss 390.3962, r2: 0.0088. lr: 0.0010 \n",
      "Epoch 70. TRAIN: loss 389.1128, r2: 0.0121. lr: 0.0010 \n",
      "Epoch 80. TRAIN: loss 387.6410, r2: 0.0158. lr: 0.0010 \n",
      "Epoch 90. TRAIN: loss 386.1702, r2: 0.0196. lr: 0.0010 \n",
      "Epoch 100. TRAIN: loss 384.3513, r2: 0.0242. lr: 0.0010 \n",
      "Epoch 110. TRAIN: loss 382.6587, r2: 0.0285. lr: 0.0010 \n",
      "Epoch 120. TRAIN: loss 380.9814, r2: 0.0327. lr: 0.0010 \n",
      "Epoch 130. TRAIN: loss 379.1213, r2: 0.0375. lr: 0.0010 \n",
      "Epoch 140. TRAIN: loss 377.1573, r2: 0.0424. lr: 0.0010 \n",
      "Epoch 150. TRAIN: loss 375.3437, r2: 0.0471. lr: 0.0010 \n",
      "Epoch 160. TRAIN: loss 375.1427, r2: 0.0476. lr: 0.0010 \n",
      "Epoch 170. TRAIN: loss 374.1987, r2: 0.0500. lr: 0.0010 \n",
      "Epoch 180. TRAIN: loss 370.2997, r2: 0.0599. lr: 0.0010 \n",
      "Epoch 190. TRAIN: loss 368.4201, r2: 0.0646. lr: 0.0010 \n",
      "Epoch 200. TRAIN: loss 366.6616, r2: 0.0691. lr: 0.0010 \n",
      "Epoch 210. TRAIN: loss 364.5738, r2: 0.0744. lr: 0.0010 \n",
      "Epoch 220. TRAIN: loss 362.6312, r2: 0.0793. lr: 0.0010 \n",
      "Epoch 230. TRAIN: loss 360.8778, r2: 0.0838. lr: 0.0010 \n",
      "Epoch 240. TRAIN: loss 359.1950, r2: 0.0881. lr: 0.0010 \n",
      "Epoch 250. TRAIN: loss 357.2093, r2: 0.0931. lr: 0.0010 \n",
      "Epoch 260. TRAIN: loss 355.8017, r2: 0.0967. lr: 0.0010 \n",
      "Epoch 270. TRAIN: loss 354.1273, r2: 0.1009. lr: 0.0010 \n",
      "Epoch 280. TRAIN: loss 352.0866, r2: 0.1061. lr: 0.0010 \n",
      "Epoch 290. TRAIN: loss 350.1346, r2: 0.1111. lr: 0.0010 \n",
      "Epoch 300. TRAIN: loss 349.7906, r2: 0.1119. lr: 0.0010 \n",
      "Epoch 310. TRAIN: loss 348.3705, r2: 0.1155. lr: 0.0010 \n",
      "Epoch 320. TRAIN: loss 345.7498, r2: 0.1222. lr: 0.0010 \n",
      "Epoch 330. TRAIN: loss 344.0347, r2: 0.1265. lr: 0.0010 \n",
      "Epoch 340. TRAIN: loss 342.4461, r2: 0.1306. lr: 0.0010 \n",
      "Epoch 350. TRAIN: loss 339.5687, r2: 0.1379. lr: 0.0010 \n",
      "Epoch 360. TRAIN: loss 335.2974, r2: 0.1487. lr: 0.0010 \n",
      "Epoch 370. TRAIN: loss 333.6940, r2: 0.1528. lr: 0.0010 \n",
      "Epoch 380. TRAIN: loss 331.0846, r2: 0.1594. lr: 0.0010 \n",
      "Epoch 390. TRAIN: loss 329.0598, r2: 0.1646. lr: 0.0010 \n",
      "Epoch 400. TRAIN: loss 326.1772, r2: 0.1719. lr: 0.0010 \n",
      "Epoch 410. TRAIN: loss 324.1085, r2: 0.1771. lr: 0.0010 \n",
      "Epoch 420. TRAIN: loss 321.5289, r2: 0.1837. lr: 0.0010 \n",
      "Epoch 430. TRAIN: loss 319.1657, r2: 0.1897. lr: 0.0010 \n",
      "Epoch 440. TRAIN: loss 316.7511, r2: 0.1958. lr: 0.0010 \n",
      "Epoch 450. TRAIN: loss 314.1758, r2: 0.2024. lr: 0.0010 \n",
      "Epoch 460. TRAIN: loss 311.6725, r2: 0.2087. lr: 0.0010 \n",
      "Epoch 470. TRAIN: loss 309.2911, r2: 0.2148. lr: 0.0010 \n",
      "Epoch 480. TRAIN: loss 307.1927, r2: 0.2201. lr: 0.0010 \n",
      "Epoch 490. TRAIN: loss 307.0199, r2: 0.2205. lr: 0.0010 \n",
      "Epoch 500. TRAIN: loss 304.1366, r2: 0.2278. lr: 0.0010 \n",
      "Epoch 510. TRAIN: loss 301.4553, r2: 0.2346. lr: 0.0010 \n",
      "Epoch 520. TRAIN: loss 299.4631, r2: 0.2397. lr: 0.0010 \n",
      "Epoch 530. TRAIN: loss 297.1258, r2: 0.2456. lr: 0.0010 \n",
      "Epoch 540. TRAIN: loss 295.0825, r2: 0.2508. lr: 0.0010 \n",
      "Epoch 550. TRAIN: loss 293.0417, r2: 0.2560. lr: 0.0010 \n",
      "Epoch 560. TRAIN: loss 291.2239, r2: 0.2606. lr: 0.0010 \n",
      "Epoch 570. TRAIN: loss 289.4677, r2: 0.2651. lr: 0.0010 \n",
      "Epoch 580. TRAIN: loss 287.2778, r2: 0.2706. lr: 0.0010 \n",
      "Epoch 590. TRAIN: loss 285.4391, r2: 0.2753. lr: 0.0010 \n",
      "Epoch 600. TRAIN: loss 283.9586, r2: 0.2791. lr: 0.0010 \n",
      "Epoch 610. TRAIN: loss 282.2582, r2: 0.2834. lr: 0.0010 \n",
      "Epoch 620. TRAIN: loss 279.9584, r2: 0.2892. lr: 0.0010 \n",
      "Epoch 630. TRAIN: loss 278.2959, r2: 0.2934. lr: 0.0010 \n",
      "Epoch 640. TRAIN: loss 276.3423, r2: 0.2984. lr: 0.0010 \n",
      "Epoch 650. TRAIN: loss 275.4451, r2: 0.3007. lr: 0.0010 \n",
      "Epoch 660. TRAIN: loss 274.2309, r2: 0.3038. lr: 0.0010 \n",
      "Epoch 670. TRAIN: loss 271.0913, r2: 0.3117. lr: 0.0010 \n",
      "Epoch 680. TRAIN: loss 269.3525, r2: 0.3162. lr: 0.0010 \n",
      "Epoch 690. TRAIN: loss 269.4375, r2: 0.3159. lr: 0.0010 \n",
      "Epoch 700. TRAIN: loss 268.1297, r2: 0.3193. lr: 0.0009 \n",
      "Epoch 710. TRAIN: loss 266.2494, r2: 0.3240. lr: 0.0009 \n",
      "Epoch 720. TRAIN: loss 265.0797, r2: 0.3270. lr: 0.0009 \n",
      "Epoch 730. TRAIN: loss 263.8641, r2: 0.3301. lr: 0.0009 \n",
      "Epoch 740. TRAIN: loss 261.2173, r2: 0.3368. lr: 0.0009 \n",
      "Epoch 750. TRAIN: loss 260.2746, r2: 0.3392. lr: 0.0009 \n",
      "Epoch 760. TRAIN: loss 258.0541, r2: 0.3448. lr: 0.0009 \n",
      "Epoch 770. TRAIN: loss 256.6185, r2: 0.3485. lr: 0.0009 \n",
      "Epoch 780. TRAIN: loss 255.2934, r2: 0.3518. lr: 0.0009 \n",
      "Epoch 790. TRAIN: loss 253.8549, r2: 0.3555. lr: 0.0009 \n",
      "Epoch 800. TRAIN: loss 251.7843, r2: 0.3608. lr: 0.0009 \n",
      "Epoch 810. TRAIN: loss 250.3847, r2: 0.3643. lr: 0.0009 \n",
      "Epoch 820. TRAIN: loss 249.2697, r2: 0.3671. lr: 0.0009 \n",
      "Epoch 830. TRAIN: loss 248.5507, r2: 0.3690. lr: 0.0009 \n",
      "Epoch 840. TRAIN: loss 246.7081, r2: 0.3736. lr: 0.0009 \n",
      "Epoch 850. TRAIN: loss 245.7829, r2: 0.3760. lr: 0.0009 \n",
      "Epoch 860. TRAIN: loss 243.8803, r2: 0.3808. lr: 0.0009 \n",
      "Epoch 870. TRAIN: loss 243.7117, r2: 0.3812. lr: 0.0009 \n",
      "Epoch 880. TRAIN: loss 242.5030, r2: 0.3843. lr: 0.0009 \n",
      "Epoch 890. TRAIN: loss 240.5619, r2: 0.3892. lr: 0.0009 \n",
      "Epoch 900. TRAIN: loss 239.4834, r2: 0.3920. lr: 0.0009 \n",
      "Epoch 910. TRAIN: loss 238.0807, r2: 0.3955. lr: 0.0009 \n",
      "Epoch 920. TRAIN: loss 236.2383, r2: 0.4002. lr: 0.0009 \n",
      "Epoch 930. TRAIN: loss 236.1005, r2: 0.4006. lr: 0.0009 \n",
      "Epoch 940. TRAIN: loss 234.5502, r2: 0.4045. lr: 0.0009 \n",
      "Epoch 950. TRAIN: loss 233.2794, r2: 0.4077. lr: 0.0009 \n",
      "Epoch 960. TRAIN: loss 235.7182, r2: 0.4015. lr: 0.0009 \n",
      "Epoch 970. TRAIN: loss 232.5203, r2: 0.4097. lr: 0.0009 \n",
      "Epoch 980. TRAIN: loss 229.4461, r2: 0.4175. lr: 0.0009 \n",
      "Epoch 990. TRAIN: loss 229.5700, r2: 0.4172. lr: 0.0009 \n",
      "Epoch 1000. TRAIN: loss 228.3951, r2: 0.4201. lr: 0.0009 \n",
      "Epoch 1010. TRAIN: loss 225.9792, r2: 0.4263. lr: 0.0009 \n",
      "Epoch 1020. TRAIN: loss 225.2564, r2: 0.4281. lr: 0.0009 \n",
      "Epoch 1030. TRAIN: loss 224.0269, r2: 0.4312. lr: 0.0009 \n",
      "Epoch 1040. TRAIN: loss 223.3031, r2: 0.4331. lr: 0.0009 \n",
      "Epoch 1050. TRAIN: loss 221.8189, r2: 0.4368. lr: 0.0009 \n",
      "Epoch 1060. TRAIN: loss 219.8722, r2: 0.4418. lr: 0.0009 \n",
      "Epoch 1070. TRAIN: loss 219.5304, r2: 0.4426. lr: 0.0009 \n",
      "Epoch 1080. TRAIN: loss 218.6684, r2: 0.4448. lr: 0.0008 \n",
      "Epoch 1090. TRAIN: loss 218.0387, r2: 0.4464. lr: 0.0008 \n",
      "Epoch 1100. TRAIN: loss 217.7237, r2: 0.4472. lr: 0.0007 \n",
      "Epoch 1110. TRAIN: loss 215.2432, r2: 0.4535. lr: 0.0007 \n",
      "Epoch 1120. TRAIN: loss 214.0543, r2: 0.4565. lr: 0.0007 \n",
      "Epoch 1130. TRAIN: loss 213.3186, r2: 0.4584. lr: 0.0007 \n",
      "Epoch 1140. TRAIN: loss 212.4281, r2: 0.4607. lr: 0.0007 \n",
      "Epoch 1150. TRAIN: loss 212.3484, r2: 0.4609. lr: 0.0007 \n",
      "Epoch 1160. TRAIN: loss 211.0562, r2: 0.4642. lr: 0.0007 \n",
      "Epoch 1170. TRAIN: loss 212.5151, r2: 0.4605. lr: 0.0007 \n",
      "Epoch 1180. TRAIN: loss 209.7039, r2: 0.4676. lr: 0.0006 \n",
      "Epoch 1190. TRAIN: loss 208.9138, r2: 0.4696. lr: 0.0006 \n",
      "Epoch 1200. TRAIN: loss 208.2886, r2: 0.4712. lr: 0.0006 \n",
      "Epoch 1210. TRAIN: loss 208.4277, r2: 0.4708. lr: 0.0005 \n",
      "Epoch 1220. TRAIN: loss 205.7538, r2: 0.4776. lr: 0.0005 \n",
      "Epoch 1230. TRAIN: loss 205.4905, r2: 0.4783. lr: 0.0005 \n",
      "Epoch 1240. TRAIN: loss 205.3066, r2: 0.4788. lr: 0.0005 \n",
      "Epoch 1250. TRAIN: loss 204.6005, r2: 0.4805. lr: 0.0005 \n",
      "Epoch 1260. TRAIN: loss 204.6917, r2: 0.4803. lr: 0.0005 \n",
      "Epoch 1270. TRAIN: loss 205.7689, r2: 0.4776. lr: 0.0005 \n",
      "Epoch 1280. TRAIN: loss 202.2368, r2: 0.4865. lr: 0.0005 \n",
      "Epoch 1290. TRAIN: loss 203.0904, r2: 0.4844. lr: 0.0005 \n",
      "Epoch 1300. TRAIN: loss 202.6808, r2: 0.4854. lr: 0.0005 \n",
      "Epoch 1310. TRAIN: loss 201.4550, r2: 0.4885. lr: 0.0005 \n",
      "Epoch 1320. TRAIN: loss 201.0060, r2: 0.4897. lr: 0.0005 \n",
      "Epoch 1330. TRAIN: loss 199.3792, r2: 0.4938. lr: 0.0005 \n",
      "Epoch 1340. TRAIN: loss 200.6328, r2: 0.4906. lr: 0.0005 \n",
      "Epoch 1350. TRAIN: loss 200.4932, r2: 0.4910. lr: 0.0005 \n",
      "Epoch 1360. TRAIN: loss 199.5000, r2: 0.4935. lr: 0.0005 \n",
      "Epoch 1370. TRAIN: loss 200.7730, r2: 0.4903. lr: 0.0005 \n",
      "Epoch 1380. TRAIN: loss 198.1407, r2: 0.4969. lr: 0.0005 \n",
      "Epoch 1390. TRAIN: loss 199.7881, r2: 0.4928. lr: 0.0005 \n",
      "Epoch 1400. TRAIN: loss 198.2646, r2: 0.4966. lr: 0.0005 \n",
      "Epoch 1410. TRAIN: loss 197.6326, r2: 0.4982. lr: 0.0005 \n",
      "Epoch 1420. TRAIN: loss 198.1731, r2: 0.4969. lr: 0.0005 \n",
      "Epoch 1430. TRAIN: loss 196.9631, r2: 0.4999. lr: 0.0005 \n",
      "Epoch 1440. TRAIN: loss 196.2092, r2: 0.5019. lr: 0.0005 \n",
      "Epoch 1450. TRAIN: loss 205.3291, r2: 0.4787. lr: 0.0005 \n",
      "Epoch 1460. TRAIN: loss 197.8636, r2: 0.4977. lr: 0.0005 \n",
      "Epoch 1470. TRAIN: loss 198.0939, r2: 0.4971. lr: 0.0005 \n",
      "Epoch 1480. TRAIN: loss 195.3279, r2: 0.5041. lr: 0.0005 \n",
      "Epoch 1490. TRAIN: loss 196.3098, r2: 0.5016. lr: 0.0005 \n",
      "Epoch 1500. TRAIN: loss 194.2237, r2: 0.5069. lr: 0.0005 \n",
      "Epoch 1510. TRAIN: loss 194.7650, r2: 0.5055. lr: 0.0005 \n",
      "Epoch 1520. TRAIN: loss 191.8158, r2: 0.5130. lr: 0.0005 \n",
      "Epoch 1530. TRAIN: loss 191.6561, r2: 0.5134. lr: 0.0005 \n",
      "Epoch 1540. TRAIN: loss 190.3911, r2: 0.5166. lr: 0.0005 \n",
      "Epoch 1550. TRAIN: loss 190.8325, r2: 0.5155. lr: 0.0005 \n",
      "Epoch 1560. TRAIN: loss 189.8925, r2: 0.5179. lr: 0.0005 \n",
      "Epoch 1570. TRAIN: loss 192.1476, r2: 0.5122. lr: 0.0005 \n",
      "Epoch 1580. TRAIN: loss 192.6992, r2: 0.5108. lr: 0.0005 \n",
      "Epoch 1590. TRAIN: loss 190.9904, r2: 0.5151. lr: 0.0005 \n",
      "Epoch 1600. TRAIN: loss 190.4710, r2: 0.5164. lr: 0.0005 \n",
      "Epoch 1610. TRAIN: loss 189.1176, r2: 0.5199. lr: 0.0005 \n",
      "Epoch 1620. TRAIN: loss 188.7479, r2: 0.5208. lr: 0.0005 \n",
      "Epoch 1630. TRAIN: loss 188.1727, r2: 0.5223. lr: 0.0005 \n",
      "Epoch 1640. TRAIN: loss 187.7369, r2: 0.5234. lr: 0.0005 \n",
      "Epoch 1650. TRAIN: loss 186.2112, r2: 0.5272. lr: 0.0005 \n",
      "Epoch 1660. TRAIN: loss 188.4470, r2: 0.5216. lr: 0.0005 \n",
      "Epoch 1670. TRAIN: loss 186.5087, r2: 0.5265. lr: 0.0005 \n",
      "Epoch 1680. TRAIN: loss 187.3940, r2: 0.5242. lr: 0.0005 \n",
      "Epoch 1690. TRAIN: loss 187.0354, r2: 0.5251. lr: 0.0005 \n",
      "Epoch 1700. TRAIN: loss 184.9337, r2: 0.5305. lr: 0.0005 \n",
      "Epoch 1710. TRAIN: loss 185.1312, r2: 0.5300. lr: 0.0005 \n",
      "Epoch 1720. TRAIN: loss 185.4374, r2: 0.5292. lr: 0.0005 \n",
      "Epoch 1730. TRAIN: loss 188.6282, r2: 0.5211. lr: 0.0005 \n",
      "Epoch 1740. TRAIN: loss 183.5538, r2: 0.5340. lr: 0.0005 \n",
      "Epoch 1750. TRAIN: loss 183.4292, r2: 0.5343. lr: 0.0005 \n",
      "Epoch 1760. TRAIN: loss 182.5933, r2: 0.5364. lr: 0.0005 \n",
      "Epoch 1770. TRAIN: loss 181.1044, r2: 0.5402. lr: 0.0005 \n",
      "Epoch 1780. TRAIN: loss 181.9233, r2: 0.5381. lr: 0.0005 \n",
      "Epoch 1790. TRAIN: loss 182.5905, r2: 0.5364. lr: 0.0005 \n",
      "Epoch 1800. TRAIN: loss 187.3682, r2: 0.5243. lr: 0.0005 \n",
      "Epoch 1810. TRAIN: loss 184.9838, r2: 0.5304. lr: 0.0005 \n",
      "Epoch 1820. TRAIN: loss 186.1762, r2: 0.5273. lr: 0.0005 \n",
      "Epoch 1830. TRAIN: loss 182.4647, r2: 0.5367. lr: 0.0005 \n",
      "Epoch 1840. TRAIN: loss 181.8674, r2: 0.5383. lr: 0.0005 \n",
      "Epoch 1850. TRAIN: loss 179.4988, r2: 0.5443. lr: 0.0005 \n",
      "Epoch 1860. TRAIN: loss 180.0645, r2: 0.5428. lr: 0.0005 \n",
      "Epoch 1870. TRAIN: loss 180.9922, r2: 0.5405. lr: 0.0005 \n",
      "Epoch 1880. TRAIN: loss 179.6526, r2: 0.5439. lr: 0.0005 \n",
      "Epoch 1890. TRAIN: loss 178.2179, r2: 0.5475. lr: 0.0005 \n",
      "Epoch 1900. TRAIN: loss 177.1170, r2: 0.5503. lr: 0.0005 \n",
      "Epoch 1910. TRAIN: loss 177.1481, r2: 0.5502. lr: 0.0005 \n",
      "Epoch 1920. TRAIN: loss 178.0819, r2: 0.5479. lr: 0.0005 \n",
      "Epoch 1930. TRAIN: loss 178.1948, r2: 0.5476. lr: 0.0005 \n",
      "Epoch 1940. TRAIN: loss 178.4699, r2: 0.5469. lr: 0.0005 \n",
      "Epoch 1950. TRAIN: loss 175.5301, r2: 0.5544. lr: 0.0005 \n",
      "Epoch 1960. TRAIN: loss 177.2885, r2: 0.5499. lr: 0.0005 \n",
      "Epoch 1970. TRAIN: loss 178.1333, r2: 0.5477. lr: 0.0005 \n",
      "Epoch 1980. TRAIN: loss 176.0431, r2: 0.5531. lr: 0.0005 \n",
      "Epoch 1990. TRAIN: loss 176.5987, r2: 0.5516. lr: 0.0005 \n",
      "Epoch 2000. TRAIN: loss 175.3138, r2: 0.5549. lr: 0.0005 \n",
      "Epoch 2010. TRAIN: loss 181.2711, r2: 0.5398. lr: 0.0005 \n",
      "Epoch 2020. TRAIN: loss 176.1958, r2: 0.5527. lr: 0.0005 \n",
      "Epoch 2030. TRAIN: loss 178.6152, r2: 0.5465. lr: 0.0005 \n",
      "Epoch 2040. TRAIN: loss 190.1083, r2: 0.5173. lr: 0.0005 \n",
      "Epoch 2050. TRAIN: loss 176.2114, r2: 0.5526. lr: 0.0005 \n",
      "Epoch 2060. TRAIN: loss 174.0829, r2: 0.5580. lr: 0.0005 \n",
      "Epoch 2070. TRAIN: loss 172.1702, r2: 0.5629. lr: 0.0005 \n",
      "Epoch 2080. TRAIN: loss 171.5788, r2: 0.5644. lr: 0.0005 \n",
      "Epoch 2090. TRAIN: loss 173.8280, r2: 0.5587. lr: 0.0005 \n",
      "Epoch 2100. TRAIN: loss 171.3794, r2: 0.5649. lr: 0.0005 \n",
      "Epoch 2110. TRAIN: loss 167.8633, r2: 0.5738. lr: 0.0005 \n",
      "Epoch 2120. TRAIN: loss 171.1399, r2: 0.5655. lr: 0.0005 \n",
      "Epoch 2130. TRAIN: loss 168.7703, r2: 0.5715. lr: 0.0005 \n",
      "Epoch 2140. TRAIN: loss 169.2750, r2: 0.5702. lr: 0.0005 \n",
      "Epoch 2150. TRAIN: loss 169.1581, r2: 0.5705. lr: 0.0005 \n",
      "Epoch 2160. TRAIN: loss 167.4149, r2: 0.5750. lr: 0.0005 \n",
      "Epoch 2170. TRAIN: loss 170.6090, r2: 0.5668. lr: 0.0005 \n",
      "Epoch 2180. TRAIN: loss 171.6056, r2: 0.5643. lr: 0.0005 \n",
      "Epoch 2190. TRAIN: loss 169.7531, r2: 0.5690. lr: 0.0005 \n",
      "Epoch 2200. TRAIN: loss 169.5121, r2: 0.5696. lr: 0.0005 \n",
      "Epoch 2210. TRAIN: loss 167.7568, r2: 0.5741. lr: 0.0005 \n",
      "Epoch 2220. TRAIN: loss 166.9863, r2: 0.5760. lr: 0.0005 \n",
      "Epoch 2230. TRAIN: loss 168.1003, r2: 0.5732. lr: 0.0005 \n",
      "Epoch 2240. TRAIN: loss 164.5953, r2: 0.5821. lr: 0.0005 \n",
      "Epoch 2250. TRAIN: loss 166.6658, r2: 0.5769. lr: 0.0005 \n",
      "Epoch 2260. TRAIN: loss 168.4075, r2: 0.5724. lr: 0.0005 \n",
      "Epoch 2270. TRAIN: loss 171.7940, r2: 0.5638. lr: 0.0005 \n",
      "Epoch 2280. TRAIN: loss 166.8575, r2: 0.5764. lr: 0.0005 \n",
      "Epoch 2290. TRAIN: loss 163.2800, r2: 0.5855. lr: 0.0005 \n",
      "Epoch 2300. TRAIN: loss 163.1931, r2: 0.5857. lr: 0.0005 \n",
      "Epoch 2310. TRAIN: loss 164.4715, r2: 0.5824. lr: 0.0005 \n",
      "Epoch 2320. TRAIN: loss 160.8933, r2: 0.5915. lr: 0.0005 \n",
      "Epoch 2330. TRAIN: loss 161.7611, r2: 0.5893. lr: 0.0005 \n",
      "Epoch 2340. TRAIN: loss 162.8616, r2: 0.5865. lr: 0.0005 \n",
      "Epoch 2350. TRAIN: loss 162.0365, r2: 0.5886. lr: 0.0005 \n",
      "Epoch 2360. TRAIN: loss 161.6023, r2: 0.5897. lr: 0.0005 \n",
      "Epoch 2370. TRAIN: loss 159.4275, r2: 0.5952. lr: 0.0005 \n",
      "Epoch 2380. TRAIN: loss 160.2966, r2: 0.5930. lr: 0.0005 \n",
      "Epoch 2390. TRAIN: loss 160.0938, r2: 0.5935. lr: 0.0005 \n",
      "Epoch 2400. TRAIN: loss 161.4686, r2: 0.5901. lr: 0.0005 \n",
      "Epoch 2410. TRAIN: loss 159.6928, r2: 0.5946. lr: 0.0005 \n",
      "Epoch 2420. TRAIN: loss 161.3016, r2: 0.5905. lr: 0.0005 \n",
      "Epoch 2430. TRAIN: loss 157.4956, r2: 0.6001. lr: 0.0005 \n",
      "Epoch 2440. TRAIN: loss 160.9198, r2: 0.5914. lr: 0.0005 \n",
      "Epoch 2450. TRAIN: loss 158.0769, r2: 0.5987. lr: 0.0005 \n",
      "Epoch 2460. TRAIN: loss 158.8666, r2: 0.5967. lr: 0.0005 \n",
      "Epoch 2470. TRAIN: loss 159.0182, r2: 0.5963. lr: 0.0005 \n",
      "Epoch 2480. TRAIN: loss 156.6271, r2: 0.6023. lr: 0.0005 \n",
      "Epoch 2490. TRAIN: loss 157.7666, r2: 0.5995. lr: 0.0005 \n",
      "Epoch 2500. TRAIN: loss 157.1002, r2: 0.6011. lr: 0.0005 \n",
      "Epoch 2510. TRAIN: loss 158.6747, r2: 0.5971. lr: 0.0005 \n",
      "Epoch 2520. TRAIN: loss 158.2618, r2: 0.5982. lr: 0.0005 \n",
      "Epoch 2530. TRAIN: loss 155.9746, r2: 0.6040. lr: 0.0005 \n",
      "Epoch 2540. TRAIN: loss 157.1058, r2: 0.6011. lr: 0.0005 \n",
      "Epoch 2550. TRAIN: loss 154.7172, r2: 0.6072. lr: 0.0005 \n",
      "Epoch 2560. TRAIN: loss 155.8271, r2: 0.6044. lr: 0.0005 \n",
      "Epoch 2570. TRAIN: loss 155.8134, r2: 0.6044. lr: 0.0005 \n",
      "Epoch 2580. TRAIN: loss 154.2260, r2: 0.6084. lr: 0.0005 \n",
      "Epoch 2590. TRAIN: loss 154.0484, r2: 0.6089. lr: 0.0005 \n",
      "Epoch 2600. TRAIN: loss 157.3198, r2: 0.6006. lr: 0.0005 \n",
      "Epoch 2610. TRAIN: loss 150.9309, r2: 0.6168. lr: 0.0005 \n",
      "Epoch 2620. TRAIN: loss 152.3022, r2: 0.6133. lr: 0.0005 \n",
      "Epoch 2630. TRAIN: loss 152.8963, r2: 0.6118. lr: 0.0005 \n",
      "Epoch 2640. TRAIN: loss 149.6042, r2: 0.6202. lr: 0.0005 \n",
      "Epoch 2650. TRAIN: loss 154.2883, r2: 0.6083. lr: 0.0005 \n",
      "Epoch 2660. TRAIN: loss 151.9207, r2: 0.6143. lr: 0.0005 \n",
      "Epoch 2670. TRAIN: loss 151.9463, r2: 0.6142. lr: 0.0005 \n",
      "Epoch 2680. TRAIN: loss 148.6909, r2: 0.6225. lr: 0.0005 \n",
      "Epoch 2690. TRAIN: loss 148.0049, r2: 0.6242. lr: 0.0005 \n",
      "Epoch 2700. TRAIN: loss 148.3790, r2: 0.6233. lr: 0.0005 \n",
      "Epoch 2710. TRAIN: loss 150.9116, r2: 0.6169. lr: 0.0005 \n",
      "Epoch 2720. TRAIN: loss 148.4340, r2: 0.6231. lr: 0.0005 \n",
      "Epoch 2730. TRAIN: loss 148.9868, r2: 0.6217. lr: 0.0005 \n",
      "Epoch 2740. TRAIN: loss 150.4100, r2: 0.6181. lr: 0.0005 \n",
      "Epoch 2750. TRAIN: loss 153.7787, r2: 0.6096. lr: 0.0005 \n",
      "Epoch 2760. TRAIN: loss 150.3239, r2: 0.6183. lr: 0.0005 \n",
      "Epoch 2770. TRAIN: loss 147.1325, r2: 0.6265. lr: 0.0005 \n",
      "Epoch 2780. TRAIN: loss 146.7911, r2: 0.6273. lr: 0.0005 \n",
      "Epoch 2790. TRAIN: loss 149.1907, r2: 0.6212. lr: 0.0005 \n",
      "Epoch 2800. TRAIN: loss 149.2086, r2: 0.6212. lr: 0.0005 \n",
      "Epoch 2810. TRAIN: loss 145.6635, r2: 0.6302. lr: 0.0005 \n",
      "Epoch 2820. TRAIN: loss 148.9465, r2: 0.6218. lr: 0.0005 \n",
      "Epoch 2830. TRAIN: loss 148.5075, r2: 0.6230. lr: 0.0005 \n",
      "Epoch 2840. TRAIN: loss 146.9302, r2: 0.6270. lr: 0.0005 \n",
      "Epoch 2850. TRAIN: loss 145.9771, r2: 0.6294. lr: 0.0005 \n",
      "Epoch 2860. TRAIN: loss 143.9220, r2: 0.6346. lr: 0.0005 \n",
      "Epoch 2870. TRAIN: loss 147.9314, r2: 0.6244. lr: 0.0005 \n",
      "Epoch 2880. TRAIN: loss 146.8541, r2: 0.6272. lr: 0.0005 \n",
      "Epoch 2890. TRAIN: loss 148.6861, r2: 0.6225. lr: 0.0005 \n",
      "Epoch 2900. TRAIN: loss 144.6927, r2: 0.6326. lr: 0.0005 \n",
      "Epoch 2910. TRAIN: loss 144.4866, r2: 0.6332. lr: 0.0005 \n",
      "Epoch 2920. TRAIN: loss 147.3785, r2: 0.6258. lr: 0.0005 \n",
      "Epoch 2930. TRAIN: loss 145.8748, r2: 0.6296. lr: 0.0005 \n",
      "Epoch 2940. TRAIN: loss 145.9430, r2: 0.6295. lr: 0.0005 \n",
      "Epoch 2950. TRAIN: loss 142.5751, r2: 0.6380. lr: 0.0005 \n",
      "Epoch 2960. TRAIN: loss 148.9036, r2: 0.6220. lr: 0.0005 \n",
      "Epoch 2970. TRAIN: loss 145.6935, r2: 0.6301. lr: 0.0005 \n",
      "Epoch 2980. TRAIN: loss 145.4144, r2: 0.6308. lr: 0.0005 \n",
      "Epoch 2990. TRAIN: loss 143.1799, r2: 0.6365. lr: 0.0005 \n",
      "Epoch 3000. TRAIN: loss 140.4871, r2: 0.6433. lr: 0.0005 \n",
      "Epoch 3010. TRAIN: loss 146.4134, r2: 0.6283. lr: 0.0005 \n",
      "Epoch 3020. TRAIN: loss 143.4925, r2: 0.6357. lr: 0.0005 \n",
      "Epoch 3030. TRAIN: loss 147.0393, r2: 0.6267. lr: 0.0005 \n",
      "Epoch 3040. TRAIN: loss 152.3103, r2: 0.6133. lr: 0.0005 \n",
      "Epoch 3050. TRAIN: loss 144.4272, r2: 0.6333. lr: 0.0005 \n",
      "Epoch 3060. TRAIN: loss 145.5054, r2: 0.6306. lr: 0.0005 \n",
      "Epoch 3070. TRAIN: loss 143.2358, r2: 0.6363. lr: 0.0005 \n",
      "Epoch 3080. TRAIN: loss 146.3197, r2: 0.6285. lr: 0.0005 \n",
      "Epoch 3090. TRAIN: loss 144.3030, r2: 0.6336. lr: 0.0005 \n",
      "Epoch 3100. TRAIN: loss 142.0213, r2: 0.6394. lr: 0.0005 \n",
      "Epoch 3110. TRAIN: loss 152.5692, r2: 0.6126. lr: 0.0005 \n",
      "Epoch 3120. TRAIN: loss 148.6103, r2: 0.6227. lr: 0.0005 \n",
      "Epoch 3130. TRAIN: loss 139.4277, r2: 0.6460. lr: 0.0005 \n",
      "Epoch 3140. TRAIN: loss 142.1877, r2: 0.6390. lr: 0.0005 \n",
      "Epoch 3150. TRAIN: loss 144.7664, r2: 0.6325. lr: 0.0005 \n",
      "Epoch 3160. TRAIN: loss 140.6657, r2: 0.6429. lr: 0.0005 \n",
      "Epoch 3170. TRAIN: loss 146.4693, r2: 0.6281. lr: 0.0005 \n",
      "Epoch 3180. TRAIN: loss 140.3837, r2: 0.6436. lr: 0.0005 \n",
      "Epoch 3190. TRAIN: loss 142.1534, r2: 0.6391. lr: 0.0005 \n",
      "Epoch 3200. TRAIN: loss 139.5040, r2: 0.6458. lr: 0.0005 \n",
      "Epoch 3210. TRAIN: loss 136.5639, r2: 0.6533. lr: 0.0005 \n",
      "Epoch 3220. TRAIN: loss 140.5617, r2: 0.6431. lr: 0.0005 \n",
      "Epoch 3230. TRAIN: loss 137.6099, r2: 0.6506. lr: 0.0005 \n",
      "Epoch 3240. TRAIN: loss 139.5255, r2: 0.6458. lr: 0.0005 \n",
      "Epoch 3250. TRAIN: loss 136.3772, r2: 0.6538. lr: 0.0005 \n",
      "Epoch 3260. TRAIN: loss 139.0446, r2: 0.6470. lr: 0.0005 \n",
      "Epoch 3270. TRAIN: loss 137.4244, r2: 0.6511. lr: 0.0005 \n",
      "Epoch 3280. TRAIN: loss 140.6520, r2: 0.6429. lr: 0.0005 \n",
      "Epoch 3290. TRAIN: loss 143.1093, r2: 0.6367. lr: 0.0005 \n",
      "Epoch 3300. TRAIN: loss 135.7860, r2: 0.6553. lr: 0.0005 \n",
      "Epoch 3310. TRAIN: loss 136.1118, r2: 0.6544. lr: 0.0005 \n",
      "Epoch 3320. TRAIN: loss 138.3226, r2: 0.6488. lr: 0.0005 \n",
      "Epoch 3330. TRAIN: loss 135.5087, r2: 0.6560. lr: 0.0005 \n",
      "Epoch 3340. TRAIN: loss 139.6395, r2: 0.6455. lr: 0.0005 \n",
      "Epoch 3350. TRAIN: loss 136.7005, r2: 0.6529. lr: 0.0005 \n",
      "Epoch 3360. TRAIN: loss 135.5789, r2: 0.6558. lr: 0.0005 \n",
      "Epoch 3370. TRAIN: loss 136.2322, r2: 0.6541. lr: 0.0005 \n",
      "Epoch 3380. TRAIN: loss 141.1552, r2: 0.6416. lr: 0.0005 \n",
      "Epoch 3390. TRAIN: loss 139.7038, r2: 0.6453. lr: 0.0005 \n",
      "Epoch 3400. TRAIN: loss 135.9348, r2: 0.6549. lr: 0.0005 \n",
      "Epoch 3410. TRAIN: loss 135.0607, r2: 0.6571. lr: 0.0005 \n",
      "Epoch 3420. TRAIN: loss 135.8885, r2: 0.6550. lr: 0.0005 \n",
      "Epoch 3430. TRAIN: loss 135.4043, r2: 0.6562. lr: 0.0005 \n",
      "Epoch 3440. TRAIN: loss 133.0741, r2: 0.6621. lr: 0.0005 \n",
      "Epoch 3450. TRAIN: loss 133.5843, r2: 0.6608. lr: 0.0005 \n",
      "Epoch 3460. TRAIN: loss 135.7190, r2: 0.6554. lr: 0.0005 \n",
      "Epoch 3470. TRAIN: loss 134.0229, r2: 0.6597. lr: 0.0005 \n",
      "Epoch 3480. TRAIN: loss 134.1327, r2: 0.6595. lr: 0.0005 \n",
      "Epoch 3490. TRAIN: loss 136.2833, r2: 0.6540. lr: 0.0005 \n",
      "Epoch 3500. TRAIN: loss 130.9240, r2: 0.6676. lr: 0.0005 \n",
      "Epoch 3510. TRAIN: loss 128.7378, r2: 0.6732. lr: 0.0005 \n",
      "Epoch 3520. TRAIN: loss 133.0628, r2: 0.6622. lr: 0.0005 \n",
      "Epoch 3530. TRAIN: loss 131.3208, r2: 0.6666. lr: 0.0005 \n",
      "Epoch 3540. TRAIN: loss 137.0898, r2: 0.6519. lr: 0.0005 \n",
      "Epoch 3550. TRAIN: loss 137.1597, r2: 0.6518. lr: 0.0005 \n",
      "Epoch 3560. TRAIN: loss 134.7013, r2: 0.6580. lr: 0.0005 \n",
      "Epoch 3570. TRAIN: loss 131.8035, r2: 0.6654. lr: 0.0005 \n",
      "Epoch 3580. TRAIN: loss 137.3910, r2: 0.6512. lr: 0.0005 \n",
      "Epoch 3590. TRAIN: loss 133.5610, r2: 0.6609. lr: 0.0005 \n",
      "Epoch 3600. TRAIN: loss 138.9963, r2: 0.6471. lr: 0.0005 \n",
      "Epoch 3610. TRAIN: loss 147.6760, r2: 0.6251. lr: 0.0005 \n",
      "Epoch 3620. TRAIN: loss 145.7840, r2: 0.6299. lr: 0.0005 \n",
      "Epoch 3630. TRAIN: loss 155.4502, r2: 0.6053. lr: 0.0005 \n",
      "Epoch 3640. TRAIN: loss 138.0507, r2: 0.6495. lr: 0.0005 \n",
      "Epoch 3650. TRAIN: loss 136.7656, r2: 0.6528. lr: 0.0005 \n",
      "Epoch 3660. TRAIN: loss 134.3712, r2: 0.6588. lr: 0.0005 \n",
      "Epoch 3670. TRAIN: loss 134.2013, r2: 0.6593. lr: 0.0005 \n",
      "Epoch 3680. TRAIN: loss 133.7180, r2: 0.6605. lr: 0.0005 \n",
      "Epoch 3690. TRAIN: loss 133.1912, r2: 0.6618. lr: 0.0005 \n",
      "Epoch 3700. TRAIN: loss 133.6068, r2: 0.6608. lr: 0.0005 \n",
      "Epoch 3710. TRAIN: loss 132.8554, r2: 0.6627. lr: 0.0005 \n",
      "Epoch 3720. TRAIN: loss 130.3174, r2: 0.6691. lr: 0.0005 \n",
      "Epoch 3730. TRAIN: loss 138.3866, r2: 0.6487. lr: 0.0005 \n",
      "Epoch 3740. TRAIN: loss 133.1480, r2: 0.6620. lr: 0.0005 \n",
      "Epoch 3750. TRAIN: loss 132.1059, r2: 0.6646. lr: 0.0005 \n",
      "Epoch 3760. TRAIN: loss 133.7557, r2: 0.6604. lr: 0.0005 \n",
      "Epoch 3770. TRAIN: loss 130.9237, r2: 0.6676. lr: 0.0005 \n",
      "Epoch 3780. TRAIN: loss 130.4541, r2: 0.6688. lr: 0.0005 \n",
      "Epoch 3790. TRAIN: loss 130.2998, r2: 0.6692. lr: 0.0005 \n",
      "Epoch 3800. TRAIN: loss 131.2093, r2: 0.6669. lr: 0.0005 \n",
      "Epoch 3810. TRAIN: loss 128.1263, r2: 0.6747. lr: 0.0005 \n",
      "Epoch 3820. TRAIN: loss 134.4610, r2: 0.6586. lr: 0.0005 \n",
      "Epoch 3830. TRAIN: loss 129.1736, r2: 0.6720. lr: 0.0005 \n",
      "Epoch 3840. TRAIN: loss 129.4971, r2: 0.6712. lr: 0.0005 \n",
      "Epoch 3850. TRAIN: loss 127.9109, r2: 0.6753. lr: 0.0005 \n",
      "Epoch 3860. TRAIN: loss 133.0238, r2: 0.6623. lr: 0.0005 \n",
      "Epoch 3870. TRAIN: loss 127.8698, r2: 0.6754. lr: 0.0005 \n",
      "Epoch 3880. TRAIN: loss 132.5283, r2: 0.6635. lr: 0.0005 \n",
      "Epoch 3890. TRAIN: loss 127.6539, r2: 0.6759. lr: 0.0005 \n",
      "Epoch 3900. TRAIN: loss 128.3700, r2: 0.6741. lr: 0.0005 \n",
      "Epoch 3910. TRAIN: loss 125.9207, r2: 0.6803. lr: 0.0005 \n",
      "Epoch 3920. TRAIN: loss 129.3579, r2: 0.6716. lr: 0.0005 \n",
      "Epoch 3930. TRAIN: loss 130.6350, r2: 0.6683. lr: 0.0005 \n",
      "Epoch 3940. TRAIN: loss 126.4851, r2: 0.6789. lr: 0.0005 \n",
      "Epoch 3950. TRAIN: loss 127.6822, r2: 0.6758. lr: 0.0005 \n",
      "Epoch 3960. TRAIN: loss 127.3617, r2: 0.6766. lr: 0.0005 \n",
      "Epoch 3970. TRAIN: loss 133.1911, r2: 0.6618. lr: 0.0005 \n",
      "Epoch 3980. TRAIN: loss 127.7911, r2: 0.6756. lr: 0.0005 \n",
      "Epoch 3990. TRAIN: loss 127.5093, r2: 0.6763. lr: 0.0005 \n",
      "Epoch 4000. TRAIN: loss 126.5695, r2: 0.6787. lr: 0.0005 \n",
      "Epoch 4010. TRAIN: loss 126.2247, r2: 0.6795. lr: 0.0005 \n",
      "Epoch 4020. TRAIN: loss 130.1746, r2: 0.6695. lr: 0.0005 \n",
      "Epoch 4030. TRAIN: loss 129.0113, r2: 0.6725. lr: 0.0005 \n",
      "Epoch 4040. TRAIN: loss 123.1351, r2: 0.6874. lr: 0.0005 \n",
      "Epoch 4050. TRAIN: loss 128.3869, r2: 0.6740. lr: 0.0005 \n",
      "Epoch 4060. TRAIN: loss 127.5005, r2: 0.6763. lr: 0.0005 \n",
      "Epoch 4070. TRAIN: loss 130.6247, r2: 0.6684. lr: 0.0005 \n",
      "Epoch 4080. TRAIN: loss 128.5229, r2: 0.6737. lr: 0.0005 \n",
      "Epoch 4090. TRAIN: loss 128.4271, r2: 0.6739. lr: 0.0005 \n",
      "Epoch 4100. TRAIN: loss 130.3240, r2: 0.6691. lr: 0.0005 \n",
      "Epoch 4110. TRAIN: loss 127.8532, r2: 0.6754. lr: 0.0005 \n",
      "Epoch 4120. TRAIN: loss 132.2816, r2: 0.6642. lr: 0.0005 \n",
      "Epoch 4130. TRAIN: loss 128.8529, r2: 0.6729. lr: 0.0005 \n",
      "Epoch 4140. TRAIN: loss 126.9531, r2: 0.6777. lr: 0.0005 \n",
      "Epoch 4150. TRAIN: loss 130.9437, r2: 0.6676. lr: 0.0005 \n",
      "Epoch 4160. TRAIN: loss 127.3270, r2: 0.6767. lr: 0.0005 \n",
      "Epoch 4170. TRAIN: loss 128.6829, r2: 0.6733. lr: 0.0005 \n",
      "Epoch 4180. TRAIN: loss 126.3305, r2: 0.6793. lr: 0.0005 \n",
      "Epoch 4190. TRAIN: loss 128.1390, r2: 0.6747. lr: 0.0005 \n",
      "Epoch 4200. TRAIN: loss 127.9271, r2: 0.6752. lr: 0.0005 \n",
      "Epoch 4210. TRAIN: loss 125.8203, r2: 0.6806. lr: 0.0005 \n",
      "Epoch 4220. TRAIN: loss 126.0523, r2: 0.6800. lr: 0.0005 \n",
      "Epoch 4230. TRAIN: loss 134.3878, r2: 0.6588. lr: 0.0005 \n",
      "Epoch 4240. TRAIN: loss 131.6166, r2: 0.6658. lr: 0.0005 \n",
      "Epoch 4250. TRAIN: loss 133.1368, r2: 0.6620. lr: 0.0005 \n",
      "Epoch 4260. TRAIN: loss 132.6461, r2: 0.6632. lr: 0.0005 \n",
      "Epoch 4270. TRAIN: loss 129.0094, r2: 0.6725. lr: 0.0005 \n",
      "Epoch 4280. TRAIN: loss 127.4654, r2: 0.6764. lr: 0.0005 \n",
      "Epoch 4290. TRAIN: loss 125.8620, r2: 0.6805. lr: 0.0005 \n",
      "Epoch 4300. TRAIN: loss 128.2031, r2: 0.6745. lr: 0.0005 \n",
      "Epoch 4310. TRAIN: loss 123.7582, r2: 0.6858. lr: 0.0005 \n",
      "Epoch 4320. TRAIN: loss 124.8971, r2: 0.6829. lr: 0.0005 \n",
      "Epoch 4330. TRAIN: loss 122.9228, r2: 0.6879. lr: 0.0005 \n",
      "Epoch 4340. TRAIN: loss 123.6766, r2: 0.6860. lr: 0.0005 \n",
      "Epoch 4350. TRAIN: loss 123.0522, r2: 0.6876. lr: 0.0005 \n",
      "Epoch 4360. TRAIN: loss 123.5040, r2: 0.6864. lr: 0.0005 \n",
      "Epoch 4370. TRAIN: loss 124.9320, r2: 0.6828. lr: 0.0005 \n",
      "Epoch 4380. TRAIN: loss 123.1477, r2: 0.6873. lr: 0.0005 \n",
      "Epoch 4390. TRAIN: loss 127.3776, r2: 0.6766. lr: 0.0005 \n",
      "Epoch 4400. TRAIN: loss 126.2972, r2: 0.6793. lr: 0.0005 \n",
      "Epoch 4410. TRAIN: loss 121.7940, r2: 0.6908. lr: 0.0005 \n",
      "Epoch 4420. TRAIN: loss 122.5105, r2: 0.6890. lr: 0.0005 \n",
      "Epoch 4430. TRAIN: loss 120.0674, r2: 0.6952. lr: 0.0005 \n",
      "Epoch 4440. TRAIN: loss 124.3337, r2: 0.6843. lr: 0.0005 \n",
      "Epoch 4450. TRAIN: loss 121.0436, r2: 0.6927. lr: 0.0005 \n",
      "Epoch 4460. TRAIN: loss 122.0731, r2: 0.6901. lr: 0.0005 \n",
      "Epoch 4470. TRAIN: loss 118.2268, r2: 0.6998. lr: 0.0005 \n",
      "Epoch 4480. TRAIN: loss 120.7462, r2: 0.6934. lr: 0.0005 \n",
      "Epoch 4490. TRAIN: loss 125.2318, r2: 0.6821. lr: 0.0005 \n",
      "Epoch 4500. TRAIN: loss 122.7590, r2: 0.6883. lr: 0.0005 \n",
      "Epoch 4510. TRAIN: loss 124.7381, r2: 0.6833. lr: 0.0005 \n",
      "Epoch 4520. TRAIN: loss 119.3645, r2: 0.6969. lr: 0.0005 \n",
      "Epoch 4530. TRAIN: loss 120.5520, r2: 0.6939. lr: 0.0005 \n",
      "Epoch 4540. TRAIN: loss 123.2745, r2: 0.6870. lr: 0.0005 \n",
      "Epoch 4550. TRAIN: loss 118.5769, r2: 0.6989. lr: 0.0005 \n",
      "Epoch 4560. TRAIN: loss 125.8862, r2: 0.6804. lr: 0.0005 \n",
      "Epoch 4570. TRAIN: loss 125.1788, r2: 0.6822. lr: 0.0005 \n",
      "Epoch 4580. TRAIN: loss 126.0180, r2: 0.6801. lr: 0.0005 \n",
      "Epoch 4590. TRAIN: loss 124.6730, r2: 0.6835. lr: 0.0005 \n",
      "Epoch 4600. TRAIN: loss 124.5453, r2: 0.6838. lr: 0.0005 \n",
      "Epoch 4610. TRAIN: loss 122.3884, r2: 0.6893. lr: 0.0005 \n",
      "Epoch 4620. TRAIN: loss 119.0250, r2: 0.6978. lr: 0.0005 \n",
      "Epoch 4630. TRAIN: loss 125.3125, r2: 0.6818. lr: 0.0005 \n",
      "Epoch 4640. TRAIN: loss 124.9918, r2: 0.6827. lr: 0.0005 \n",
      "Epoch 4650. TRAIN: loss 121.9904, r2: 0.6903. lr: 0.0005 \n",
      "Epoch 4660. TRAIN: loss 121.7734, r2: 0.6908. lr: 0.0005 \n",
      "Epoch 4670. TRAIN: loss 122.5496, r2: 0.6889. lr: 0.0005 \n",
      "Epoch 4680. TRAIN: loss 123.6696, r2: 0.6860. lr: 0.0005 \n",
      "Epoch 4690. TRAIN: loss 124.5191, r2: 0.6839. lr: 0.0005 \n",
      "Epoch 4700. TRAIN: loss 125.4784, r2: 0.6814. lr: 0.0005 \n",
      "Epoch 4710. TRAIN: loss 131.3064, r2: 0.6666. lr: 0.0005 \n",
      "Epoch 4720. TRAIN: loss 122.8746, r2: 0.6880. lr: 0.0005 \n",
      "Epoch 4730. TRAIN: loss 120.9774, r2: 0.6929. lr: 0.0005 \n",
      "Epoch 4740. TRAIN: loss 123.0771, r2: 0.6875. lr: 0.0005 \n",
      "Epoch 4750. TRAIN: loss 165.6612, r2: 0.5794. lr: 0.0005 \n",
      "Epoch 4760. TRAIN: loss 131.0723, r2: 0.6672. lr: 0.0005 \n",
      "Epoch 4770. TRAIN: loss 128.2902, r2: 0.6743. lr: 0.0005 \n",
      "Epoch 4780. TRAIN: loss 123.2317, r2: 0.6871. lr: 0.0005 \n",
      "Epoch 4790. TRAIN: loss 118.3327, r2: 0.6996. lr: 0.0005 \n",
      "Epoch 4800. TRAIN: loss 124.7990, r2: 0.6832. lr: 0.0005 \n",
      "Epoch 4810. TRAIN: loss 117.8728, r2: 0.7007. lr: 0.0005 \n",
      "Epoch 4820. TRAIN: loss 126.4170, r2: 0.6790. lr: 0.0005 \n",
      "Epoch 4830. TRAIN: loss 120.2603, r2: 0.6947. lr: 0.0005 \n",
      "Epoch 4840. TRAIN: loss 118.4725, r2: 0.6992. lr: 0.0005 \n",
      "Epoch 4850. TRAIN: loss 124.5509, r2: 0.6838. lr: 0.0005 \n",
      "Epoch 4860. TRAIN: loss 119.1770, r2: 0.6974. lr: 0.0005 \n",
      "Epoch 4870. TRAIN: loss 120.1198, r2: 0.6950. lr: 0.0005 \n",
      "Epoch 4880. TRAIN: loss 116.9793, r2: 0.7030. lr: 0.0005 \n",
      "Epoch 4890. TRAIN: loss 118.3302, r2: 0.6996. lr: 0.0005 \n",
      "Epoch 4900. TRAIN: loss 116.7872, r2: 0.7035. lr: 0.0005 \n",
      "Epoch 4910. TRAIN: loss 119.0192, r2: 0.6978. lr: 0.0005 \n",
      "Epoch 4920. TRAIN: loss 119.6667, r2: 0.6962. lr: 0.0005 \n",
      "Epoch 4930. TRAIN: loss 117.6452, r2: 0.7013. lr: 0.0005 \n",
      "Epoch 4940. TRAIN: loss 116.5040, r2: 0.7042. lr: 0.0005 \n",
      "Epoch 4950. TRAIN: loss 114.0064, r2: 0.7106. lr: 0.0005 \n",
      "Epoch 4960. TRAIN: loss 119.4790, r2: 0.6967. lr: 0.0005 \n",
      "Epoch 4970. TRAIN: loss 118.5233, r2: 0.6991. lr: 0.0005 \n",
      "Epoch 4980. TRAIN: loss 113.6894, r2: 0.7114. lr: 0.0005 \n",
      "Epoch 4990. TRAIN: loss 116.0273, r2: 0.7054. lr: 0.0005 \n",
      "Epoch 5000. TRAIN: loss 116.1357, r2: 0.7051. lr: 0.0005 \n"
     ]
    }
   ],
   "source": [
    "datetime_now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = \"./logs/\" + \"add_features_256_5_layers\" + datetime_now\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "trained_model = train_func(dataset.data, model_v1, 5000, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_v1, \"/var/essdata/IDU/other/mm_22/industrial-location/ml/model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
